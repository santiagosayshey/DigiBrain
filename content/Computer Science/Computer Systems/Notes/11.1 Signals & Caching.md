## The HACK Computer
- 64KB of separate read only instruction memory
- 32KB + 16KB + 2B of separate read / write data memory
	- 32KB for RAM
	- 16KB for screen
	- 2B for I/O
	- Instructions are 1 word - 16 bits or 2 bytes
- The entire machine runs off a **single clock signal**
- All instructions take exactly the same amount of time to execute
- A lot of embedded systems may be structured like this

![](docs/Images/Pasted%20image%2020231017113441.png)

## Signal Propagation - What is the limitation of clock speed?
- The clock must not tick again until propagation completes
	- The output of every gate needs time to get to the next gate
	- Every gate needs time to respond to it's inputs
	- The longest path through the machine must be able to complete

![](docs/Images/Pasted%20image%2020231017114054.png)

- The longest path is *very* long
	1. The PC must send a new address to instruction memory
	2. Only then can a new instruction be used to change control signals
	3. Only then can the correct inputs reach the ALU
	4. Only then can the correct output leave the ALU
	5. Only then can a Jump be evaluated
	6. Only then is the new value of the PC known

![](docs/Images/Pasted%20image%2020231017114758.png)

- Until propagation completes, bad things appear to happen
	- Wrong data being read from memory
	- Wrong data being written to the wrong memory address
	- The wrong values flowing through multiplexers
	- The wrong address being written to PC
	- The wrong values being written to the A and D registers
- Why does the computer still work?
	- Because the wrong values are never saved!
	- Memory, PC, A and D not change until  the next clock tick


### The Power Wall
- Switching consumes power
	- If the outputs of chips change, power is consumed
	- Even if externally there is no change, internal changes can occur
	- Careful routing of wires to minimise changes can save power

- The overall power consumption of a processor approximates:
$$
P = \frac{1}{2} \cdot C \cdot V^2 \cdot F
$$
- Power consumption performance challenges:
	- There is a practical limit to how much power can be used
	- Capacitive load reduction requires new manufacturing processes
	- Voltage reductions may be nearing their limits
	- Removing the generated can be expensive
	- We want lower power consumption

### What Can Be Done?
- Shorten the longest path
	- More efficient adders
		- Our 16 bit adder requires a signal to traverse 32 gates
		- Carry look ahead could reduce this to 5 gates
		- A 16 bit multiplier could be implemented with a 20 gate delay
	- A shorter longest path may allow a shorter clock cycle / faster clock
	- But a faster clock means more power consumption
- Split the processor into smaller parts linked by registers
	- Pipelining
- Use separate clocks for different components
	- Processor
	- Memory
	- I/O

### Slow Memory:
- **Hypothetical Scenario**: Imagine we replace the RAM in our Hack computer with a new type that's very slow—it takes 500 clock cycles to read from or write to it. This isn't great for performance because it's slow to respond, but it's inexpensive.
- **Ready Signal**: This new RAM has a "ready" signal that indicates when it's safe to read from or write to memory. This signal goes off (gets cleared) when there's a change in the address we're accessing (`addressM`) or when we want to write something new (`writeM`). It turns on (gets set) when the data has been successfully written or the data to be read is ready.
- **Pausing the Processor**: To cope with this slow memory, we could pause the Hack processor's operation while waiting for the memory. This can be done by using the "ready" signal in combination with the control signals for the A, D, and PC (Program Counter) registers. If we make it so these registers can't change unless the memory is ready, the processor effectively stops and waits whenever it needs to access RAM.
- **Impact on Performance**: Any instruction that needs to access this slow RAM will take 500 times longer to execute than it did before. However, instructions not using RAM are still fast. This means the computer still works, but it's much slower when using RAM.
    

### Real-world Scenario with Slow Memory:
- **Simplicity of the Hack Computer**: The Hack machine is straightforward—it has different memories for instructions and data, everything is synchronized to one clock signal, and every instruction takes the same time to execute.
- **Complexity of Usual Computers (Von Neumann architecture)**: Most real-world computers use a design where there's only one memory for both instructions (the code) and data. This setup can lead to "traffic jams" since both the CPU and the data operations need to use the same memory space.
- **Slow Memory Issue**: In real systems, the main memory (RAM) can be much slower compared to the CPU because of its larger size, meaning it takes more time (clock cycles) to access the data stored in it.
- **The Big Question**: So, how do we design a computer that works quickly even when it takes a long time to access memory? (This leads to discussions of various strategies used in computer architecture, like caching, pipelining, and parallelism, to mitigate the performance impact of slow memory access, but that's beyond the scope of the current passage.)

```
Imagine two cyclists (representing the CPU and the memory) riding together:

1. One cyclist (the CPU) is very fast, capable of riding at high speeds.
2. The other cyclist (the memory) is much slower, perhaps because they're on a heavier bike or are just not as fast.
```

## Memory Hierarchies
- The usual Von-Neumann architecture is more complex
	- There is only one memory
		- All instructions and data live in the same memory
		- Reading instructions interferes with reading / writing data
	- Real memories can be very large but much slower than the CPU
		- Takes anywhere from 2 to more than 500 clock cycles to access
	- How do make this work?

### Memory Caching - Hardware Implemented
- A memory cache
	- It may appear to behave like memory from the outside
	- Internally it keeps copies of blocks of memory that were accessed
	- It has a lookup table recording the address of the copies
	- Access to data in the cache is very fast
	- Access to data not in the cache must wait for the memory
	- Access to data not in the cache may be slower than without one

![](docs/Images/Pasted%20image%2020231017135218.png)

- Instruction caching
	- When an instruction is required, copy it, and others around it
	- Instructions already in the cache can be read very quickly
	- Instructions do not change
		- If the cache is full, you can safely overwrite anything you like
	- The same instructions are used over and over again - temporal locality
		- Loops and frequently used functions
	- Instructions are read in sequence, so can be cached in blocks - spatial locality
	- But the wrong choices can have a negative impact on performance
- Data caching
	- When data is required, copy it and the data around it
	- Data already in the cache can be read very quickly
	- When data is written, keep a copy of it
	- Caching data is a more complex problem
	- If data is frequently accessed, you want it to stay in the cache
	- If the cache is full, what do you overwrite?
	- You cannot overwrite new data that has been modified, but not written to memory
	- The wrong choices can have a catastrophic impact on performance
	- The choices are implemented in hardware, change may be impossible

![](docs/Images/Pasted%20image%2020231017135907.png)

- To fix the issues of data caching, we have multiple layers of cache

![](docs/Images/Pasted%20image%2020231017141654.png)

```
In our cycling duo, the faster cyclist (CPU) often needs quick sips of water (data) during the ride. The slower cyclist (memory) can't keep up but has a small, fast cart (cache) with water bottles. 

This cart doesn't speed up the cyclist but holds water bottles within easy reach for quick handoffs, reducing wait times. However, if the cart runs out of water (cache miss), the slower cyclist must fetch more from a distant van (main memory), causing delays. 

The key isn't making the cyclists match pace, but strategically minimizing waits by keeping the most needed supplies (frequently used data) at hand (in the cache).
```

### Memory Hierarchies
- The caches are mostly hidden from the programmer
- In some cases, virtual memory can be manipulated directly
- Memory mapped files are a good example
- Virtual memory is partly implemented in software
- Hardware exceptions run page fault handlers to move data
- Caches work to hide themselves from the programmer
- What the programmer sees on a modern CPU:

![](docs/Images/Pasted%20image%2020231017141856.png)

### Benefits of Caching
- Temporal locality
	- Instructions and data tend to get used again in the near future
	- Caches that replace the least recently used content may work well
- Spatial locality
	- Instructions and data tend to be near something that was just used
	- Caches usually copy an enclosing block of instructions or data
	- Cache line size can vary a lot, the best size depends on the program

## Summary
- Caching is used everywhere in a computer
- Instruction aching is usually very effective
	- Instructions do not change during execution
	- Most instructions get used over and over again
- Data caching can be very effective
	- Data changes during execution
	- When data is accessed, it may be used again soon
	- When data is accessed, nearby data may be used again soon
- Virtual Memory (VM)
	- Main memory can be a cache for a much larger virtual memory
	- Every program can view it's entire 16 exabytes of VM
	- It works so long as the programs do not use too much of it
	- Physical resources still need to be available

##### Examples


![](docs/Images/Pasted%20image%2020231019141214.png)

- The longest path gets longer, but multiply operations are now much much shorter

![](docs/Images/Pasted%20image%2020231019141520.png)
$$
P = \frac{1}{2} \cdot C \cdot V^2 \cdot F
$$
- Where F = 1/T

![](docs/Images/Pasted%20image%2020231019142053.png)

- At the start of a clock cycle, the RAM will always output the data from the location identified by the a register's previous value until the current value has had time to propagate through the RAM's Multiplexes. So in this case, the previous instruction must have started with the value 500 12 in the A Register, even if it was not accessing RAM.

![](docs/Images/Pasted%20image%2020231019142246.png)

