## Large Language Models

> *A large language model or LLM is a deep learning algorithm that can recognise summarise, translate, predict and generate text and other forms of content based on knowledge gained from massive data sets. Large language models are amongst the most successful applications of transformer models*

### Transformers
- Tokenise data
- Run mathematical and statistical formula across the data to discover relationships
- Look for patterns
- Use the self attention mechanism that allows it to associate context with the tokens
- **Internet trained models have internet scale biases**

### Where does this data come from?
- Internet data sources
- "Conversations"
- Human feedback 

### Issues under GDPR

![](docs/Images/Pasted%20image%2020230912232928.png)

### Issues of Fact

![](docs/Images/Pasted%20image%2020230912233101.png)

![](docs/Images/Pasted%20image%2020230912233245.png)

##### Examples

```
You use github's copilot to write a piece of software, which is rather niche. The code you receive is heavily based on 1 sample as there aren't many to use, although it's been perturbed and altered. 

What are the ethical uses of this code? what are the conditions under which there is not ethical or legal use?
```

Doesn't it depend on the license under which the original code segment was issued? If it's completely open source, then your code segment suggested by copilot by inheritance should also be open source I think. This means that it must be shared and cannot generate revenue


```
We define value neutrals being impartial and unbiased. In terms of the technology, it means that the technology is not intrinsically unethical or immoral, it's just that it can be put to that usage. 

Your design software that uses chat GPT to use a tweet as its basis and then build up a body of evidence, synthetically, that provides opposition to the opinion expressed in the tweet. The software constructs articles, tweets, identities, images and puts them all together to provide a cohesive referential thread. You argue that this is value neutral, as it's only the way people chose to use it that potentially causes problems. Discuss
```

It can't be value neutral. All chatGPT is doing is guessing what is the most likely word to come after a previous word. If it's trained only off of opinions from a select subset of tweets, those opinions coming from it will be heavily skewed to it's data that it learned from.

