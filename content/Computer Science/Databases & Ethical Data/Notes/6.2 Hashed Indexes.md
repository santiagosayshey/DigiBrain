## Hashing
- A hash function transforms its input in the following way:
	- The data can be of any size but the hash value is of a fixed size
	- This means that the range of hash values may be (and often is) smaller than the range of values we hash
	- Hash values may not be unique
	- The same data will always generate the same hash value

```
Imagine you have a big box of crayons, and each crayon has a unique, fancy color name like "Sunset Orange" or "Deep Ocean Blue." Now, let's say you want a system to quickly find a crayon without having to read all those long names every time.

So, you decide to label each crayon with a simple number instead. This way, "Sunset Orange" might become "3", and "Deep Ocean Blue" might become "7". 

You do this using a special machine: you put a crayon into it, and it gives you a number. This machine is like our hash function.

Now, here are some things about this crayon-numbering system:

1. No matter how long the crayon's name is, the machine always gives you a small number (fixed size).
2. Sometimes, two different crayons might get the same number because there are so many crayons and only a few numbers to give. This is like when hash values are not unique.
3. Every time you put the same crayon (like "Sunset Orange") into the machine, it will always give you the same number ("3").
```

### Suitability
- Hashed based indexes are best for equality selections.
- They cannot do range searches.

```
A **hashed based index** has a "narrow" search range because it's optimized for pinpointing an exact piece of data. It's like asking, "Where is the book titled 'Moby Dick'?" and getting a direct location.

On the other hand, it's not designed for a "wide" search range where you might ask, "Can you show me all books written by authors whose last name starts with an 'H'?" For such broader, range-based queries, other types of indexes or search methods are more suitable.
```

![[docs/Images/Pasted image 20230821132528.png]]

## Static Hashing
- Build `n` buckets and use the modulus operation to group the range of the hash values to buckets containing data entries
- Collisions over time can lead to many overflow buckets - degrading performance

![[docs/Images/Pasted image 20230821132636.png]]
### Static Hashing: The Post Office Analogy

Imagine a small post office with exactly `n` post boxes where people receive their mail.

1. **Building `n` Buckets**:
   - Think of each post box as a "bucket." The post office has a fixed number (`n`) of these buckets (or post boxes). So, maybe there are 10 post boxes numbered from 0 to 9.

2. **Using the Modulus Operation**:
   - To decide which post box a letter should go into, the post office looks at the last digit of the house number on the letter. If it's house number 123, the last digit is 3, so the letter goes into post box #3. 
   - This is like using the modulus operation! Taking a number (like a house number) and doing "mod 10" gives us a result between 0 and 9, which corresponds to one of our 10 post boxes. 

3. **Collisions**:
   - Now, imagine that there are a LOT of houses with numbers ending in 3. Over time, post box #3 gets really full, while some other post boxes might be almost empty. 
   - In our analogy, this is a "collision". In hashing, when two different pieces of data produce the same hash value (like two house numbers ending in 3), they both want to go into the same bucket, causing a "collision."

4. **Overflow Buckets**:
   - Since post box #3 is full, the post office brings in an extra, temporary box for the excess mail. This is an "overflow bucket." But if even more mail comes for houses with numbers ending in 3, we might need another overflow bucket, and then another.
   - The problem is, every time a letter arrives for a house number ending in 3, the mail carrier has to check the main post box, then the first overflow bucket, then the second, and so on. This can slow down the mail delivery process!

5. **Degrading Performance**:
   - As more overflow buckets are added, the efficiency of our system decreases. Instead of quickly placing mail in the right box, the mail carrier spends more time handling overflows. This is the "degrading performance" part. 

### Considerations

1. **Buckets can fill**: Just like our post boxes, if many data entries end up in the same bucket, it can get full.
  
2. **Expensive to double and copy**: If you decide to add more post boxes (or buckets) to alleviate the overcrowding, you'd need to rearrange and possibly rehash all the mail (or data entries). This is time-consuming and costly in terms of performance.
  
3. **Risk of large overflow chains**: If you don't expand the number of post boxes, and one gets too full, you have to add temporary storage (like our overflow buckets) in a chain-like manner.
  
4. **Searching is effective but maintenance is difficult**: It's easy to find a specific piece of mail if you know the house number, but managing the system, especially when post boxes overflow, becomes challenging.
  
5. **Conceptually simple**: The idea is straightforward – take a value, hash it, and put it in the right post box.
  
6. **Works well if the index isn't changing**: If the number of houses or amount of mail remains stable, this system is effective. Problems arise when there's a lot of new data or change.

### Extendible Hashing:

This is like an upgraded version of our post office system to deal with the problems of static hashing.

1. **Don't point directly to fixed blocks**: Instead of having a direct one-to-one relationship between the hash value and the post box, there's a middleman – a directory of pointers.
  
2. **Uses directory pointers to buckets**: Imagine there's a board in the post office with arrows (or pointers) showing where each type of mail should go. This board can change and adapt without needing to move the post boxes themselves.
  
3. **Double the size only of tables that overflowed**: If one area gets too full, you can just change the board's arrows to point to additional storage, rather than moving everything around.
  
4. **No overflow page**: Since we have this adaptable board of pointers, we can efficiently handle overcrowding without needing temporary storage boxes.
  
5. **Pointer-based so easier to manage**: Managing the system is simpler because the main job is updating the board of pointers, not moving all the mail.
  
6. **Skewed data can cause issues**: If there's a LOT of mail for certain house numbers and very little for others, the board of pointers can become very large, trying to manage all the special cases. In computer terms, if this table of pointers becomes too large, it might not fit in fast access memory (like RAM), which can slow down the system.

In essence, extendible hashing is a more flexible and adaptable system than static hashing. While it addresses many of the challenges of static hashing, it introduces its own set of considerations, especially when dealing with unevenly distributed data.


##### Examples

```
Why is the growth of the overflow chain going to impact in performance? You may assume that the database is growing and that the overflow size starts to rival the main buckets in size.
```

- Because we need to check more and more spaces in memory for 1 thing, instead of just a singular space, more I/O operations

```
To reduce the chances of collision, your db manager has created one table entry for each possible value of an MD5 hatch 

#1. Has this reduced the chances of collision? Be precise in your answer.
#2. If each entry is four bytes long, how large is this table?
#3. From an entropic entropic perspective, what can you say about the average information loss of any English language sentence that is 30 characters long?
```

1. Absolutely. If there are unique table entries for each possible value, then it's impossible for there to be any collisions. However, the hashing process itself might cause some collisions.

2. 
$$
\text{Size of the table} = \text{Number of possible hash values} \times \text{Size of each entry}
$$

$$
= 2^{128} \times 4 \text{ bytes}
$$

Given that:

$$
2^{128} \approx 3.4 \times 10^{38}
$$

The table size would be:

$$
\approx 1.36 \times 10^{39} \text{ bytes}
$$
3. 
Entropy, in this context, is a measure of the amount of information or randomness present in data. When you take an English sentence that is 30 characters long and convert it into an MD5 hash (which is always 16 bytes or 128 bits long), you're condensing a lot of information into a small space.

Considering that the possible combinations of 30-character English sentences are immensely vast, there's significant information loss when this is reduced to a 128-bit hash. Essentially, many different 30-character sentences would theoretically map to the same MD5 hash value (due to the pigeonhole principle), leading to loss of unique information about the original sentence.

In other words, from an entropic perspective, hashing a 30-character English sentence using MD5 results in significant average information loss, since a vast range of potential sentences is being mapped to a limited set of hash values.