> [!idea] Introduction to Dynamic Programming
>
> Dynamic Programming (DP) is an algorithmic technique that solves complex problems by breaking them down into simpler subproblems and storing the solutions to these subproblems to avoid redundant calculations.
>
> The key characteristics of problems that can be solved using Dynamic Programming are:
> 1. **Optimal Substructure:** The optimal solution to the overall problem can be constructed from the optimal solutions to its subproblems.
> 2. **Overlapping Subproblems:** The problem can be broken down into subproblems that are reused multiple times during the computation.
>
> Dynamic Programming is commonly used in optimization problems, where the goal is to find the best solution among all possible solutions. It is particularly useful when the problem exhibits optimal substructure and overlapping subproblems, as it can significantly reduce the time complexity compared to brute-force or recursive approaches.

> [!consider] Dynamic Programming vs. Other Approaches
>
> When solving problems, there are various approaches one can take, such as brute-force, recursion, and dynamic programming. Here's a comparison of these approaches:
>
> 1. **Brute-Force:**
>    - Brute-force algorithms explore all possible solutions to a problem exhaustively.
>    - They are straightforward to implement but can be extremely inefficient for large problem sizes.
>    - The time complexity of brute-force algorithms is often exponential.
>
> 2. **Recursion:**
>    - Recursive algorithms break down a problem into smaller subproblems and solve them recursively.
>    - Recursion can lead to elegant and concise solutions but may result in redundant calculations if the subproblems overlap.
>    - Without memoization or tabulation, recursive solutions can have exponential time complexity due to the redundant calculations.
>
> 3. **Dynamic Programming:**
>    - Dynamic Programming also breaks down a problem into subproblems but solves each subproblem only once and stores the solutions for future use.
>    - By avoiding redundant calculations, DP can significantly reduce the time complexity compared to brute-force and recursive approaches.
>    - DP is particularly effective when the problem has optimal substructure and overlapping subproblems.
>
> Dynamic Programming is often preferred over brute-force and recursive approaches when:
> - The problem has overlapping subproblems, and the recursive solution leads to redundant calculations.
> - The problem has an optimal substructure, and the solution can be constructed from the solutions to smaller subproblems.
> - The problem size is large, and a brute-force approach would be computationally infeasible.
>
> However, it's important to note that not all problems can be solved using Dynamic Programming. DP is applicable only when the problem exhibits the characteristics of optimal substructure and overlapping subproblems. In some cases, a brute-force or recursive approach might be sufficient or even more efficient if the problem size is small or the subproblems do not overlap significantly.

> [!example] Shortest Path Problem using Dynamic Programming
>
> One example of solving a shortest path problem using Dynamic Programming is the "Shortest Path in a Grid" problem. Consider a grid of size $m \times n$, where each cell contains a non-negative cost. The goal is to find the shortest path from the top-left cell to the bottom-right cell, where you can only move down or right at each step.
>
> Here's how we can solve this problem using Dynamic Programming:
>
> 1. Create a 2D DP table `dp` of size $m \times n$ to store the shortest path costs.
>
> 2. Initialize the base cases:
>    - `dp[0][0] = grid[0][0]` (cost of the top-left cell)
>    - `dp[i][0] = dp[i-1][0] + grid[i][0]` for `i > 0` (cost of the leftmost column)
>    - `dp[0][j] = dp[0][j-1] + grid[0][j]` for `j > 0` (cost of the topmost row)
>
> 3. Fill the DP table:
>    - For each cell `(i, j)` in the grid (excluding the base cases):
>      - `dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j]`
>      - The shortest path cost to reach cell `(i, j)` is the minimum of the shortest path costs from the cell above `(i-1, j)` and the cell to the left `(i, j-1)`, plus the cost of the current cell `grid[i][j]`.
>
> 4. Return `dp[m-1][n-1]`, which represents the shortest path cost from the top-left cell to the bottom-right cell.
>
> By using Dynamic Programming, we can solve this problem efficiently with a time complexity of $O(m \times n)$, as we fill the DP table once for each cell. The space complexity is also $O(m \times n)$ to store the DP table.
>
> This approach avoids the need for a brute-force exploration of all possible paths, which would have an exponential time complexity. It also eliminates the redundant calculations that would occur in a recursive solution without memoization.
>
> The "Shortest Path in a Grid" problem is just one example of how Dynamic Programming can be applied to solve shortest path problems efficiently. DP can be used to solve various other shortest path problems, such as the "Longest Common Subsequence" problem or the "Edit Distance" problem, by breaking them down into subproblems and storing the solutions to avoid redundant calculations.

