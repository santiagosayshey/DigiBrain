

> [!idea]+ Hash Tables
> 
> A hash table is a dynamic data structure that allows for **efficient insertion, deletion, and searching** of elements using a **unique key**. It achieves **constant average time $O(1)$** for these basic operations by using a **hash function** to map keys to indices in an underlying array. This makes hash tables **highly useful for maintaining and manipulating sets of data** where quick access is essential.
>
>
> > [!consider] Consider a Phone Book
> >
> > Imagine storing phone numbers in an array, where the index corresponds to the person's name (an integer). While this allows for immediate access $O(1)$, it becomes impractical for large ranges of names. For example, if the highest name is "1,000,000", you would need an array of size 1,000,001, even if only a few people are listed, resulting in wasted space.
> >
> > Hash tables solve this issue by using a hash function to map keys (e.g., strings) to indices in a smaller array, providing space-efficient storage and retrieval. This makes hash tables versatile and powerful, handling large key ranges without excessive memory overhead.
>
> <br>

> [!idea] Hash Functions
>
> A hash function is a crucial component of a hash table that **maps a given key to an index in the underlying array**. The goal is to distribute the keys evenly across the array. A well-designed hash function should be efficient to compute and provide a uniform distribution of keys.
>
> When a key is passed through the hash function, it generates a hash code, which is then mapped to an index within the bounds of the array. This allows for **direct access to the desired element without the need to search through the entire array**.
>
> **Example: Modulo Function**
>
> - The modulo function is a simple hash function that takes the remainder of the key divided by the size of the array.
> - Formula: `hash(key) = key % array_size`
> - Example: If the key is 42 and the array size is 10, the hash function would map the key to index 2 (42 % 10 = 2).
> 
> ![[Hash table.png]]


> [!consider] Collisions and Hashing Techniques
>
> Hash **collisions** occur when multiple keys map to the same array index. There are two main approaches to handle collisions: open hashing and closed hashing.
>
> 1. **Open Hashing** - Storing all elements with the same hash as 1 entry
>    - One approach is chaining - each array index points to a linked list.
>    - When a collision happens, the key-value pair is **appended to the linked list** at that index, allowing multiple pairs to be stored.
>    - Searching for a key involves **traversing the linked list** at the corresponding index to find the desired value.
>
>![[Group 2.png]]
>
> 2. **Closed Hashing** - Storing elements with the same hash in different table entries
>    - When a collision occurs, **probing** is used to find an alternative empty slot in the array. âŠ¥ is used to indicate empty slots that values can be placed in.
>    - Common probing techniques include:
>      - Linear Probing: Incrementally search for the next empty slot.
>      - Quadratic Probing: Use quadratic function to determine the next slot.
>      - Double Hashing: Use a secondary hash function to calculate the step size for probing.
>    - Searching for a key involves **probing the array** until the key is found or an empty slot is encountered.
>
>![[Group 10.png]]

 > [!idea]+ Hash Table Complexity
 > 
> | Operation | Average Time Complexity | Worst Case Time Complexity |
> |-----------|-------------------------|----------------------------|
> | Insertion | O(1)                    | O(n)                       |
> | Deletion  | O(1)                    | O(n)                       |
> | Search    | O(1)                    | O(n)                       |
>
> In the worst case, when all keys map to the same index (i.e., high number of collisions), the time complexity degrades to $O(n)$ as the linked list at that index needs to be traversed. However, with a well-designed hash function and appropriate collision resolution technique (e.g., chaining), the average case performance remains $O(1)$.

> [!application] Application: 2-SUM Problem
>
> - **Input**: An unsorted array `A` of `n` integers and a target sum `T`.
> - **Goal**: Determine whether or not there are two numbers `x` and `y` in `A` such that `x + y = T`.
> - **Naive solution**: Exhaustive search, which has a time complexity of $O(n^2)$.
> - **Better solution**: Sort the array `A` $O(n \cdot log n)$ and use binary search $O(\log n)$ to find the complement of each number.
> - **Best solution**: Use a hash table. For each element in the array, search for the difference of the target minus the current number. This operation takes constant time. By iterating through each number in the array, the overall time complexity becomes $O(n)$.




