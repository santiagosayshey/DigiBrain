

> [!idea]+ Hash Tables
> 
> A hash table is a dynamic data structure that allows for **efficient insertion, deletion, and searching** of elements using a **unique key**. It achieves **constant average time $O(1)$** for these basic operations by using a **hash function** to map keys to indices in an underlying array. This makes hash tables **highly useful for maintaining and manipulating sets of data** <font color="#ffc000">where quick access is essential</font>.
>
>
> > [!consider] Consider a Phone Book
> >
> > Imagine storing phone numbers in an array, where the index corresponds to the person's name (an integer). While this allows for immediate access $O(1)$, it becomes impractical for large ranges of names. For example, if the highest name is "1,000,000", you would need an array of size 1,000,001, even if only a few people are listed, resulting in wasted space.
> >
> > Hash tables solve this issue by using a hash function to map keys (e.g., strings) to indices in a smaller array, providing space-efficient storage and retrieval. This makes hash tables versatile and powerful, handling large key ranges without excessive memory overhead.
>
> <br>

> [!idea] Hash Functions
>
> A hash function **maps a given key to an index in the underlying array**. The goal is to distribute the keys evenly across the array. A well-designed hash function should be efficient to compute and provide a uniform distribution of keys.
>
> When a key is passed through the hash function, it generates a hash code, which is then mapped to an index within the bounds of the array. This allows for **direct access to the desired element without the need to search through the entire array**.
>
> **Example: Modulo Function**
>
> - The modulo function is a simple hash function that takes the remainder of the key divided by the size of the array.
> - Formula: `hash(key) = key % array_size`
> - Example: If the key is 42 and the array size is 10, the hash function would map the key to index 2 (42 % 10 = 2).
>
> ![[Pasted image 20240608120815.png]]


> [!consider] Collisions and Hashing Techniques
>
> Hash **collisions** occur when multiple keys map to the same array index. There are two main approaches to handle collisions: open hashing (chaining) and closed hashing (open addressing).
>
> 1. **Open Hashing / Closed Addressing / Hashing with Chaining**
>    - Elements with the same hash are stored as a linked list in the same array slot.
>    - **Advantages**: 
>      - Ensures referential integrity by maintaining a correct and navigable link for each key-value pair, despite collisions.
>      - The average length of the list at a slot is n/m, where n is the number of elements and m is the array size.
>    - **Disadvantages**: 
>      - Can lead to wasted space if the array size is much larger than the number of elements.
>      - Requires additional memory for the linked list pointers.
>
> 2. **Closed Hashing / Open Addressing**
>    - Elements with the same hash are stored in different array slots, determined by a probing sequence.
>    - **Advantages**: 
>      - Uses contiguous memory, which can be more cache-friendly.
>      - Does not require additional memory for pointers.
>    - **Disadvantages**:
>      - Can lead to clustering and longer probe sequences as the table fills up.
>      - Deletion is more complicated, as it requires marking slots as "deleted" to maintain the probing sequence.
>
> The choice between open hashing and closed hashing depends on the specific requirements of the application, such as memory constraints, expected load factor, and the cost of collisions.


> [!consider] Hash Table Operation Complexities
>
> | Case          | Complexity | Description                                                       |
> |---------------|------------|-------------------------------------------------------------------|
> | **Best Case**   | $O(1)$       | Direct access as no collisions occur.                             |
> | **Average Case**| $O(1)$       | Efficient handling with minimal collisions, assuming a good hash function and low load factor. |
> | **Worst Case**  | $O(n)$       | All keys hash to the same index, leading to a linked list (if chaining is used) or long probing sequences (in open addressing). |

> [!idea] **Load Factor and Rehashing**
> 
> The **load factor** of a hash table is a measure that indicates how full the hash table is. It's calculated as the ratio of the number of entries to the total number of slots available in the table. A high load factor means the table is getting full, leading to an increased chance of collisions and thus, a decrease in performance.
> 
> - **Formula:** Load Factor = (Number of Entries) / (Size of Hash Table)
> 
> **Rehashing** is the process of adjusting the size of the hash table and re-distributing the entries. This typically happens in two scenarios:
> 
> 1. **When the load factor exceeds a certain threshold**, indicating the table is too full. Expanding the table size and rehashing helps spread out the entries more thinly, reducing the chance of collisions.
> 
> 2. **When the table becomes too sparse**, usually after many deletions. This scenario is less common but can lead to unnecessary memory usage. Shrinking the table and rehashing makes it more space-efficient.
> 
> Rehashing involves creating a new hash table (usually of a larger size to accommodate more entries) and then re-inserting each existing entry. This process ensures that the hash table continues to operate efficiently, but it can be computationally expensive, affecting the time complexity of operations that trigger rehashing.
> 
> | Operation | Open Hashing (Chaining) | Closed Hashing (Open Addressing) |
> |-----------|-------------------------|----------------------------------|
> | Find      | O(1 + α)                | O(1 + α)                         |
> | Insert    | O(1 + α)                | O(1 + α)                         |
> | Delete    | O(1 + α)                | O(1 + α)                         |
>
> Note: α (alpha) represents the load factor, which is the ratio of the number of elements to the array size (n/m). In practice, the load factor is kept below a certain threshold (e.g., 0.75) to maintain good performance.


> [!idea] Open Hashing - Chaining
>
> Each array index points to a linked list.
> - When a collision happens, the key-value pair is **appended to the linked list** at that index, allowing multiple pairs to be stored.
> - Searching for a key involves **traversing the linked list** at the corresponding index to find the desired value.
> - Deleting a value requires adjusting the pointers in the linkedlist
>
> ![[Pasted image 20240608120834.png]]

> [!idea] Closed Hashing - Probing
>
> In closed hashing, when a collision occurs, **probing** is used to find an alternative empty slot in the array. ⊥ is used to indicate empty slots that values can be placed in.
>
> - Common probing techniques include:
>
> - Linear Probing: Incrementally search for the next empty slot.
>
> - Quadratic Probing: Use quadratic function to determine the next slot.
>
> - Double Hashing: Use a secondary hash function to calculate the step size for probing.
>
> **Insertion in Linear Probing:**
>
> - Start at the initial index obtained by applying the hash function.
>
> - If the slot is occupied, increment to the next available slot and place the key there.
>
> ```
> insert(e: Element)
>    1. Get index i = h(key(e))
>    2. If t[i] == ⊥, store e at t[i]  
>    3. If t[i] is not empty, increase i by 1 and go to step 2.
> ```
>
> ![[Pasted image 20240608120906.png]]
>
> **Search in Linear Probing:**
>
> - To search for a key in a hash table using linear probing, we start at the initial index obtained by applying the hash function to the key.
>
> - We compare the key at the current index with the key we are searching for.
>
> - If the keys match, we have found the desired key-value pair.
>
> - If the keys don't match, we linearly probe to the next index.
>
> ```
> search(k: Key)
>    1. Get index i = h(k)
>    2. If t[i] == ⊥, return null
>    3. If key(t[i]) == k, return t[i]
>    4. Increase i by 1 and go to step 2.
> ```
>
> ![[Pasted image 20240608120923.png]]
>
> **Deletion in Linear Probing:**
>
> When deleting a key-value pair in a hash table using linear probing, the process involves shifting elements to maintain the integrity of the probing sequence. Here's how the deletion process works:
>
> 1. Get the index `i` by applying the hash function `h` to the key `k`.
>
> 2. If the slot at index `i` is empty (denoted by `⊥`), the key is not present in the hash table, so we return.
>
> 3. If the element `e` at index `i` has a key different from `k`, we linearly probe to the next slot by incrementing `i` by 1 and go back to step 2.
>
> 4. If the key at index `i` matches `k`, we have found the key-value pair to be deleted. We set the slot at index `i` to `⊥` to mark it as empty.
>
> 5. We set the index `j` to `i+1` to start shifting elements.
>
> 6. If the slot at index `j` is empty (`⊥`), we have finished shifting and can return.
>
> 7. If the hash value of the element at index `j` is greater than `i`, it means the element at `j` was not probed to its current position and should not be shifted. We increment `j` by 1 and go back to step 6.
>
> 8. If the hash value of the element at index `j` is less than or equal to `i`, it means the element at `j` was probed to its current position and needs to be shifted. We move the element from index `j` to index `i` and set the slot at index `j` to `⊥`.
>
> 9. We update the index `i` to `j` and go back to step 5 to continue shifting elements.
>
> ```
> delete(k: Key)
>    1. Get index i = h(k)
>    2. If t[i] == ⊥, return
>    3. If key(t[i]) != k, increase i by 1 and go to step 2
>    4. Set t[i] = ⊥
>    5. Set j = i + 1
>    6. If t[j] == ⊥, return
>    7. If h(key(t[j])) > i, increase j by 1 and go to step 6
>    8. Move t[j] to t[i], set t[j] = ⊥
>    9. Set i = j and go to step 5
> ```
>
> ![[Pasted image 20240608120937.png]]




> [!application] Application: 2-SUM Problem
>
> - **Input**: An unsorted array `A` of `n` integers and a target sum `T`.
> - **Goal**: Determine whether or not there are two numbers `x` and `y` in `A` such that `x + y = T`.
> - **Naive solution**: Exhaustive search, which has a time complexity of $O(n^2)$.
> - **Better solution**: Sort the array `A` $O(n \cdot log n)$ and use binary search $O(\log n)$ to find the complement of each number.
> - **Best solution**: Use a hash table. For each element in the array, search for the difference of the target minus the current number. This operation takes constant time. By iterating through each number in the array, the overall time complexity becomes $O(n)$.


> [!idea] **Dynamic Resizing**
> 
> **Dynamic resizing** is a critical feature of modern hash table implementations, ensuring they can adapt to varying amounts of stored data while maintaining high performance and efficient space usage. This process is closely related to the concept of load factor and rehashing.
> 
> The main goals of dynamic resizing are:
> 
> - **Maintain optimal performance:** By adjusting the size of the hash table in response to changes in the number of stored elements, dynamic resizing helps maintain a low load factor, which is crucial for minimizing collisions and maintaining fast access times.
> 
> - **Efficient use of memory:** Dynamic resizing helps avoid excessive memory consumption when the number of elements is small, and ensures there's enough space to add more elements without significantly increasing the chance of collisions.
> 
> Implementations vary, but a common approach is to double the size of the hash table once the load factor exceeds a certain threshold (e.g., 0.75). Conversely, if a large number of elements are removed, reducing the size of the hash table can reclaim unused space.
> 
> While dynamic resizing is a powerful mechanism for maintaining efficiency, it's important to handle it carefully to minimize the computational cost of rehashing all keys during resize operations.