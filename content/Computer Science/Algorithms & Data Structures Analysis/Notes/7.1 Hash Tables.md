

> [!idea]+ Hash Tables
> 
> A hash table is a dynamic data structure that allows for **efficient insertion, deletion, and searching** of elements using a **unique key**. It achieves **constant average time $O(1)$** for these basic operations by using a **hash function** to map keys to indices in an underlying array. This makes hash tables **highly useful for maintaining and manipulating sets of data** <font color="#ffc000">where quick access is essential</font>.
>
>
> > [!consider] Consider a Phone Book
> >
> > Imagine storing phone numbers in an array, where the index corresponds to the person's name (an integer). While this allows for immediate access $O(1)$, it becomes impractical for large ranges of names. For example, if the highest name is "1,000,000", you would need an array of size 1,000,001, even if only a few people are listed, resulting in wasted space.
> >
> > Hash tables solve this issue by using a hash function to map keys (e.g., strings) to indices in a smaller array, providing space-efficient storage and retrieval. This makes hash tables versatile and powerful, handling large key ranges without excessive memory overhead.
>
> <br>


> [!idea] Open Hashing - Chaining
>
> Each array index points to a linked list.
> - When a collision happens, the key-value pair is **appended to the linked list** at that index, allowing multiple pairs to be stored.
> - Searching for a key involves **traversing the linked list** at the corresponding index to find the desired value.
> - Deleting a value requires adjusting the pointers in the linkedlist
>
> ![[Group 2.png]]

> [!idea] Closed Hashing - Probing
>
> In closed hashing, when a collision occurs, **probing** is used to find an alternative empty slot in the array. ⊥ is used to indicate empty slots that values can be placed in.
> - Common probing techniques include:
>   - Linear Probing: Incrementally search for the next empty slot.
>   - Quadratic Probing: Use quadratic function to determine the next slot.
>   - Double Hashing: Use a secondary hash function to calculate the step size for probing.
>
>**Insertion in Linear Probing:**
>- Start at the initial index obtained by applying the hash function.
>- If the slot is occupied, increment the the next available slot and place the key there.
>
> ![[Group 10.png]]
>
> **Search in Linear Probing:**
>
> - To search for a key in a hash table using linear probing, we start at the initial index obtained by applying the hash function to the key.
> - We compare the key at the current index with the key we are searching for.
> - If the keys match, we have found the desired key-value pair.
> - If the keys don't match, we linearly probe to the next index.
> 
> ![[Group 12.png]]
>
> **Deletion in Linear Probing:**
> 
When deleting a key-value pair in a hash table using linear probing, the process involves shifting elements to maintain the integrity of the probing sequence. Here's how the deletion process works:
>1. Get the index `i` by applying the hash function `h` to the key `k`.
>2. If the slot at index `i` is empty (denoted by `⊥`), the key is not present in the hash table, so we return.
>3. If the element `e` at index `i` has a key different from `k`, we linearly probe to the next slot by incrementing `i` by 1 and go back to step 2.
>4. If the key at index `i` matches `k`, we have found the key-value pair to be deleted. We set the slot at index `i` to `⊥` to mark it as empty.
>5. We set the index `j` to `i+1` to start shifting elements.
>6. If the slot at index `j` is empty (`⊥`), we have finished shifting and can return.
>7. If the hash value of the element at index `j` is greater than `i`, it means the element at `j` was not probed to its current position and should not be shifted. We increment `j` by 1 and go back to step 6.
>8. If the hash value of the element at index `j` is less than or equal to `i`, it means the element at `j` was probed to its current position and needs to be shifted. We move the element from index `j` to index `i` and set the slot at index `j` to `⊥`.
>9. We update the index `i` to `j` and go back to step 5 to continue shifting elements.
>
>![[Group 13 1.png]]




 > [!idea]+ Hash Table Complexity
 > 
> | Operation | Average Time Complexity | Worst Case Time Complexity |
> |-----------|-------------------------|----------------------------|
> | Insertion | O(1)                    | O(n)                       |
> | Deletion  | O(1)                    | O(n)                       |
> | Search    | O(1)                    | O(n)                       |
>
> In the worst case, when all keys map to the same index (i.e., high number of collisions), the time complexity degrades to $O(n)$ as the linked list at that index needs to be traversed. However, with a well-designed hash function and appropriate collision resolution technique (e.g., chaining), the average case performance remains $O(1)$.

> [!application] Application: 2-SUM Problem
>
> - **Input**: An unsorted array `A` of `n` integers and a target sum `T`.
> - **Goal**: Determine whether or not there are two numbers `x` and `y` in `A` such that `x + y = T`.
> - **Naive solution**: Exhaustive search, which has a time complexity of $O(n^2)$.
> - **Better solution**: Sort the array `A` $O(n \cdot log n)$ and use binary search $O(\log n)$ to find the complement of each number.
> - **Best solution**: Use a hash table. For each element in the array, search for the difference of the target minus the current number. This operation takes constant time. By iterating through each number in the array, the overall time complexity becomes $O(n)$.


> [!idea] **Dynamic Resizing**
> 
> **Dynamic resizing** is a critical feature of modern hash table implementations, ensuring they can adapt to varying amounts of stored data while maintaining high performance and efficient space usage. This process is closely related to the concept of load factor and rehashing.
> 
> The main goals of dynamic resizing are:
> 
> - **Maintain optimal performance:** By adjusting the size of the hash table in response to changes in the number of stored elements, dynamic resizing helps maintain a low load factor, which is crucial for minimizing collisions and maintaining fast access times.
> 
> - **Efficient use of memory:** Dynamic resizing helps avoid excessive memory consumption when the number of elements is small, and ensures there's enough space to add more elements without significantly increasing the chance of collisions.
> 
> Implementations vary, but a common approach is to double the size of the hash table once the load factor exceeds a certain threshold (e.g., 0.75). Conversely, if a large number of elements are removed, reducing the size of the hash table can reclaim unused space.
> 
> While dynamic resizing is a powerful mechanism for maintaining efficiency, it's important to handle it carefully to minimize the computational cost of rehashing all keys during resize operations.