> [!motivation] The Need for Virtual Memory
> Modern computer systems must balance a critical challenge:
> - **Programs rely on memory addresses:** In a 64-bit system, programs can theoretically access up to 16 exabytes of memory, a vast address space.
> - **Physical memory (RAM) is limited:** Actual physical memory is far smaller, typically measured in gigabytes, creating a significant gap between potential and available resources.
>
> Without memory virtualization, several problems arise:
> - **Single-process limitation:** Without virtualization, only one process could occupy physical memory at a time, severely restricting multitasking. Multiple programs would risk overwriting each other's data, leading to instability and crashes.
> - **Inefficient memory usage:** Programs might demand large blocks of memory, leading to fragmentation and inefficient use of limited physical resources.
> - **Lack of protection:** All programs would directly access physical memory, increasing the risk of interference between programs, potentially causing security vulnerabilities and system crashes.

> [!idea] Goals of Virtual Memory Systems
> Virtual memory creates an abstraction layer between programs and physical memory, addressing several key objectives:
> 
> 1. **Transparency:**
>    - Programs operate as if they have access to the entire address space
>    - Each process sees a full, contiguous memory range (e.g., 64-bit address space)
> 
> 2. **Protection:**
>    - Prevents processes from corrupting other processes' data
>    - Operating system enforces memory access permissions
> 
> 3. **Privacy:**
>    - Processes cannot read data that doesn't belong to them
>    - Each process has its own isolated address space
> 
> 4. **Efficiency:**
>    - Minimizes waste of physical memory resources
>    - Reduces fragmentation through paging and sophisticated allocation strategies
>    - Allows overcommitment of memory, utilizing disk space when necessary
> 
> 5. **Sharing:**
>    - Enables controlled sharing of memory between processes when needed
>    - Facilitates efficient use of shared libraries and inter-process communication

> [!consider] Time-Shared Memory: A Historical Approach
> A memory management system that divides physical memory into fixed partitions of time, assigning each to a different process and rapidly switching between them.
> 
> **Concept:**
> - Divide physical memory into fixed partitions
> - Assign each partition to a different process
> - Rapidly switch between processes, giving each a "time slice" of CPU and memory access
> 
> **Potential benefits:**
> - **Simplicity:** No need for complex address translation
> - **Multitasking:** Allows multiple processes to run concurrently
> - **Protection:** Processes are isolated in their own partitions
> 
> **Why it's not used today:**
> 1. **Inflexibility:** Fixed partitions can't adapt to varying process memory needs
> 2. **Inefficiency:** Large processes might not fit in a partition, while small ones waste space
> 3. **Limited processes:** Number of active processes limited by number of partitions
> 4. **Fragmentation:** Both internal (within partitions) and external (between partitions)
> 5. **Lack of virtual memory benefits:** No overcommitment, paging, or efficient sharing
> 

> [!consider] Static Relocation: A Step Forward
> A memory allocation technique where the **compiler generates code with relative addresses, and the loader adjusts these addresses based on where the program is loaded in memory, allowing each process to have a contiguous block of memory sized** to its needs.
> 
> **Concept:**
> - Compiler generates code with relative addresses
> - Loader adjusts these addresses based on where the program is loaded in memory
> - Each process gets a contiguous block of memory, sized to its needs
> 
> **Advantages over time-shared memory:**
> 1. **Flexibility:** Can accommodate varying process sizes
> 2. **Efficiency:** Less internal fragmentation within process memory blocks
> 3. **More processes:** Not limited by a fixed number of partitions
> 4. **Better resource utilization:** Memory allocated based on actual process needs
> 
> **Why it's not used in modern systems:**
> 1. **External fragmentation:** As processes are loaded and unloaded, memory becomes fragmented
> 2. **Lack of runtime flexibility:** Difficult to expand process memory after initial allocation
> 3. **No memory overcommitment:** Can't allocate more memory than physically available
> 4. **Complexity in shared libraries:** Challenges in using shared code at different addresses for different processes
> 5. **Limited protection:** While better than time-sharing, still lacks fine-grained memory protection


> [!idea] Dynamic Relocation: Modern Memory Management
> A memory management approach that uses hardware support for real-time address translation, **dividing memory into fixed-size pages and maintaining a page table for each process to map virtual addresses to physical memory locations.**
>
> **Concept:**
> - Uses hardware support (Memory Management Unit - MMU) for address translation
> - Divides memory into fixed-size pages (usually 4KB)
> - Maintains a page table for each process, mapping virtual to physical pages
>
> **Key features:**
> 1. **On-the-fly translation:** Converts virtual addresses to physical addresses in real-time
> 2. **Paging:** Allows non-contiguous physical memory allocation
> 3. **Demand paging:** Loads pages into memory only when accessed
> 4. **Page protection:** Fine-grained control over read, write, and execute permissions
>
> **Advantages:**
> - **Flexibility:** Easily accommodates varying and changing process sizes
> - **Efficiency:** Minimizes fragmentation through paging
> - **Overcommitment:** Can allocate more virtual memory than physically available
> - **Protection:** Robust isolation between processes
> - **Sharing:** Efficient sharing of memory pages (e.g., for shared libraries)
>
> **Challenges:**
> - **Complexity:** Requires sophisticated hardware and software support
> - **Performance overhead:** Address translation can impact speed (mitigated by TLB)
>
> Key point: Dynamic relocation forms the foundation of modern virtual memory systems, providing the flexibility, efficiency, and protection required in contemporary computing environments.


> [!idea] Stacks and Heaps in Virtual Memory
> Within a program's virtual address space, memory is typically organized into stacks and heaps, each with distinct characteristics and interaction methods:
> 
> | Aspect | Stack | Heap |
> |--------|-------|------|
> | **Purpose** | Stores local variables, function parameters, return addresses | Stores dynamically allocated memory |
> | **Allocation** | Automatic, managed by compiler | Manual (in languages like C) or garbage-collected |
> | **Lifetime** | Short-lived, tied to function scope | Persists until explicitly freed or program ends |
> | **Size** | Fixed, determined at compile time | Flexible, can grow or shrink at runtime |
> | **Speed** | Fast allocation and deallocation | Slower allocation and deallocation |
> | **Order** | Last-In-First-Out (LIFO) | No specific order |
> | **Fragmentation** | No fragmentation | Can suffer from fragmentation |
> | **Typical usage** | Local variables, function calls | Objects, large data structures |
> | **Interaction** | At high level: automatic<br>At low level: assembly instructions (push, pop) | In code: functions like malloc(), free() (C)<br>Or new, delete (C++) |
> 
> **Key points:**
> - Stacks are managed automatically at the high level, but compiled code uses push/pop operations
> - Heap memory is explicitly managed in code using allocation and deallocation functions
> - Both exist within the program's virtual address space
> - The operating system manages the translation of these virtual addresses to physical memory locations
> - **Fragmentation:** Stacks avoid fragmentation due to their LIFO nature, while heaps can suffer from fragmentation due to random allocation and deallocation patterns


> [!example] Stack in Action
> Let's consider a simple C function call and how it affects the stack:
> 
> ```c
> int add(int a, int b) {
>     int result = a + b;
>     return result;
> }
> 
> int main() {
>     int x = 5;
>     int y = 3;
>     int sum = add(x, y);
>     return 0;
> }
> ```
> 
> Stack operations during execution:
> 
> 1. `main()` is called:
>    - Push return address
>    - Allocate space for `x` and `y`
> 
> 2. Before calling `add(x, y)`:
>    - Push parameters (y, then x) in reverse order
>    - Push return address
> 
> 3. Inside `add()`:
>    - Push old base pointer
>    - Move stack pointer to new base pointer
>    - Allocate space for `result`
> 
> 4. After `add()` completes:
>    - Return value stored in register
>    - Pop stack back to `main()`'s frame
>    - Store return value in `sum`
> 
> ```
> High memory addresses
> +-------------------+
> | return addr (main)|
> | y (3)             |
> | x (5)             |
> | return addr (add) |
> | old base pointer  |
> | result (8)        |
> +-------------------+
> Low memory addresses
> ```
> 
> This example shows how the stack grows and shrinks as functions are called and return, demonstrating its Last-In-First-Out (LIFO) nature and role in managing function execution.

> [!example] Memory Allocation in a C Function
> Consider this simple C function:
>
> ```c
> static int global_count = 0;
>
> void process_data(int* data, int size) {
>     static int call_count = 0;
>     int local_sum = 0;
>     int* dynamic_array = malloc(size * sizeof(int));
>
>     call_count++;
>     for (int i = 0; i < size; i++) {
>         local_sum += data[i];
>         dynamic_array[i] = data[i] * 2;
>     }
>     global_count += local_sum;
>
>     free(dynamic_array);
> }
> ```
>
> Memory allocation for various elements:
>
> | Element | Memory Location | Notes |
> |---------|-----------------|-------|
> | `global_count` | Data segment | Static global variable |
> | `process_data` | Text segment | Function code |
> | `call_count` | Data segment | Static local variable |
> | `data` | Stack | Pointer to input data |
> | `size` | Stack | Function parameter |
> | `local_sum` | Stack | Local variable |
> | `dynamic_array` | Stack | Pointer to heap memory |
> | `*dynamic_array` | Heap | Dynamically allocated array |
> | `i` | Stack | Loop counter |
>
> Key points:
> - Stack variables are deallocated when the function returns
> - Heap memory persists until explicitly freed
> - Static variables (global and local) persist throughout program execution

