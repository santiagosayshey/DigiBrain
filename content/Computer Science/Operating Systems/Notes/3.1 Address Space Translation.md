> [!motivation] The Need for Virtual Memory
> Modern computer systems must balance a critical challenge:
> - **Programs rely on memory addresses:** In a 64-bit system, programs can theoretically access up to 16 exabytes of memory, a vast address space.
> - **Physical memory (RAM) is limited:** Actual physical memory is far smaller, typically measured in gigabytes, creating a significant gap between potential and available resources.
>
> Without memory virtualization, several problems arise:
> - **Single-process limitation:** Without virtualization, only one process could occupy physical memory at a time, severely restricting multitasking. Multiple programs would risk overwriting each other's data, leading to instability and crashes.
> - **Inefficient memory usage:** Programs might demand large blocks of memory, leading to fragmentation and inefficient use of limited physical resources.
> - **Lack of protection:** All programs would directly access physical memory, increasing the risk of interference between programs, potentially causing security vulnerabilities and system crashes.

> [!idea] Goals of Virtual Memory Systems
> Virtual memory creates an abstraction layer between programs and physical memory, addressing several key objectives:
> 
> 1. **Transparency:**
>    - Programs operate as if they have access to the entire address space
>    - Each process sees a full, contiguous memory range (e.g., 64-bit address space)
> 
> 2. **Protection:**
>    - Prevents processes from corrupting other processes' data
>    - Operating system enforces memory access permissions
> 
> 3. **Privacy:**
>    - Processes cannot read data that doesn't belong to them
>    - Each process has its own isolated address space
> 
> 4. **Efficiency:**
>    - Minimizes waste of physical memory resources
>    - Reduces fragmentation through paging and sophisticated allocation strategies
>    - Allows overcommitment of memory, utilizing disk space when necessary
> 
> 5. **Sharing:**
>    - Enables controlled sharing of memory between processes when needed
>    - Facilitates efficient use of shared libraries and inter-process communication
> 
> **Key mechanism:**
> - **Address translation:** Converts virtual addresses to physical addresses
> - Managed by the OS and hardware (MMU), transparent to applications
> 
> **Outcome:** Programs operate under the illusion of vast, private memory, while the system efficiently manages and protects limited physical resources.


> [!consider] Implementing Virtual Memory
> To implement virtual memory, systems employ several key mechanisms:
> 
> - **Page Tables:** Data structures that map virtual addresses to physical addresses
>   - Maintained by the operating system for each process
>   - Contain entries for each "page" of memory (typically 4KB blocks)
> 
> - **Translation Lookaside Buffer (TLB):** 
>   - Hardware cache for recent address translations
>   - Speeds up the translation process for frequently accessed memory locations
> 
> - **Memory Management Unit (MMU):**
>   - Hardware component that performs the actual translation
>   - Uses page tables and TLB to convert virtual addresses to physical addresses
> 
> - **Page Faults:**
>   - Occur when a program accesses a virtual address not currently in physical memory
>   - Trigger the operating system to load the required data from disk into RAM
> 
> - **Swapping:**
>   - Process of moving less frequently used memory pages to disk
>   - Frees up physical memory for more immediate needs
> 
> These mechanisms work together to create the illusion of a large, contiguous memory space for each process while efficiently managing the limited physical resources.


> [!idea] Stacks and Heaps in Virtual Memory
> Within a program's virtual address space, memory is typically organized into stacks and heaps, each with distinct characteristics and interaction methods:
> 
> | Aspect | Stack | Heap |
> |--------|-------|------|
> | **Purpose** | Stores local variables, function parameters, return addresses | Stores dynamically allocated memory |
> | **Allocation** | Automatic, managed by compiler | Manual (in languages like C) or garbage-collected |
> | **Lifetime** | Short-lived, tied to function scope | Persists until explicitly freed or program ends |
> | **Size** | Fixed, determined at compile time | Flexible, can grow or shrink at runtime |
> | **Speed** | Fast allocation and deallocation | Slower allocation and deallocation |
> | **Order** | Last-In-First-Out (LIFO) | No specific order |
> | **Fragmentation** | No fragmentation | Can suffer from fragmentation |
> | **Typical usage** | Local variables, function calls | Objects, large data structures |
> | **Interaction** | At high level: automatic<br>At low level: assembly instructions (push, pop) | In code: functions like malloc(), free() (C)<br>Or new, delete (C++) |
> 
> **Key points:**
> - Stacks are managed automatically at the high level, but compiled code uses push/pop operations
> - Heap memory is explicitly managed in code using allocation and deallocation functions
> - Both exist within the program's virtual address space
> - The operating system manages the translation of these virtual addresses to physical memory locations
> - **Fragmentation:** Stacks avoid fragmentation due to their LIFO nature, while heaps can suffer from fragmentation due to random allocation and deallocation patterns


> [!example] Stack in Action
> Let's consider a simple C function call and how it affects the stack:
> 
> ```c
> int add(int a, int b) {
>     int result = a + b;
>     return result;
> }
> 
> int main() {
>     int x = 5;
>     int y = 3;
>     int sum = add(x, y);
>     return 0;
> }
> ```
> 
> Stack operations during execution:
> 
> 1. `main()` is called:
>    - Push return address
>    - Allocate space for `x` and `y`
> 
> 2. Before calling `add(x, y)`:
>    - Push parameters (y, then x) in reverse order
>    - Push return address
> 
> 3. Inside `add()`:
>    - Push old base pointer
>    - Move stack pointer to new base pointer
>    - Allocate space for `result`
> 
> 4. After `add()` completes:
>    - Return value stored in register
>    - Pop stack back to `main()`'s frame
>    - Store return value in `sum`
> 
> ```
> High memory addresses
> +-------------------+
> | return addr (main)|
> | y (3)             |
> | x (5)             |
> | return addr (add) |
> | old base pointer  |
> | result (8)        |
> +-------------------+
> Low memory addresses
> ```
> 
> This example shows how the stack grows and shrinks as functions are called and return, demonstrating its Last-In-First-Out (LIFO) nature and role in managing function execution.

> [!example] Memory Allocation in a C Function
> Consider this simple C function:
>
> ```c
> static int global_count = 0;
>
> void process_data(int* data, int size) {
>     static int call_count = 0;
>     int local_sum = 0;
>     int* dynamic_array = malloc(size * sizeof(int));
>
>     call_count++;
>     for (int i = 0; i < size; i++) {
>         local_sum += data[i];
>         dynamic_array[i] = data[i] * 2;
>     }
>     global_count += local_sum;
>
>     free(dynamic_array);
> }
> ```
>
> Memory allocation for various elements:
>
> | Element | Memory Location | Notes |
> |---------|-----------------|-------|
> | `global_count` | Data segment | Static global variable |
> | `process_data` | Text segment | Function code |
> | `call_count` | Data segment | Static local variable |
> | `data` | Stack | Pointer to input data |
> | `size` | Stack | Function parameter |
> | `local_sum` | Stack | Local variable |
> | `dynamic_array` | Stack | Pointer to heap memory |
> | `*dynamic_array` | Heap | Dynamically allocated array |
> | `i` | Stack | Loop counter |
>
> Key points:
> - Stack variables are deallocated when the function returns
> - Heap memory persists until explicitly freed
> - Static variables (global and local) persist throughout program execution

