> [!motivation] The Need for Process Management
> In modern computing environments, **multiple programs often need to run concurrently on a single processor**. For example:
> - A user might be browsing the web while a background application checks for updates
> - An operating system needs to manage various system tasks while running user applications
> - A server must handle requests from multiple clients simultaneously
> 
> This creates a challenge: **How can a single processor efficiently handle multiple tasks**, giving each the illusion of continuous execution? 
> 
> While multi-core processors can run truly parallel tasks, even these systems often have **more active processes than available cores**. This necessitates a system for managing which code runs when, and for how long.

> [!idea] Dispatch: Switching Between Processes
> Dispatching is the **mechanism that switches the CPU from one process to another**. It involves:
> 
> - **Saving the state** of the currently running process
> - **Loading the state** of the next process to run
> - **Transferring control** to that process
> 
> Key components:
> 1. **Context Switch**: Saving and loading process states
> 2. **Dispatch Latency**: Time taken to stop one process and start another
> 
> Dispatching enables the **illusion of concurrent execution** on a single processor by rapidly switching between processes.


> [!consider] Dispatcher Control and Process State
> **How does the dispatcher gain control?**
> The dispatcher can gain control through several mechanisms:
> 
> 1. **Voluntary Yielding**: A process voluntarily gives up the CPU by calling a yield() function
> 2. **Blocking System Calls**: When a process makes a system call that causes it to block (e.g., waiting for I/O)
> 3. **Timer Interrupts**: For preemptive scheduling, a timer interrupt allows the OS to regain control
> 4. **I/O Completion**: When an I/O operation completes, the OS may decide to schedule the waiting process
> 
> **What must be saved and restored?**
> When switching between processes, the dispatcher must manage:
> 
> - **CPU Registers**: Program Counter (PC), Stack Pointer (SP), general-purpose registers
> - **Process State**: Running, Ready, Blocked, etc.
> - **Scheduling Information**: Priority, time quantum used, etc.
> - **Memory Management Info**: Page table pointers, memory limits
> - **I/O Status**: Information about ongoing I/O operations
> 
> This information is typically stored in a **Process Control Block (PCB)** or similar structure. The dispatcher saves the state of the current process to its PCB and loads the state of the next process from its PCB.


> [!example] Dispatch in Action
> Consider two processes: A (text editor) and B (web browser)
> 
> 1. Process A is running
> 2. Dispatcher decides to switch to B
> 3. Dispatcher **saves A's state** (registers, program counter, etc.)
> 4. Dispatcher **loads B's previously saved state**
> 5. Execution resumes with Process B
> 
> This happens many times per second, creating the illusion of simultaneous execution.

> [!idea] Scheduling: Deciding Which Process Runs Next
> Scheduling is the **strategy used to determine which process should be dispatched to run next**. It involves:
> 
> - Maintaining a **queue of ready-to-run processes**
> - **Selecting the next process** based on a scheduling algorithm
> - **Balancing system objectives** (e.g., fairness, throughput, response time)
> 
> Key concepts:
> 1. **Scheduling Algorithms**: Methods for selecting the next process (e.g., Round Robin, Priority Scheduling)
> 2. **Preemption**: Ability to interrupt a running process to schedule another
> 
> Scheduling aims to **optimize system performance and ensure fair allocation** of CPU time among processes.

> [!example] Scheduling in Practice
> Consider three processes: A, B, and C, with a Round Robin scheduler
> 
> 1. Scheduler **starts with A**, gives it a time slice
> 2. After time slice expires, **A is preempted, B is scheduled**
> 3. B runs for its time slice, then **C is scheduled**
> 4. After C's time slice, the **cycle repeats with A**
> 
> This ensures each process gets regular CPU time, preventing any single process from monopolizing the processor.

