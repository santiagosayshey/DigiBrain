> [!motivation] The Need for Process Management
> In modern computing environments, **multiple programs often need to run concurrently on a single processor**. For example:
> - A user might be browsing the web while a background application checks for updates
> - An operating system needs to manage various system tasks while running user applications
> - A server must handle requests from multiple clients simultaneously
> 
> This creates a challenge: **How can a single processor efficiently handle multiple tasks**, giving each the illusion of continuous execution? 
> 
> While multi-core processors can run truly parallel tasks, even these systems often have **more active processes than available cores**. This necessitates a system for managing which code runs when, and for how long.

> [!idea] Dispatch: Switching Between Processes
> Dispatching is the **mechanism that switches the CPU from one process to another**. It involves:
> 
> - **Saving the state** of the currently running process
> - **Loading the state** of the next process to run
> - **Transferring control** to that process
> 
> Key components:
> 1. **Context Switch**: Saving and loading process states
> 2. **Dispatch Latency**: Time taken to stop one process and start another
> 
> Dispatching enables the **illusion of concurrent execution** on a single processor by rapidly switching between processes.

> [!consider] Dispatcher as Traffic Controller
> Imagine the **CPU as a busy intersection** and the **dispatcher as a traffic controller**. Here's how this analogy helps us understand dispatching:
> 
> **How does the dispatcher gain control?**
> The traffic controller (dispatcher) steps in when:
> - **Traffic light changes** (Timer interrupts): Every few seconds, the light changes, allowing the controller to redirect traffic
> - **Emergency vehicle needs to pass** (System calls or I/O interrupts): The controller temporarily halts all traffic to let it through
> - **A lane is empty** (Voluntary yields): If a lane (process) finishes, the controller can immediately direct new traffic
> 
> **What must be saved and restored?**
> Think of each vehicle (process) having a **detailed information card**:
> - **Current position** (CPU registers): Exactly where the vehicle is in the intersection
> - **Destination and route** (Process state): Where it's going and how it plans to get there
> - **Cargo manifest** (I/O state): What the vehicle is carrying or waiting for
> - **Vehicle status** (Processor flags): Is it an emergency vehicle, a heavy truck, etc.?
> 
> When the traffic controller stops a vehicle, they take this card and store it. When it's time for that vehicle to move again, they use the card to **place it back exactly where it was**, facing the right direction, with all its cargo intact.
> 
> Just as smooth traffic flow depends on the controller's ability to quickly and accurately manage these transitions, efficient CPU utilization relies on the dispatcher's ability to swiftly switch between processes while maintaining their states.


> [!example] Dispatch in Action
> Consider two processes: A (text editor) and B (web browser)
> 
> 1. Process A is running
> 2. Dispatcher decides to switch to B
> 3. Dispatcher **saves A's state** (registers, program counter, etc.)
> 4. Dispatcher **loads B's previously saved state**
> 5. Execution resumes with Process B
> 
> This happens many times per second, creating the illusion of simultaneous execution.

> [!idea] Scheduling: Deciding Which Process Runs Next
> Scheduling is the **strategy used to determine which process should be dispatched to run next**. It involves:
> 
> - Maintaining a **queue of ready-to-run processes**
> - **Selecting the next process** based on a scheduling algorithm
> - **Balancing system objectives** (e.g., fairness, throughput, response time)
> 
> Key concepts:
> 1. **Scheduling Algorithms**: Methods for selecting the next process (e.g., Round Robin, Priority Scheduling)
> 2. **Preemption**: Ability to interrupt a running process to schedule another
> 
> Scheduling aims to **optimize system performance and ensure fair allocation** of CPU time among processes.

> [!example] Scheduling in Practice
> Consider three processes: A, B, and C, with a Round Robin scheduler
> 
> 1. Scheduler **starts with A**, gives it a time slice
> 2. After time slice expires, **A is preempted, B is scheduled**
> 3. B runs for its time slice, then **C is scheduled**
> 4. After C's time slice, the **cycle repeats with A**
> 
> This ensures each process gets regular CPU time, preventing any single process from monopolizing the processor.

