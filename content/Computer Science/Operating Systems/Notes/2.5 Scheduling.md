
> [!idea] Scheduling: Deciding Which Process Runs Next
> Scheduling is the **strategy used to determine which process should be dispatched to run next**. It involves:
> 
> - Maintaining a **queue of ready-to-run processes**
> - **Selecting the next process** based on a scheduling algorithm
> - **Balancing system objectives** (e.g., fairness, throughput, response time, turnaround time, waiting time, utilisation, overhead)
> 
> Key concepts:
> 1. **Scheduling Algorithms**: Methods for selecting the next process (e.g., Round Robin, Priority Scheduling)
> 2. **Preemption**: Ability to interrupt a running process to schedule another
> 
> Scheduling aims to **optimize system performance and ensure fair allocation** of CPU time among processes.

> [!example] General Scheduling
> Consider three processes: A, B, and C, with a Round Robin scheduler
> 
> 1. Scheduler **starts with A**, gives it a time slice
> 2. After time slice expires, **A is preempted, B is scheduled**
> 3. B runs for its time slice, then **C is scheduled**
> 4. After C's time slice, the **cycle repeats with A**
> 
> This ensures each process gets regular CPU time, preventing any single process from monopolizing the processor.

> [!idea] State Transitions in Process Scheduling
> State transitions refer to the changes in the status of a process as it moves through different stages of its lifecycle. This includes:
> 
> - **Ready State**: Process is ready to run but waiting for CPU time
> - **Running State**: Process is currently being executed by the CPU
> - **Blocked State**: Process is waiting for an event (e.g., I/O completion)
> - **Terminated State**: Process has finished execution
> 
> These transitions are crucial for efficient process management and scheduling.

> [!idea] Key Scheduling Concepts
> 
> 1. **Workload**: The set of processes that need to be scheduled
> 2. **Job**: A single unit of work that needs to be processed
> 3. **Scheduler**: The algorithm or mechanism that decides which job runs next
> 4. **Metric**: Criteria used to evaluate the performance of the scheduling algorithm (e.g., turnaround time, response time, waiting time)
>
> - **Turnaround Time**: Time from job submission to job completion
> - **Response Time**: Time from job submission to first response
> - **Waiting Time**: Total time a job spends in the ready queue
> - **Throughput**: Number of jobs completed per unit time
> - **Resource Utilization**: Percentage of time resources are busy
> - **Overhead**: Extra time consumed in managing the processes
> - **Fairness**: Equal distribution of CPU time among processes

> [!consider] Evaluating Scheduling Algorithms
> When evaluating scheduling algorithms, we consider:
> 
> - **Arrival Time**: When a job arrives in the system
> - **Run Time**: How long a job needs to run
> 
> We use various schedulers like FIFO, SJK, STCF, and RR, comparing the turnaround and response times for each job under each scheduler.
> 
> **Assumptions**:
> - Same running time for each job
> - All jobs arrive at the same time
> - No I/O operations
> - Determined run time

> [!example] Evaluating Scheduling Algorithms
> This example illustrates how different scheduling algorithms perform under various conditions.
> 
> **FIFO (First-In-First-Out)**
> - Description: Jobs executed in the order they arrive.
> - Scenario: Jobs A, B, C with run times 10, 10, 10 units, arriving simultaneously.
> 
> ![[Pasted image 20240804232600.png]]
> 
> **FIFO with Different Running Times**
> - Removed Assumption: Jobs have the same running time.
> - Issue: Convoy Effect â€“ Short jobs wait for long jobs to complete.
> - Scenario: Jobs A, B, C with run times 100, 10, 10 units, arriving simultaneously.
> 
> ![[Pasted image 20240804232643.png]]
> 
> **SJF (Shortest Job First)**
> - Description: Jobs executed in order of their length, shortest job first.
> - Scenario: Jobs A, B, C with run times 100, 10, 10 units, arriving simultaneously.
>   
> ![[Pasted image 20240804232706.png]]
> 
> **SJF with Different Arrival Times**
> - Removed Assumption: All jobs arrive at the same time.
> - Issue: Smaller jobs arriving after a larger job face delays.
> - Scenario: Jobs A, B, C with run times 80, 10, 20 units. A arrives at 0, B at 10, C at 20.
>   
> ![[Pasted image 20240804232726.png]]
> 
> **STCF (Shortest Time-to-Completion First)**
> - Description: Preempts longer jobs when shorter jobs arrive.
> - Removed Assumptions: All jobs arrive at the same time, jobs cannot be preempted.
> - Scenario: Jobs A, B, C with run times 100, 10, 10 units. A arrives at 0, B at 10, C at 20.
> 
> ![[Pasted image 20240804232754.png|500]]
> 
> **RR (Round Robin)**
> - Description: Each job gets a fixed time slice (quantum) in cyclic order.
> - Focus: Improved response time compared to STCF.
> - Scenario: Jobs A, B, C with run times 100, 20, 30 units. All jobs arrive at time 0. Quantum = 10 units.
> - Response Times: A: 0, B: 10, C: 20 units
> - Average Response Time: (0 + 10 + 20) / 3 = 10 units
> 
> ![[Pasted image 20240804233056.png|500]]
> 
> **RR with I/O Operations**
> - Description: Round Robin scheduling with jobs that perform I/O operations.
> - Scenario: Jobs A, B, C with run times 50, 50, 50 units. A and C have I/O operations.
> - I/O occurs every 10 units of CPU time and takes 10 units to complete.
> - Quantum = 10 units
> 
> ![[Pasted image 20240804233424.png|500]]
> 
> - Note: The scheduler can switch to another job during I/O wait, improving overall CPU utilization.
> - Response Times: A: 0, B: 10, C: 20 units
> - Turnaround Times: A: 140, B: 110, C: 150 units (including I/O time)


> [!idea] MLFQ (Multi-Level Feedback Queue)
> - Description: Uses multiple queues with different priority levels, adjusting job priorities based on behavior.
> - Scenario: Jobs A (I/O-bound), B (CPU-bound), C (mixed) arrive simultaneously.
> - Rules:
>   1. Higher priority queues run first.
>   2. Jobs at the same priority level are scheduled round-robin.
>   3. When a job enters the system, it's placed at the highest priority.
>   4. If a job uses up its time quantum, its priority is reduced.
>   5. After a certain time period, move all jobs to the highest priority queue.
> 
> - Benefits:
>   - Adapts to different job types without prior knowledge of their behavior.
>   - Provides good response time for short and I/O-bound jobs.
>   - Ensures long-running CPU-bound jobs eventually make progress.

> [!consider] Gaming the MLFQ System
> MLFQ can be vulnerable to exploitation by processes that understand its rules:
> 
> **Issue 1: Giving up the CPU just before the time slice ends**
> - A process could perform a short I/O operation just before its time quantum expires.
> - This keeps the process at a high priority level, potentially starving CPU-bound processes.
> 
> **Issue 2: Long-running jobs monopolizing CPU time**
> - Once all jobs reach the lowest priority level, CPU-bound jobs can monopolize the processor.
> - This leads to poor performance for I/O-bound and interactive processes.
> 
> Several strategies can be employed to address MLFQ vulnerabilities:
> 
> **Better Accounting**
> - Track total CPU time used by a process across all priority levels.
> - Demote processes based on total CPU usage rather than per-quantum usage.
> 
> **Priority Boost**
> - Periodically boost all processes to the highest priority queue.
> - Prevents starvation of long-running jobs and allows for adaptation to changing process behavior.
> 
> **Varying Time Quantum**
> - Use shorter time quanta for higher priority queues and longer ones for lower priority queues.
> - Encourages processes to complete quickly at higher priorities.
> 
> **Smarter Scheduling**
> - Implement more sophisticated algorithms to detect and prevent gaming behavior.
> - For example, monitor patterns of CPU usage and I/O operations to identify potentially malicious behavior.


> [!idea] Lottery Scheduling
> Lottery scheduling is a probabilistic approach to process scheduling:
> 
> **Key Concepts**
> - Each process is assigned a number of lottery tickets.
> - The scheduler regularly holds a lottery, picking a random ticket.
> - The process holding the winning ticket gets to run.
> 
> **Benefits**
> - Provides proportional share scheduling.
> - Naturally adaptive to changing workloads.
> - Simple to implement and understand.
> 
> **Example Scenario**
> - Process A: 100 tickets
> - Process B: 50 tickets
> - Process C: 25 tickets
> 
> Over time, A should get about 57% of CPU time, B about 29%, and C about 14%.


> [!idea] Completely Fair Scheduler (CFS)
> CFS is the default scheduler in the Linux kernel since version 2.6.23:
> 
> **Key Principles**
> - Aims to give each process a "fair" share of CPU time.
> - Uses a red-black tree to track running processes.
> - Processes with the least CPU time get to run next.
> 
> **Virtual Runtime**
> - CFS tracks the "virtual runtime" of each process.
> - Processes with lower virtual runtime are given priority.
> 
> **Dynamic Time Slices**
> - Instead of fixed time quanta, CFS calculates dynamic time slices based on the number of runnable processes.
> 
> **Niceness**
> - Incorporates the traditional Unix "nice" values to adjust process priorities.
> - Nice values range from -20 (highest priority) to 19 (lowest priority).
> 
> **Benefits**
> - Provides good responsiveness for interactive tasks.
> - Scales well with the number of processes.
> - Balances efficiently between fairness and performance.

