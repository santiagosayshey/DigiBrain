> [!motivation] Safe Driving
>
> Driverless cars are designed to navigate and operate without human intervention, relying on sensors, software, and complex algorithms. A primary motivation behind their development is the potential to significantly enhance road safety, as human error is a leading cause of traffic accidents. However, the programming of these vehicles for unavoidable accident scenarios introduces profound ethical challenges.
>
> Key considerations in this context include:
> - **Ethical Programming Dilemmas:** How should a driverless car be programmed to act when a crash is inevitable? This involves deciding what outcomes to prioritize, such as the safety of its passengers versus pedestrians or other road users.
> - **The Trolley Problem Analogy:** This ethical thought experiment, where one must choose between two undesirable outcomes, is often invoked. For instance, should a car swerve to hit one person to avoid hitting five? However, real-world accident scenarios are far more complex than such binary choices, involving a multitude of variables and uncertainties.
> - **Persistent Safety Concerns:** Despite the promise of increased safety, fatal accidents involving driverless cars have occurred. Examples include incidents where the vehicle's systems failed to correctly identify or react to hazards. These events underscore the ongoing need to critically examine and refine the technology and its ethical frameworks.

> [!idea] Choices
>
> When programming driverless cars for unavoidable crash scenarios, several core ethical approaches, or "choices," regarding decision-making logic emerge. These choices dictate the principles upon which the car's actions will be based when facing an imminent collision.
>
> The main programming options typically revolve around:
> - **Prioritizing Passenger Safety:** The vehicle's primary directive would be to protect its occupants, even if it means redirecting harm towards others outside the car.
> - **Minimizing Overall Harm:** This approach dictates that the car should aim to reduce the total number of injuries or fatalities, irrespective of whether those harmed are passengers or external parties (e.g., pedestrians, occupants of other vehicles).
> - **Adherence to an Alternative Ethical Principle:** This category encompasses other moral guidelines. For instance, the car could be programmed to randomize its decision to avoid pre-determined bias, or to prioritize vulnerable individuals, or to act according to pre-set deontological rules that forbid certain actions regardless of consequences.

> [!consider] A Choice of Choices
>
> Beyond defining the ethical principles a driverless car might follow in a crash, a crucial secondary question arises: who should have the authority to make these programming decisions? This meta-ethical dilemma, or "choice of choices," considers the source of the moral code embedded in autonomous vehicles.
>
> Two primary models for this decision-making responsibility are often discussed:
> - **Individual User Preference:** Should the owner or user of a specific driverless car be able to select the ethical settings it operates under? This would allow individuals to align the car's behavior with their personal moral convictions (e.g., choosing between a car that prioritizes occupants versus one that minimizes overall harm).
> - **Collective or Regulatory Decision:** Alternatively, should these ethical parameters be determined collectively through societal consensus, expert panels, or governmental regulation? This approach would aim for a uniform ethical standard for all autonomous vehicles to ensure public safety and predictability, potentially reflecting broader societal values.

> [!example] The Trolley Problem
>
> The classic "trolley problem" is a philosophical thought experiment that questions whether it is permissible to sacrifice one person to save a larger number. This ethical puzzle serves as a common, though simplified, framework for discussing the crash optimization choices for driverless cars.
>
> Consider these adapted scenarios:
> - **Scenario 1 (Save More Lives by Swerving):** A self-driving car with five passengers suddenly detects an unavoidable obstacle directly in its path. Continuing straight will result in the death of all five occupants. The only alternative is to swerve onto a sidewalk, where it will kill one pedestrian. The question is whether the car should be programmed to swerve.
> - **Scenario 2 (Save Fewer Lives by Swerving):** A self-driving car with only one passenger detects an unavoidable obstacle. To save the passenger, the car would have to swerve onto a sidewalk, but this action would result in the death of five pedestrians. Here, the dilemma is whether the car should prioritize its single occupant over a larger number of pedestrians.

> [!consider] Trolley Skepticism
>
> While the trolley problem offers a starting point for ethical discussions about driverless cars, Nyholm suggests exercising skepticism regarding its direct applicability to real-world crash scenarios. The analogy, though illustrative, has significant limitations.
>
> Reasons for this skepticism include:
> - **Oversimplification and Abstraction:** Trolley problems deliberately abstract away many details (e.g., the identities of individuals, how they ended up in peril) to focus on a narrow set of moral variables, primarily numbers. Real-world accidents are information-rich and involve complex, often ambiguous, factors. For example, if a car must choose between hitting a motorcyclist wearing a helmet versus one without, selecting the helmeted rider might seem to penalize responsible behavior, while choosing the unhelmeted rider could be seen as penalizing their lack of protection, introducing complexities not captured by simple numerical trade-offs.
> - **Neglect of Moral and Legal Responsibility:** Trolley thought experiments typically ask us to set aside questions of legal liability and moral blame. However, these are precisely the central ethical and practical dilemmas in actual driverless car incidents. Determining who is at fault—the programmer, the owner, the manufacturer, or the car itself—is a critical aspect that the trolley problem framework largely ignores.
> - **Assumption of Perfect Knowledge:** Trolley scenarios usually assume complete and accurate knowledge of the situation and the exact outcomes of each possible action. In reality, crash situations are dynamic and unpredictable. An autonomous vehicle's sensors and algorithms operate with imperfect information and cannot foresee all consequences with certainty, making the decision-making process far more probabilistic and uncertain than the clear-cut choices presented in trolley cases.

> [!consider] Value of Trolley Problem Analogies
>
> Despite the valid criticisms and limitations of directly applying trolley problem scenarios to the complex reality of driverless car ethics, these thought experiments still offer valuable contributions to the discussion. They serve as useful tools for initiating ethical reflection and exploring foundational moral questions.
>
> Key insights and benefits include:
> - **Testing Moral Intuitions:** Trolley problems effectively challenge and test our initial moral intuitions about how autonomous vehicles should be programmed in critical situations. They force us to confront difficult trade-offs and articulate the underlying principles guiding our preferences.
> - **Stimulating New Lines of Inquiry:** By simplifying complex scenarios, these analogies can highlight core ethical conflicts, thereby giving rise to new questions and deeper areas of philosophical and practical investigation into machine ethics and artificial intelligence safety.
> - **Raising Awareness and Sparking Debate:** The stark and often unsettling nature of trolley problems helps to awaken public and professional curiosity in the ethical dimensions of driverless car technology, fostering broader discussion and engagement with these important societal issues.

> [!idea] Empirical Ethics
>
> Empirical ethics offers an alternative approach to informing the programming of driverless cars by focusing on the study of actual moral attitudes, preferences, and behaviors of people. Instead of relying solely on abstract ethical theories or thought experiments, this method gathers data on what moral decisions people would make or find acceptable in various scenarios.
>
> Key aspects of this approach include:
> - **Data-Driven Moral Insights:** It involves collecting and analyzing public opinion, often through surveys or simulated scenarios (like the "Moral Machine" experiment), to understand prevailing societal views on how autonomous vehicles should resolve ethical dilemmas. This contrasts with purely theoretical approaches by grounding ethical considerations in observable human judgments.
> - **Revealing Inconsistencies:** Empirical studies often highlight a tension in public preferences. For instance, while many people might agree in principle that driverless cars should be programmed to minimize overall harm (a utilitarian approach), a significant number also indicate a personal preference for a car that would prioritize their own safety or the safety of their passengers, even if it means causing more harm to others. This reveals a potential conflict between what people believe is ethically ideal for society versus what they would choose for themselves.

> [!consider] Empirical Skepticism
>
> While empirical ethics provides valuable data on public moral intuitions regarding driverless cars, Nyholm suggests that this approach also warrants critical scrutiny. Relying heavily on current public attitudes to determine ethical programming for autonomous vehicles has several potential drawbacks.
>
> Reasons for skepticism towards a purely empirical ethics approach include:
> - **Limited Experience and Uninformed Opinions:** Most people have little to no direct experience with the complex scenarios driverless cars might face. Current attitudes may therefore be based on uninformed or weakly considered intuitions, which might change with deeper reflection or more information.
> - **Focus on Preferences over Justifications:** Empirical data typically reveals *what* people's preferences are (the "end" choice), but often doesn't capture the underlying moral reasoning or *why* they hold those preferences. A robust ethical framework may require understanding the justifications for choices, not just the choices themselves.
> - **Inconsistency in Attitudes:** As highlighted previously, people can exhibit inconsistencies in their moral preferences—for example, desiring utilitarian programming for others' cars but self-preservation for their own. Basing ethical programming on such inconsistent attitudes is problematic, as it makes it difficult to establish a coherent and universally acceptable ethical standard.

> [!idea] Nyholm's View
>
> Nyholm proposes that to navigate the ethical complexities of programming driverless cars, particularly in unavoidable accident scenarios, we should turn to established normative ethical theories. This approach suggests moving beyond the limitations of both direct analogies like the trolley problem and the potential inconsistencies of purely empirical ethics.
>
> Instead of relying solely on simplified dilemmas or public opinion, Nyholm advocates for applying broader, more systematic ethical frameworks, such as:
> - Consequentialism (e.g., Utilitarianism, focusing on outcomes)
> - Deontology (e.g., Kantian ethics, focusing on duties and rules)
> - Virtue Ethics (focusing on moral character)
> - Contractualism (focusing on principles that informed individuals would agree to)
>
> These theories can provide more comprehensive and reasoned bases for making decisions about how autonomous vehicles should be programmed to behave.

> [!idea] Utilitarianism
>
> Utilitarianism, as a normative ethical theory, suggests that the morally right action is the one that maximizes overall good or "utility," often interpreted as maximizing happiness or well-being and minimizing harm. Applied to driverless cars, this typically translates to programming them to achieve the best possible outcomes in crash scenarios.
>
> There are different perspectives on how utilitarian principles might be applied:
> - **Traditional Utilitarian Approach:** The most straightforward interpretation is that a driverless car should be programmed to save the maximum number of lives possible in an unavoidable accident. In this view, the decision algorithm would calculate the probable outcomes of different actions (e.g., swerving vs. not swerving) and choose the one that results in the fewest fatalities or serious injuries, regardless of whether those saved are passengers or pedestrians.
> - **Nyholm's Broader Utilitarian Argument:** Nyholm proposes a more nuanced utilitarian perspective. He suggests that the greatest overall utility might actually be achieved by programming cars to prioritize saving their own driver/passengers. The reasoning is that this approach would likely increase public acceptance and adoption of driverless cars, as most people would prefer a car that protects them. Wider adoption, in turn, would lead to a greater reduction in accidents overall (since autonomous vehicles are expected to be safer than human drivers on average), thus maximizing utility on a larger scale by improving road safety comprehensively. This also affirms the common desire of individuals for self-preservation.

> [!idea] Kantian Ethics
>
> Kantian ethics, a deontological theory developed by Immanuel Kant, proposes that moral actions are those performed out of a sense of duty, based on universal principles. When applied to the programming of driverless cars, the focus shifts from outcomes (like in utilitarianism) to the inherent rightness or wrongness of the actions themselves, as determined by key principles.
>
> Core Kantian considerations for autonomous vehicle programming include:
> - **Universalizability (Categorical Imperative - First Formulation):** Any rule or "maxim" governing the car's action in a crash scenario should be capable of being a universal law that could be applied to all autonomous vehicles without contradiction. For example, if a car is programmed to sacrifice its passenger to save more pedestrians, would this be a rule that all rational agents could consistently will for all such vehicles?
> - **Treating Humanity as an End, Never Merely as a Means (Categorical Imperative - Second Formulation):** This principle dictates that all individuals possess inherent dignity and should not be used solely as instruments to achieve some other goal. Programming a car to deliberately sacrifice one person (whether a passenger or a pedestrian) to save others could be seen as treating that individual merely as a means to an end (the saving of other lives), which would be ethically problematic from a Kantian perspective. This raises complex questions about whether any pre-programmed decision to cause harm to an individual treats them as a mere means.


> [!idea] Virtue Ethics
>
> Virtue ethics is an approach to morality that emphasizes the cultivation of virtuous character traits as the primary basis for ethical behavior, rather than focusing on duties (like Kantianism) or consequences (like utilitarianism). It asks what a virtuous person would do in a given situation.
>
> When applied to driverless cars, virtue ethics shifts the focus towards:
> - **The "Character" of the System:** While a car cannot possess virtues in the human sense, the designers and programmers can embed principles that reflect virtuous dispositions. For example, the car's systems could be designed to exhibit "carefulness" or "prudence" by prioritizing cautious driving patterns and robust safety measures to prevent accidents in the first place.
> - **Responsibilities of Designers and Users:** Virtue ethics would also examine the moral character and responsibilities of the human agents involved.
>     - **Designers and Manufacturers:** They should strive for virtues like conscientiousness, due diligence, and a commitment to public safety in the development and deployment of autonomous vehicle technology.
>     - **Users:** Users also have a role in acting virtuously, for instance, by responsibly overseeing the vehicle (where applicable), maintaining it properly, and using it in a manner consistent with societal well-being.
> - **Promoting Human Flourishing:** Ultimately, the design and regulation of driverless cars, from a virtue ethics perspective, should aim to contribute to human flourishing and a well-ordered society, where technology serves virtuous ends. This could involve ensuring that autonomous systems operate in a way that is perceived as fair, reliable, and considerate.

> [!idea] Contractualism
>
> Contractualism is an ethical theory suggesting that moral principles are those that would be agreed upon by free, equal, and rational individuals entering into a social contract or agreement. Applied to driverless cars, this framework asks what rules for their operation and crash-response programming people would have self-interested, rational reasons to endorse if they were designing these rules from a fair and impartial standpoint.
>
> Key considerations from a contractualist perspective include:
> - **Principles of Mutual Agreement:** The core idea is to identify programming ethics that no one could reasonably reject. This often involves considering what principles individuals would accept, not knowing their specific future role in any given traffic scenario (e.g., whether they would be a passenger, a pedestrian, or an occupant of another vehicle).
> - **Self-Interested Preference for Safety:** From a self-interested standpoint, individuals would likely prefer programming that generally enhances their own chances of survival and well-being. This could lead to an agreement on principles that aim to minimize overall harm in unavoidable accidents, as such a rule, applied universally, would statistically increase each person's safety.
> - **Justifiable Rules:** The agreed-upon rules must be justifiable to everyone. For instance, a rule that arbitrarily sacrifices one group for another might be rejected by those in the potentially sacrificed group. Therefore, contractualism might favor programming that aims to reduce risk for all road users in a way that is transparent and defensible. This could mean programming cars to avoid causing collisions where possible and, in unavoidable situations, to act in a way that minimizes the likelihood of severe harm to any party involved, based on principles that all could find acceptable.

