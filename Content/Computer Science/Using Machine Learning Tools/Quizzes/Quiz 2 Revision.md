
| Concept                     | Abbreviation | Definition                                                                                                                                                                                                                                                                                                                                                                                                         | Example |
| --------------------------- | ------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------- |
| True Positive               | TP           | All the data labelled as postitive that was actually positive                                                                                                                                                                                                                                                                                                                                                      |         |
| False Positive              | FP           | All the data labelled as positive but was actually negative                                                                                                                                                                                                                                                                                                                                                        |         |
| False Negative              | FN           | All the data labelled as negative but was actually positive                                                                                                                                                                                                                                                                                                                                                        |         |
| True Negative               | TN           | All the data labelled as negative that was actually negative                                                                                                                                                                                                                                                                                                                                                       |         |
| Precision                   | P            | What fraction of positive predictions are correct?<br>$$\text{Precision} = \frac{TP}{TP + FP}$$<br>- Important when false positives are costly (like identifying an important email as spam)                                                                                                                                                                                                                       |         |
| Recall (True Positive Rate) | R or TPR     | What fraction of the real positive class was detected?<br>$$\text{Recall} = \frac{TP}{TP + FN}$$<br>- Important when missing positive cases is costly (like missing cancer in someone that has it)                                                                                                                                                                                                                 |         |
| False Positive Rate         | FPR          | What fraction of the real negative class was incorrectly labeled as positive? $$\text{FPR} = \frac{FP}{FP + TN}$$ - Important when false alarms are costly (like incorrectly flagging a legitimate transaction as fraud)                                                                                                                                                                                           |         |
| F1-Score                    | F1-Score     | F1-Score is a metric that combines precision and recall into a single number.<br><br>F1-Score is the harmonic mean of precision and recall (it weighs them equally): $$\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$<br><br>It's useful when you need a balance between precision and recall - when both false positives and false negatives matter. |         |
| ROC/AUC | ROC/AUC | ROC (Receiver Operating Characteristic) curve plots True Positive Rate vs False Positive Rate at various thresholds.<br><br>AUC (Area Under the Curve) summarizes ROC performance in one number: $$\text{AUC} \in [0,1]$$<br><br>AUC = 1: Perfect classifier<br>AUC = 0.5: Random guessing<br>AUC = 0: Perfectly wrong (invert predictions)<br><br>- Useful for comparing models overall performance regardless of threshold | |
