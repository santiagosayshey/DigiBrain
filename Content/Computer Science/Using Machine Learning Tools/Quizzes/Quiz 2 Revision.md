| Concept                     | Abbreviation | Definition                                                                                                                                                                                                                                                                                                                                                                                                 | Example (Medical Test Scenario)                                                                                                                                                                                                                                                                                                                                                                                    |
| :-------------------------- | :----------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| True Positive               | TP           | All the data labelled as postitive that was actually positive                                                                                                                                                                                                                                                                                                                                              | The patient has the disease, and the test correctly indicates they have the disease.                                                                                                                                                                                                                                                                                                                               |
| False Positive              | FP           | All the data labelled as positive but was actually negative                                                                                                                                                                                                                                                                                                                                                | The patient does *not* have the disease, but the test incorrectly indicates they *do* have the disease (a false alarm).                                                                                                                                                                                                                                                                                            |
| False Negative              | FN           | All the data labelled as negative but was actually positive                                                                                                                                                                                                                                                                                                                                                | The patient *has* the disease, but the test incorrectly indicates they do *not* have the disease (a missed case).                                                                                                                                                                                                                                                                                                  |
| True Negative               | TN           | All the data labelled as negative that was actually negative                                                                                                                                                                                                                                                                                                                                               | The patient does *not* have the disease, and the test correctly indicates they do not have the disease.                                                                                                                                                                                                                                                                                                            |
| Precision                   | P            | What fraction of positive predictions are correct?<br>$$\text{Precision} = \frac{TP}{TP + FP}$$<br>- Important when false positives are costly (like identifying an important email as spam)                                                                                                                                                                                                               | If the test flags 10 patients as having the disease, but only 8 actually do (2 are FP), the precision is $8/(8+2) = 0.8$. 80% of patients identified by the test actually have the disease.                                                                                                                                                                                                                        |
| Recall (True Positive Rate) | R or TPR     | What fraction of the real positive class was detected?<br>$$\text{Recall} = \frac{TP}{TP + FN}$$<br>- Important when missing positive cases is costly (like missing cancer in someone that has it)                                                                                                                                                                                                         | If there are 15 patients who truly have the disease, and the test correctly identifies 12 of them (missing 3, which are FN), the recall is $12/(12+3) = 0.8$. The test found 80% of the actual disease cases.                                                                                                                                                                                                      |
| False Positive Rate         | FPR          | What fraction of the real negative class was incorrectly labeled as positive? $$\text{FPR} = \frac{FP}{FP + TN}$$ - Important when false alarms are costly (like incorrectly flagging a legitimate transaction as fraud)                                                                                                                                                                                   | If there are 100 healthy patients (negative class), and the test incorrectly flags 5 of them as having the disease (5 FP), the FPR is $5/(5+95) = 0.05$. 5% of healthy patients received a false positive result.                                                                                                                                                                                                  |
| F1-Score                    | F1-Score     | F1-Score is a metric that combines precision and recall into a single number.<br>F1-Score is the harmonic mean of precision and recall (it weighs them equally): $$\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$<br>It's useful when you need a balance between precision and recall - when both false positives and false negatives matter. | Using the Precision (0.8) and Recall (0.8) from the examples above: F1-Score = $2 \times (0.8 \times 0.8) / (0.8 + 0.8) = 2 \times 0.64 / 1.6 = 1.28 / 1.6 = 0.8$. This single score reflects the balance between precision and recall.                                                                                                                                                                            |
| ROC/AUC                     | ROC/AUC      | ROC curve plots TPR vs FPR as you change the classification threshold<br>Shows the trade-off: higher threshold = fewer false positives but might miss real positives<br>AUC = area under the ROC curve (0 to 1)<br>- Measures overall ability to distinguish between positive and negative classes<br>- Useful for choosing optimal threshold and comparing models                                         | Comparing two different diagnostic tests: Test A has an AUC of 0.90, Test B has an AUC of 0.75. This indicates Test A is generally better at distinguishing between patients with and without the disease across all possible sensitivity levels (thresholds). An AUC of 0.90 means there's a 90% chance that the test will rank a randomly chosen diseased patient higher than a randomly chosen healthy patient. |