> [!motivation] The Problem of Different Feature Scales
> 
> In raw datasets, features naturally exist on vastly different scales - age might be measured in years (20-80), income in dollars (20,000-200,000), and blood pressure in mmHg (80-200).
> 
> These scale disparities create fundamental mathematical issues when calculating distances or gradients, making it difficult to compare or combine features in meaningful ways.
> 
> For example, in a medical dataset, blood tests like white blood cell count (~5-10 K/Î¼L) and cholesterol (~150-300 mg/dL) have different units and ranges, making it unclear how to properly weight their importance in any calculation.

> [!idea] Scaling Data
> 
> Data scaling transforms features to a specific range, ensuring comparability across different scales and improving algorithm performance.
> 
> Common scaling techniques:
> 
> - **Standardization**: Transforms data to have mean=0 and standard deviation=1
> - **Min-Max Scaling**: Rescales data to a fixed range, typically [0,1]
> - **Robust Scaling**: Uses statistics that are robust to outliers (median, IQR)
> 
> When to scale:
> 
> - Before applying distance-based algorithms (k-means, KNN)
> - For gradient-based optimization algorithms (neural networks)
> - When features have different units or ranges
> 
> Scaling should be applied after imputation to ensure the imputed values don't distort the scaling parameters.

> [!example] Min-Max Scaling Implementation
> 
> Consider a dataset with features on different scales:
> 
> |Patient|Age|Income ($)|Blood Pressure|
> |---|---|---|---|
> |A|25|35,000|140|
> |B|62|85,000|125|
> |C|41|55,000|130|
> |D|33|48,000|135|
> 
> **Min-Max Scaling to [0,1] range:**
> 
> 1. Calculate min and max for each feature
>     - Age: min = 25, max = 62
>     - Income: min = 35,000, max = 85,000
>     - BP: min = 125, max = 140
> 2. Apply formula: x_scaled = (x - min) / (max - min)
> 
> Min-Max scaled data:
> 
> |Patient|Age|Income|Blood Pressure|
> |---|---|---|---|
> |A|0.00|0.00|1.00|
> |B|1.00|1.00|0.00|
> |C|0.43|0.40|0.33|
> |D|0.22|0.26|0.67|
> 
> All features now range precisely from 0 to 1, with their original relative relationships preserved within this new scale.

> [!consider] Consequences of Unscaled Data
> 
> Failing to scale data with different ranges can lead to several problematic outcomes in machine learning applications.
> 
> Potential issues:
> 
> - **Biased distance calculations**: In distance-based algorithms like K-means or KNN, features with larger numerical ranges will dominate the distance calculation
> - **Slow or unstable convergence**: Gradient-based algorithms may converge slowly or erratically when features are on different scales
> - **Feature importance distortion**: Tree-based methods are generally scale-invariant, but model interpretation can be misleading when comparing unscaled coefficients
> 
> Example impact: Using the unscaled data from our previous example, a 1-unit change in Income ($1) would have 1/140 the impact of a 1-unit change in Blood Pressure, regardless of their true importance for prediction.
> 
> Algorithms particularly vulnerable to unscaled data include:
> 
> - K-means clustering
> - Principal Component Analysis (PCA)
> - Support Vector Machines
> - Neural Networks

> [!example] Standardization (Z-score) Scaling
> 
> Consider a dataset with outliers:
> 
> |Patient|Age|Income ($)|Cholesterol|
> |---|---|---|---|
> |A|25|45,000|180|
> |B|32|52,000|210|
> |C|41|59,000|195|
> |D|37|320,000|190|
> 
> **Standardization vs. Min-Max with outliers:**
> 
> 1. Apply Standard Scaling (Z-score):
>     
>     - Income: mean = 119,000, std = 131,891
>     
>     |Patient|Income (standardized)|
>     |---|---|
>     |A|-0.56|
>     |B|-0.51|
>     |C|-0.46|
>     |D|1.52|
>     
> 2. Apply Min-Max Scaling:
>     
>     - Income: min = 45,000, max = 320,000
>     
>     |Patient|Income (min-max)|
>     |---|---|
>     |A|0.00|
>     |B|0.03|
>     |C|0.05|
>     |D|1.00|
>     
> 
> Standardization preserves the relative distance between typical values (patients A, B, C), while min-max scaling compresses them into a tiny range due to the outlier (patient D).
> 
> Use standardization when:
> 
> - Outliers are present
> - Normal distribution of features is assumed or desired
> - Using algorithms sensitive to outliers (like PCA or SVM)