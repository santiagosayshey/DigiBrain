> [!motivation] Motivation: Predicting Values, Not Just Categories
>
> We've explored the architecture of Multilayer Perceptrons (MLPs) and how they can learn complex patterns through mechanisms like backpropagation. Much of the discussion implicitly focused on classification tasksâ€”assigning an input to a discrete category (like "cat" vs. "dog", "spam" vs. "not spam", or different Iris species).
>
> However, many important real-world problems involve predicting a **continuous numerical value**, rather than a category label. Consider tasks such as:
> -   Estimating the future price of a stock based on market trends.
> -   Predicting the exact temperature for tomorrow based on current weather data.
> -   Calculating the expected sale price of a house given its features (size, location, condition).
> -   Determining the fuel efficiency (e.g., miles per gallon) of a car based on its engine size, weight, and aerodynamics.
> -   Estimating the age of a person from their photograph.
>
> These problems require a model to output a specific number along a continuous scale. This raises the question: how can the powerful pattern-recognition capabilities of MLPs, which excel at learning complex functions, be adapted or configured to tackle these **regression tasks** where the goal is precise numerical prediction?

> [!idea] Configuring MLPs for Regression Tasks
>
> Multilayer Perceptrons (MLPs) are not limited to classification; they can be effectively configured to perform regression tasks, where the goal is to predict continuous numerical values. This involves specific considerations for the network's architecture, particularly its output layer, and the choice of loss function.
>
> -   **Output Layer Neuron Count**:
>     -   A fundamental principle is that **each output neuron typically corresponds to one prediction dimension or target value**.
>     -   If you are predicting a single numerical value (e.g., the price of a house), the output layer will generally have just one neuron.
>     -   If you need to predict multiple values simultaneously (e.g., the (x, y) coordinates of an object's center), the output layer will have one neuron for each value (e.g., two neurons in this case).
>

> [!example] Building with Regression MLPs
>
> This example demonstrates building and training a Multilayer Perceptron for a regression task using Scikit-Learn's `MLPRegressor` with the California housing dataset. The key steps involve loading and preparing the data, scaling features, defining the MLP, training it within a pipeline, and evaluating its performance.
>
> **Scikit-Learn Implementation:**
>
> ```python
> from sklearn.datasets import fetch_california_housing
> from sklearn.metrics import mean_squared_error
> from sklearn.model_selection import train_test_split
> from sklearn.neural_network import MLPRegressor
> from sklearn.pipeline import make_pipeline
> from sklearn.preprocessing import StandardScaler
>
> # Load and split the California housing data
> # This dataset contains only numerical features and no missing values.
> housing = fetch_california_housing()
> X_train_full, X_test, y_train_full, y_test = train_test_split(
>     housing.data, housing.target, random_state=42
> )
> X_train, X_valid, y_train, y_valid = train_test_split(
>     X_train_full, y_train_full, random_state=42
> )
>
> # Define the MLPRegressor with three hidden layers of 50 neurons each
> # Note: The MLPRegressor uses ReLU for hidden layers, Adam optimizer,
> # and MSE loss by default.
> mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)
>
> # Create a pipeline to standardize input features before training the MLP
> # Feature scaling is important for Gradient Descent-based algorithms.
> pipeline = make_pipeline(StandardScaler(), mlp_reg)
>
> # Train the model
> pipeline.fit(X_train, y_train)
>
> # Make predictions on the validation set
> y_pred = pipeline.predict(X_valid)
>
> # Evaluate the model using Root Mean Squared Error (RMSE)
> rmse = mean_squared_error(y_valid, y_pred, squared=False)
> # According to the source text, this RMSE is about 0.505
> ```
>
> **Outcome:**
> In this specific example, the `MLPRegressor`, configured with three hidden layers of 50 neurons each and using feature scaling, achieves a validation Root Mean Squared Error (RMSE) of approximately 0.505 on the California housing dataset. This demonstrates a practical application of MLPs for regression tasks.