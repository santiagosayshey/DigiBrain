> [!motivation] Motivation
> 
> Many **real-world problems require making a series of decisions based on multiple factors**. Consider a doctor diagnosing a patient:
> 
> - What symptoms are present?
> - What risk factors does the patient have?
> - What test results are available?
> 
> Each answer narrows down the possibilities until a conclusion is reached. This systematic, rule-based approach to classification is intuitive and transparent - we can **follow the reasoning path to understand why a particular decision was made.**

> [!idea] Decision Trees
> 
> Decision trees are classification models that s**plit data recursively based on features to create a tree-like structure of decision rules.**
> 
> **Core structure:**
> 
> - Root node: Starting point representing the entire dataset
> - Internal nodes: Decision points that split data based on feature values
> - Branches: Outcomes of decisions connecting nodes
> - Leaf nodes: Final classifications after all decisions
> 
> **How they work:**
> 
> - The algorithm searches for the most informative feature to split data
> - It creates a decision node based on this feature
> - It recursively repeats this process on resulting subsets
> - It stops when reaching specified criteria (e.g., max depth, min samples)
> 
> |Advantages|Limitations|
> |---|---|
> |Interpretable|Can overfit with deep trees|
> |No scaling required|Unstable (small changes → different trees)|
> |Handles mixed data types|Biased toward features with more levels|
> |Captures non-linear patterns|Can't represent some relationships well|

> [!example] Credit Application Classification
> 
> A bank built a decision tree to classify loan applications as approved or denied.
> 
> **Dataset:**
> 
> - 10,000 past loan applications with features like income, credit score, debt ratio
> - Known outcomes (approved/denied) for training
> 
> **The resulting tree:**
> 
> ```
> Credit Score < 650?
> ├── Yes → Debt-to-Income > 30%?
> │   ├── Yes → DENY
> │   └── No → Employment History < 2 years?
> │       ├── Yes → DENY
> │       └── No → APPROVE
> └── No → Income < $40,000?
>     ├── Yes → Loan Amount > $20,000?
>     │   ├── Yes → DENY
>     │   └── No → APPROVE
>     └── No → APPROVE
> ```
> 
> **In practice:**
> 
> - New applicants' information flows through the tree
> - Each internal node tests one feature
> - The path followed determines the final classification
> - Example path: Credit score 700 → Income $35,000 → Loan amount $15,000 → APPROVE
> 
> **Model performance:**
> 
> - Accuracy: 89% on test data
> - False positives (bad loans approved): 7%
> - False negatives (good loans denied): 4%
> - Most common misclassification: borderline credit scores with other compensating factors

> [!idea] GINI Impurity and CART Algorithm
> 
> **What is GINI Impurity:**
> 
> GINI impurity measures how mixed different classes are within a dataset. It tells us how "pure" our data groups are.
> 
> When all data in a group belongs to the same class (completely pure), GINI equals 0. When data is evenly split between classes (completely mixed), GINI reaches its maximum.
> 
> Lower GINI values mean better classification potential.
> 
> ![[Pasted image 20250324124301.png|500]]
> 
> **How CART Works:**
> 
> The CART algorithm builds decision trees through these steps:
> 
> 1. Start with all data in one group
> 2. Try every possible way to split the data based on different features
> 3. Calculate how much each potential split would reduce GINI impurity
> 4. Select the split that creates the purest subgroups (greatest GINI reduction)
> 5. Repeat this process on each resulting subgroup
> 
> This creates a tree where each branch point represents a decision that best separates the data classes.

> [!example] GINI Impurity and CART in Action
> 
> Imagine we have 100 loan applicants: 65 are low-risk and 35 are high-risk. We want to create a decision tree to classify future applicants.
> 
> **GINI Formula Explained**: The GINI impurity formula is: $$\text{GINI} = 1 - \sum_{i=1}^{c} (p_i)^2$$
> 
> Where $p_i$ is the proportion of class $i$ in the dataset and $c$ is the number of classes.
> 
> We square each class proportion because:
> 
> - It penalizes imbalance - a 50/50 split produces a higher value than 90/10
> - When squared, values between 0-1 become smaller, so perfectly pure nodes (proportion = 1) will have GINI = 0
> - The sum represents all possible misclassifications
> 
> **Initial GINI Calculation**: For our starting dataset: $$\text{GINI} = 1 - [(65/100)^2 + (35/100)^2] = 1 - [0.4225 + 0.1225] = 1 - 0.545 = 0.455$$
> 
> **CART in Action**: Let's examine potential splits using "income" feature:
> 
> - Split A (income ≤ $50K): 40 applicants (30 low-risk, 10 high-risk)
>     - $\text{GINI}_A = 1 - [(30/40)^2 + (10/40)^2] = 1 - [0.5625 + 0.0625] = 0.375$
> - Split B (income > $50K): 60 applicants (35 low-risk, 25 high-risk)
>     - $\text{GINI}_B = 1 - [(35/60)^2 + (25/60)^2] = 1 - [0.34 + 0.17] = 0.49$
> 
> **Weighted GINI for this split**: $\text{GINI}_{\text{split}} = \frac{40}{100} \times 0.375 + \frac{60}{100} \times 0.49 = 0.15 + 0.294 = 0.444$
> 
> Since $0.444 < 0.455$, this split reduces impurity!
> 
> After examining all features, if "debt-to-income ratio" creates an even lower weighted GINI of 0.32, CART would choose that as the first split instead, continuing recursively until reaching pure or nearly-pure leaf nodes.