> [!motivation] Motivation
> 
> Many **real-world problems require making a series of decisions based on multiple factors**. Consider a doctor diagnosing a patient:
> 
> - What symptoms are present?
> - What risk factors does the patient have?
> - What test results are available?
> 
> Each answer narrows down the possibilities until a conclusion is reached. This systematic, rule-based approach to classification is intuitive and transparent - we can **follow the reasoning path to understand why a particular decision was made.**

> [!idea] Decision Trees
> 
> Decision trees are classification models that s**plit data recursively based on features to create a tree-like structure of decision rules.**
> 
> **Core structure:**
> 
> - Root node: Starting point representing the entire dataset
> - Internal nodes: Decision points that split data based on feature values
> - Branches: Outcomes of decisions connecting nodes
> - Leaf nodes: Final classifications after all decisions
> 
> **How they work:**
> 
> - The algorithm searches for the most informative feature to split data
> - It creates a decision node based on this feature
> - It recursively repeats this process on resulting subsets
> - It stops when reaching specified criteria (e.g., max depth, min samples)
> 
> |Advantages|Limitations|
> |---|---|
> |Interpretable|Can overfit with deep trees|
> |No scaling required|Unstable (small changes → different trees)|
> |Handles mixed data types|Biased toward features with more levels|
> |Captures non-linear patterns|Can't represent some relationships well|

> [!example] Credit Application Classification
> 
> A bank built a decision tree to classify loan applications as approved or denied.
> 
> **Dataset:**
> 
> - 10,000 past loan applications with features like income, credit score, debt ratio
> - Known outcomes (approved/denied) for training
> 
> **The resulting tree:**
> 
> ```
> Credit Score < 650?
> ├── Yes → Debt-to-Income > 30%?
> │   ├── Yes → DENY
> │   └── No → Employment History < 2 years?
> │       ├── Yes → DENY
> │       └── No → APPROVE
> └── No → Income < $40,000?
>     ├── Yes → Loan Amount > $20,000?
>     │   ├── Yes → DENY
>     │   └── No → APPROVE
>     └── No → APPROVE
> ```
> 
> **In practice:**
> 
> - New applicants' information flows through the tree
> - Each internal node tests one feature
> - The path followed determines the final classification
> - Example path: Credit score 700 → Income $35,000 → Loan amount $15,000 → APPROVE
> 
> **Model performance:**
> 
> - Accuracy: 89% on test data
> - False positives (bad loans approved): 7%
> - False negatives (good loans denied): 4%
> - Most common misclassification: borderline credit scores with other compensating factors

> [!consider] GINI Impurity and CART Algorithm
> 
> **What GINI impurity actually measures**
> 
> GINI impurity measures the probability of misclassification if we randomly pick an item and randomly assign it a label based on the class distribution in our dataset.
> 
> - Low GINI = Low chance of misclassification = Better for decision making
> - High GINI = High chance of misclassification = Worse for decision making
> 
> **The concept in simple terms:**
> 
> - If all emails in a group are spam, you'll never misclassify (GINI = 0)
> - If half are spam and half are legitimate, random guessing gives 50% error (GINI = 0.5)
> 
> **The mathematics behind it:**
> 
> - Formula: $\text{Gini}(S) = 1 - \sum_{i=1}^{c} p_i^2$
> - For binary classification this simplifies to: $\text{Gini}(S) = 2p(1-p)$
> - Where $p$ is the proportion of one class
> 
> **How CART uses GINI to build trees:**
> 
> CART aims to create the purest possible groups at each step:
> 
> 1. Start with mixed data (high GINI)
> 2. Try every possible split on every feature
> 3. Choose the split that results in the largest decrease in GINI
> 4. Repeat the process on each resulting group
> 
> **Example of splitting decision:**
> 
> - Original data: 60 spam, 40 legitimate (GINI = 0.48)
> - After splitting on "contains word 'free'":
>     - Group 1: 50 spam, 10 legitimate (GINI = 0.32)
>     - Group 2: 10 spam, 30 legitimate (GINI = 0.42)
>     - Weighted average GINI after split = 0.35
>     - Improvement: 0.48 - 0.35 = 0.13
> 
> This improvement means our split has created more homogeneous groups, making classification more reliable.