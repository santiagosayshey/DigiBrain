> [!motivation] Motivation
> 
> Many **real-world problems require making a series of decisions based on multiple factors**. Consider a doctor diagnosing a patient:
> 
> - What symptoms are present?
> - What risk factors does the patient have?
> - What test results are available?
> 
> Each answer narrows down the possibilities until a conclusion is reached. This systematic, rule-based approach to classification is intuitive and transparent - we can **follow the reasoning path to understand why a particular decision was made.**

> [!idea] Decision Trees
> 
> Decision trees are classification models that s**plit data recursively based on features to create a tree-like structure of decision rules.**
> 
> **Core structure:**
> 
> - Root node: Starting point representing the entire dataset
> - Internal nodes: Decision points that split data based on feature values
> - Branches: Outcomes of decisions connecting nodes
> - Leaf nodes: Final classifications after all decisions
> 
> **How they work:**
> 
> - The algorithm searches for the most informative feature to split data
> - It creates a decision node based on this feature
> - It recursively repeats this process on resulting subsets
> - It stops when reaching specified criteria (e.g., max depth, min samples)
> 
> |Advantages|Limitations|
> |---|---|
> |Interpretable|Can overfit with deep trees|
> |No scaling required|Unstable (small changes → different trees)|
> |Handles mixed data types|Biased toward features with more levels|
> |Captures non-linear patterns|Can't represent some relationships well|

> [!example] Credit Application Classification
> 
> A bank built a decision tree to classify loan applications as approved or denied.
> 
> **Dataset:**
> 
> - 10,000 past loan applications with features like income, credit score, debt ratio
> - Known outcomes (approved/denied) for training
> 
> **The resulting tree:**
> 
> ```
> Credit Score < 650?
> ├── Yes → Debt-to-Income > 30%?
> │   ├── Yes → DENY
> │   └── No → Employment History < 2 years?
> │       ├── Yes → DENY
> │       └── No → APPROVE
> └── No → Income < $40,000?
>     ├── Yes → Loan Amount > $20,000?
>     │   ├── Yes → DENY
>     │   └── No → APPROVE
>     └── No → APPROVE
> ```
> 
> **In practice:**
> 
> - New applicants' information flows through the tree
> - Each internal node tests one feature
> - The path followed determines the final classification
> - Example path: Credit score 700 → Income $35,000 → Loan amount $15,000 → APPROVE
> 
> **Model performance:**
> 
> - Accuracy: 89% on test data
> - False positives (bad loans approved): 7%
> - False negatives (good loans denied): 4%
> - Most common misclassification: borderline credit scores with other compensating factors

> [!idea] GINI Impurity and CART Algorithm
> 
> **What is GINI Impurity:**
> 
> GINI impurity measures how mixed different classes are within a dataset. It tells us how "pure" our data groups are.
> 
> When all data in a group belongs to the same class (completely pure), GINI equals 0. When data is evenly split between classes (completely mixed), GINI reaches its maximum.
> 
> Lower GINI values mean better classification potential.
> 
> **How CART Works:**
> 
> The CART algorithm builds decision trees through these steps:
> 
> 1. Start with all data in one group
> 2. Try every possible way to split the data based on different features
> 3. Calculate how much each potential split would reduce GINI impurity
> 4. Select the split that creates the purest subgroups (greatest GINI reduction)
> 5. Repeat this process on each resulting subgroup
> 
> This creates a tree where each branch point represents a decision that best separates the data classes.

> [!consider] Mathematical Details of GINI and CART
> 
> **GINI Formula Explained:**
> 
> The GINI impurity is calculated using: Gini(S) = 1 - ∑(pi²)
> 
> Breaking this down:
> 
> - pi represents the proportion of items belonging to class i
> - We square each class proportion (pi²)
> - We add up all these squared proportions (∑pi²)
> - We subtract this sum from 1
> 
> For binary classification with classes A and B:
> 
> - If pA is the proportion of class A, then pB = 1-pA
> - The formula becomes: Gini(S) = 1 - [pA² + pB²] = 1 - [pA² + (1-pA)²] = 2pA(1-pA)
> 
> **GINI Properties:**
> 
> - GINI = 0: Perfect purity (all items belong to one class)
> - GINI = 0.5: Maximum impurity for binary classification (50% each class)
> - GINI is always between 0 and 1-(1/n) where n is the number of classes
> 
> **Example Calculation:** For a dataset with 70 items of class A and 30 items of class B:
> 
> - pA = 70/100 = 0.7
> - pB = 30/100 = 0.3
> - Gini = 1 - (0.7² + 0.3²) = 1 - (0.49 + 0.09) = 1 - 0.58 = 0.42
> 
> **CART's Mathematical Process:**
> 
> For each potential split:
> 
> 1. Calculate GINI before split (parent node)
> 2. Calculate GINI for each resulting subset (child nodes)
> 3. Calculate weighted average GINI after split: Gini_split = (nLeft/nTotal) × Gini_left + (nRight/nTotal) × Gini_right
> 4. Calculate information gain: Gain = Gini_parent - Gini_split
> 5. Choose split with maximum gain
> 
> Example: If splitting on feature X creates:
> 
> - Left node: 40 samples (35 class A, 5 class B), Gini = 0.24
> - Right node: 60 samples (35 class A, 25 class B), Gini = 0.47
> 
> The weighted GINI after split = (40/100 × 0.24) + (60/100 × 0.47) = 0.096 + 0.282 = 0.378
> 
> Information gain = 0.42 - 0.378 = 0.042
> 
> This process repeats for all possible splits, choosing the one with highest gain.