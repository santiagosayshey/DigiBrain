> [!motivation] Motivation
> 
> Many **real-world problems require making a series of decisions based on multiple factors**. Consider a doctor diagnosing a patient:
> 
> - What symptoms are present?
> - What risk factors does the patient have?
> - What test results are available?
> 
> Each answer narrows down the possibilities until a conclusion is reached. This systematic, rule-based approach to classification is intuitive and transparent - we can **follow the reasoning path to understand why a particular decision was made.**

> [!idea] Decision Trees
> 
> Decision trees are classification models that s**plit data recursively based on features to create a tree-like structure of decision rules.**
> 
> **Core structure:**
> 
> - Root node: Starting point representing the entire dataset
> - Internal nodes: Decision points that split data based on feature values
> - Branches: Outcomes of decisions connecting nodes
> - Leaf nodes: Final classifications after all decisions
> 
> **How they work:**
> 
> - The algorithm searches for the most informative feature to split data
> - It creates a decision node based on this feature
> - It recursively repeats this process on resulting subsets
> - It stops when reaching specified criteria (e.g., max depth, min samples)
> 
> **Mathematical foundations:**
> 
> - Splits are chosen to maximize information gain
> - Information gain = entropy(parent) - weighted_avg(entropy(children))
> - Entropy measures impurity: $H(S) = -\sum_{i=1}^{c} p_i \log_2(p_i)$
>     - Where $p_i$ is the proportion of class $i$ in set $S$
> 
> |Advantages|Limitations|
> |---|---|
> |Interpretable|Can overfit with deep trees|
> |No scaling required|Unstable (small changes → different trees)|
> |Handles mixed data types|Biased toward features with more levels|
> |Captures non-linear patterns|Can't represent some relationships well|

> [!example] Credit Application Classification
> 
> A bank built a decision tree to classify loan applications as approved or denied.
> 
> **Dataset:**
> 
> - 10,000 past loan applications with features like income, credit score, debt ratio
> - Known outcomes (approved/denied) for training
> 
> **The resulting tree:**
> 
> ```
> Credit Score < 650?
> ├── Yes → Debt-to-Income > 30%?
> │   ├── Yes → DENY
> │   └── No → Employment History < 2 years?
> │       ├── Yes → DENY
> │       └── No → APPROVE
> └── No → Income < $40,000?
>     ├── Yes → Loan Amount > $20,000?
>     │   ├── Yes → DENY
>     │   └── No → APPROVE
>     └── No → APPROVE
> ```
> 
> **In practice:**
> 
> - New applicants' information flows through the tree
> - Each internal node tests one feature
> - The path followed determines the final classification
> - Example path: Credit score 700 → Income $35,000 → Loan amount $15,000 → APPROVE
> 
> **Model performance:**
> 
> - Accuracy: 89% on test data
> - False positives (bad loans approved): 7%
> - False negatives (good loans denied): 4%
> - Most common misclassification: borderline credit scores with other compensating factors