> [!motivation] Motivation
> 
> Many **real-world problems require making a series of decisions based on multiple factors**. Consider a doctor diagnosing a patient:
> 
> - What symptoms are present?
> - What risk factors does the patient have?
> - What test results are available?
> 
> Each answer narrows down the possibilities until a conclusion is reached. This systematic, rule-based approach to classification is intuitive and transparent - we can **follow the reasoning path to understand why a particular decision was made.**

> [!idea] Decision Trees
> 
> Decision trees are classification models that s**plit data recursively based on features to create a tree-like structure of decision rules.**
> 
> **Core structure:**
> 
> - Root node: Starting point representing the entire dataset
> - Internal nodes: Decision points that split data based on feature values
> - Branches: Outcomes of decisions connecting nodes
> - Leaf nodes: Final classifications after all decisions
> 
> **How they work:**
> 
> - The algorithm searches for the most informative feature to split data
> - It creates a decision node based on this feature
> - It recursively repeats this process on resulting subsets
> - It stops when reaching specified criteria (e.g., max depth, min samples)
> 
> |Advantages|Limitations|
> |---|---|
> |Interpretable|Can overfit with deep trees|
> |No scaling required|Unstable (small changes → different trees)|
> |Handles mixed data types|Biased toward features with more levels|
> |Captures non-linear patterns|Can't represent some relationships well|

> [!example] Credit Application Classification
> 
> A bank built a decision tree to classify loan applications as approved or denied.
> 
> **Dataset:**
> 
> - 10,000 past loan applications with features like income, credit score, debt ratio
> - Known outcomes (approved/denied) for training
> 
> **The resulting tree:**
> 
> ```
> Credit Score < 650?
> ├── Yes → Debt-to-Income > 30%?
> │   ├── Yes → DENY
> │   └── No → Employment History < 2 years?
> │       ├── Yes → DENY
> │       └── No → APPROVE
> └── No → Income < $40,000?
>     ├── Yes → Loan Amount > $20,000?
>     │   ├── Yes → DENY
>     │   └── No → APPROVE
>     └── No → APPROVE
> ```
> 
> **In practice:**
> 
> - New applicants' information flows through the tree
> - Each internal node tests one feature
> - The path followed determines the final classification
> - Example path: Credit score 700 → Income $35,000 → Loan amount $15,000 → APPROVE
> 
> **Model performance:**
> 
> - Accuracy: 89% on test data
> - False positives (bad loans approved): 7%
> - False negatives (good loans denied): 4%
> - Most common misclassification: borderline credit scores with other compensating factors

> [!consider] GINI Impurity and CART Algorithm
> 
> **What is GINI Impurity?**
> 
> GINI impurity measures how mixed the classes are in a group of data. Lower values mean better separation.
> 
> - Perfect separation (all samples in one class): GINI = 0
> - Complete mixing (equal number of all classes): GINI approaches 1
> - Formula: $\text{Gini}(S) = 1 - \sum_{i=1}^{c} p_i^2$
> 
> **How CART uses GINI to build decision trees:**
> 
> CART (Classification and Regression Trees) builds decision trees by:
> 
> 1. **Examining each possible split:**
>     - "What if we split on age at 30 years?"
>     - "What if we split on income at $50K?"
> 2. **Evaluating split quality:**
>     - Calculate GINI before the split
>     - Calculate GINI for each resulting group
>     - Compute the weighted average of the new GINI values
>     - The difference shows how much the split improves class separation
> 3. **Choosing the best split:**
>     - Select the feature and threshold that gives the largest GINI reduction
>     - This creates the most homogeneous groups possible
> 
> **Practical example:**
> 
> For loan approval data:
> 
> - Original set: 60% approved, 40% denied (GINI = 0.48)
> - Split on "credit score < 650":
>     - Left group: 33% approved, 67% denied (more pure, GINI = 0.44)
>     - Right group: 71% approved, 29% denied (more pure, GINI = 0.41)
>     - Overall purity improvement: 0.06
> 
> This incremental improvement in purity at each step is what builds an effective decision tree.