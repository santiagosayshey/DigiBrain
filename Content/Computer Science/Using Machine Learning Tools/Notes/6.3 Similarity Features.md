> [!motivation] The Need for Similarity-Based Transformations
> 
> Polynomial transformations provide one approach to handling nonlinear data, but they come with significant limitations:
> 
> - Higher-degree polynomials create an explosion of features, making computation expensive
> - Polynomial curves may not naturally match the underlying patterns in many datasets
> - Some datasets require more flexible, localized transformations that respond to specific regions
> 

> [!idea] Similarity Features
> 
> Similarity features transform data by measuring how closely each instance resembles specific reference points called landmarks.
> 
> **Core concept:**
> 
> - Create new features based on similarity to landmark points
> - Replace or augment original features with these similarity scores
> - Transform nonlinearly separable data into a linearly separable space
> 
> **How it works:**
> 
> - Select landmark points in the original feature space
> - Define a similarity function that measures distance between points
> - For each data point, calculate similarity to each landmark
> - These similarity scores become the new feature coordinates
> 
> ![[Pasted image 20250426095811.png|500]]
> 
> As shown in the image, points from original 1D space are transformed to a 2D space where each coordinate represents similarity to a landmark. The previously inseparable data becomes linearly separable with a simple diagonal line.
> 
> The main advantage of this approach is its ability to capture complex, nonlinear relationships within data by measuring proximity to reference points rather than using polynomial combinations of features.
> 
> When using every training point as a landmark, this creates a rich feature space that dramatically increases the chances of finding a linear separation, though potentially at high computational cost.


> [!idea] Gaussian RBF Kernel
> 
> The Gaussian Radial Basis Function (RBF) kernel allows SVMs to operate in a similarity-based feature space without explicitly computing all similarity features.
> 
> **Mathematical foundation:**
> 
> - Uses a bell-shaped function: $K(x, y) = \exp(-\gamma |x-y|^2)$
> - Measures similarity between points that decreases with distance
> - Implicitly maps data to an infinite-dimensional space
> 
> **Implementation in scikit-learn:**
> 
> ```python
> from sklearn.pipeline import make_pipeline
> from sklearn.preprocessing import StandardScaler
> from sklearn.svm import SVC
> 
> # Create an RBF kernel SVM classifier
> rbf_kernel_svm_clf = make_pipeline(
>     StandardScaler(),  # Scale features for optimal performance
>     SVC(kernel="rbf", gamma=5, C=0.001)  # RBF kernel with specific gamma and C
> )
> 
> # Train the model
> rbf_kernel_svm_clf.fit(X, y)
> ```
> 
> **Key parameters:**
> 
> - **γ (gamma)**: Controls the width of the bell curve
>     - Higher γ: Narrower curves, more irregular boundaries, risk of overfitting
>     - Lower γ: Wider curves, smoother boundaries, risk of underfitting
> - **C**: Controls regularization strength
>     - Higher C: Stricter classification, complex boundaries
>     - Lower C: More relaxed classification, simpler boundaries
> 
> ![[Pasted image 20250426095915.png|500]]
> 
> The image shows how different combinations of γ and C affect the decision boundary:
> 
> - Top left (γ=0.1, C=0.001): Nearly linear boundary (underfitting)
> - Top right (γ=0.1, C=1000): Slightly more flexible boundary
> - Bottom left (γ=5, C=0.001): Moderate complexity with smooth curves
> - Bottom right (γ=5, C=1000): Highly complex boundary that closely follows data points
> 
> The RBF kernel is often the default choice for nonlinear SVMs because it works well across many types of data and provides a good balance between model complexity and computational efficiency.