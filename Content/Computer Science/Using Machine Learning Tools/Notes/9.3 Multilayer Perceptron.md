> [!motivation] Single is Too Simple
>
> Single-layer perceptrons, composed of a single layer of Threshold Logic Units (TLUs), marked an important early development in neural networks. They demonstrated the **ability to learn and classify patterns, provided those patterns were linearly separable**—meaning a single straight line (or hyperplane in higher dimensions) could distinguish between the classes.
>
> However, this fundamental characteristic also exposed a significant limitation:
> - **Linear Separability Constraint**: **Many real-world problems involve data distributions that are not linearly separable**. Single-layer perceptrons are inherently unable to solve these more complex classification tasks.
>
> **The XOR Problem: A Case in Point**
> A classic illustration of this limitation is the XOR (exclusive OR) problem:
> -   **Inputs & Outputs**:
>     -   Input (0, 0) → Output 0
>     -   Input (0, 1) → Output 1
>     -   Input (1, 0) → Output 1
>     -   Input (1, 1) → Output 0
> -   **The Challenge**: If you visualize these four points on a 2D plane, with inputs as coordinates and outputs as class labels, no single straight line can separate the points that output '0' from those that output '1'.
>

> [!idea] Multi Layer Perceptron
>
> The Multilayer Perceptron (MLP) is an advancement from the single-layer perceptron, designed to overcome its limitations. The fundamental architectural difference is the inclusion of one or more **hidden layers** of neurons (which can be TLUs or other types of artificial neurons) between the input and output layers.
>
> -   **Structure**: An MLP typically consists of:
>     1.  An **input layer** that receives the initial features.
>     2.  One or more **hidden layers**. These layers are not directly connected to the external input or output; they process intermediate representations of the data.
>     3.  An **output layer** that produces the final classification or regression result.
>     Each neuron in one layer is typically fully connected to every neuron in the subsequent layer.
>
> ![[Pasted image 20250513042355.png|500]]
>
> **Solving the Motivation (Overcoming Single-Layer Limitations):**
> The introduction of hidden layers is precisely what allows MLPs to address the problem of non-linearly separable data, such as the XOR problem, which single-layer perceptrons cannot solve.
>
> -   **Hierarchical Feature Learning**: Neurons in the hidden layers learn to transform the input data into new, more abstract representations. Each successive layer can build upon the features learned by the previous ones.
> -   **Non-Linear Decision Boundaries**: Through these transformations across multiple layers, an MLP can learn complex, non-linear decision boundaries.
>     -   For the XOR problem, for instance, a hidden layer can learn to map the original inputs (which are not linearly separable) into a new feature space where they *become* linearly separable. The output layer can then easily make the final classification using these transformed features.
> -   **Increased Representational Power**: By adding depth (more layers), MLPs gain significantly more representational power than single-layer perceptrons, enabling them to model intricate patterns and solve a much wider array of complex problems.

> [!consider]- MLPs: Feature Abstraction and the Path to Separability
>
> Multilayer Perceptrons (MLPs) with one or more hidden layers are foundational examples of feedforward neural networks. When these networks possess significant depth due to multiple hidden layers, they embody the core principles of **Deep Neural Networks (DNNs)**. The term "deep" in DNNs refers to this layered architecture.
>
> The power of MLPs largely stems from their ability to perform hierarchical feature abstraction and data transformation:
>
> -   **Hierarchical Feature Abstraction**: Each layer within an MLP processes the output from the preceding layer, learning to transform the data into progressively more abstract and complex representations.
>     -   Initial hidden layers might learn to identify basic patterns or simple features from the raw input data.
>     -   Subsequent layers then combine these elementary features to detect higher-level, more intricate features. For example, in image processing, early layers might detect edges, which are then used by later layers to identify shapes, then parts of objects, and finally, whole objects.
>
> -   **Transformation Towards Separability**: A critical outcome of this layered processing is the transformation of the input data.
>     -   Input data that is not linearly separable in its original form (as seen in problems like XOR) can be mapped by the hidden layers into a new high-dimensional feature space.
>     -   Within this transformed space, the different classes of data often become more readily separable, ideally linearly separable, by the neurons in the subsequent layers, particularly the output layer.
>     -   Thus, the hidden layers effectively work to "untangle" complex data, simplifying the task for the final classification stage.
>
> This capacity for automatic discovery and construction of a relevant feature hierarchy allows MLPs to model complex, non-linear relationships within data, making them significantly more powerful than single-layer perceptrons for a wide range of pattern recognition tasks.

> [!example] Example: DNN Layered Abstraction in Facial Recognition
>
> Deep Neural Networks (DNNs), such as Multilayer Perceptrons with several hidden layers, excel at tasks like facial recognition by processing information through a hierarchy of feature abstractions. Here’s a heavily abstracted view of how this might work when a DNN analyzes an image to recognize a face:
>
> -   **Input Layer: Raw Pixel Data**
>     -   The network first receives the raw image as a grid of pixel values (e.g., color intensities). At this stage, the data is highly detailed but lacks any explicit meaning or structure related to a face.
>
> -   **Early Hidden Layers: Detecting Low-Level Features**
>     -   The initial hidden layers process these pixels. They typically learn to identify very basic elements like:
>         -   Edges (horizontal, vertical, diagonal lines)
>         -   Corners and simple curves
>         -   Basic textures or color gradients
>     -   The output here is a set of "feature maps" highlighting where these elementary patterns occur in the image. This is the first level of abstraction from raw pixels.
>
> -   **Middle Hidden Layers: Assembling Mid-Level Features (Facial Components)**
>     -   These layers take the low-level features (edges, corners) as input.
>     -   They learn to combine these simpler patterns into more complex and recognizable shapes that might correspond to parts of a face, such as:
>         -   An eye shape
>         -   The curve of a nostril or lip
>         -   The general outline of an ear
>     -   The representation becomes more abstract, now dealing with rudimentary facial components rather than just lines and curves.
>
> -   **Later Hidden Layers: Constructing High-Level Features (Facial Structures)**
>     -   Building upon the detected facial components, these deeper layers learn to recognize more complete facial structures or configurations.
>     -   They might identify common arrangements of eyes, a nose, and a mouth that signify a generic face, or even start to capture features unique to specific facial types.
>     -   The abstraction level is now very high, representing an almost complete concept of a "face" or specific facial gestalts.
>
> -   **Output Layer: Final Classification/Identification**
>     -   This final layer takes the highly abstract representation from the last hidden layer.
>     -   Based on this processed information, it makes the final decision:
>         -   For face *detection*: It might output a probability that a face is present.
>         -   For face *identification*: It might output the most likely identity of the recognized face from a set of known individuals.
>
> Through this cascaded process of abstraction, the DNN automatically learns to transform raw sensory data into meaningful, high-level concepts like "a specific person's face," enabling it to perform complex pattern recognition tasks. Each layer refines and builds upon the understanding of the layer before it.

> [!idea] Backpropagation: Learning Through Accountable Adjustments
>
> Backpropagation is the cornerstone algorithm for training Multilayer Perceptrons and many other neural networks. It efficiently determines how to adjust the network's weights and biases to minimize its prediction errors. It essentially combines **reverse-mode automatic differentiation** (autodiff) with **Gradient Descent**.
>
> Imagine the MLP as a line of people (layers of neurons) passing a message (the input data) from one end to the other, with each person slightly modifying it based on their interpretation rules (weights and biases).
>
> The backpropagation process for a mini-batch of data involves these key phases:
>
> 1.  **Forward Pass (Processing the Message)**:
>     -   The input data (e.g., features of an image) is fed into the input layer.
>     -   It then travels through each hidden layer to the output layer. Each neuron receives inputs, computes a weighted sum, applies an activation function, and passes its output to neurons in the next layer.
>     -   Crucially, all intermediate values (like the output of each neuron) are preserved.
>     -   *Analogy*: The message goes down the line. Each person notes what they heard from the previous person and what specific message they passed on to the next.
>
> 2.  **Error Measurement (Checking the Final Message)**:
>     -   Once the data reaches the output layer, the network's predictions are compared to the actual target values using a loss function (e.g., mean squared error, cross-entropy). This calculates how "wrong" the network's predictions were for the current mini-batch.
>     -   *Analogy*: The final message at the end of the line is compared to the intended original message, and any discrepancies (errors) are noted.
>
> 3.  **Backward Pass (Assigning Responsibility for Errors)**:
>     -   This is where reverse-mode autodiff shines. It efficiently calculates the gradient of the error with respect to every single weight and bias in the network. It determines how much each parameter contributed to the overall error.
>     -   The process starts at the output layer, calculating how much its biases and connection weights contributed to the measured error (using the chain rule from calculus).
>     -   It then works backward, layer by layer. For each neuron, it determines how much its output contributed to the errors calculated for the neurons in the layer *ahead* of it. Based on this, and its own inputs (from the forward pass), it calculates the error contributions of its *own incoming weights and bias*.
>     -   *Analogy*: Starting from the final error, we go back up the line. The last person figures out how much their interpretation rules (weights/bias) led to the error. They then tell the person before them, "Here's the error signal I got, and here's how much of it seems to have come from what *you* told me." This continues, with each person assessing their share of the blame (error gradient) for all connections leading to them, based on the error signal from the person they passed the message to and the notes they kept from the forward pass.
>
> 4.  **Gradient Descent Step (Making Adjustments)**:
>     -   With the error gradients for all weights and biases calculated, the algorithm performs a Gradient Descent step. This means all connection weights and biases in the network are tweaked slightly in the direction that will reduce the error. The size of these tweaks is controlled by a learning rate.
>     -   *Analogy*: Everyone in the line slightly adjusts their interpretation and forwarding rules (weights/biases) based on their calculated responsibility, aiming to make fewer mistakes the next time a similar message comes through.
>
> This entire cycle (forward pass, error calculation, backward pass, weight update) is repeated for many mini-batches and for multiple passes through the entire training dataset (epochs), gradually improving the network's performance.

> [!example] A Single Learning Cycle: Backpropagation at Work
>
> Let's walk through a conceptual example of one learning cycle using backpropagation for a Multilayer Perceptron (MLP) processing a mini-batch of data.
>
> **Initial State**:
> -   The MLP has an input layer, one or more hidden layers, and an output layer.
> -   All connection weights and biases have been randomly initialized.
> -   A mini-batch of training instances (e.g., 32 images with their correct labels) is selected.
>
> **The Cycle:**
>
> 1.  **Forward Pass**:
>     -   Each instance from the mini-batch is fed into the input layer.
>     -   The data propagates forward through the network:
>         -   Neurons in the first hidden layer compute their outputs based on the inputs and their current weights/biases. These outputs are stored.
>         -   These outputs become inputs for the next hidden layer (if any), and so on.
>         -   Finally, the output layer neurons produce the MLP's predictions for each instance in the mini-batch.
>     -   *Key*: All intermediate outputs (activations) at each neuron are temporarily stored, as they are needed for the backward pass.
>
> 2.  **Error Calculation**:
>     -   The network's predictions (from the output layer) for the mini-batch are compared against the true target labels for those instances.
>     -   A loss function (e.g., cross-entropy for classification) quantifies the total error or "loss" for this mini-batch.
>
> 3.  **Backward Pass (Gradient Computation)**:
>     -   The algorithm starts at the output layer. It calculates how much a small change in each output neuron's bias and each weight connecting to an output neuron would affect the total loss. These are the error gradients for the output layer parameters. This involves using the chain rule of calculus.
>     -   The algorithm then moves backward to the last hidden layer. For each neuron in this layer, it calculates how much its output contributed to the errors of the output layer neurons it's connected to (again, using the chain rule and the gradients already computed for the output layer).
>     -   Based on this "error contribution" and the activations stored during the forward pass for this hidden layer, the gradients for its incoming weights and its bias are computed.
>     -   This process repeats, layer by layer, moving backward towards the input layer. Each layer uses the error signals (gradients) from the layer succeeding it and its own stored activations to determine the gradients for its own parameters.
>     -   This reverse flow efficiently determines how sensitive the overall loss is to every single weight and bias in the entire network.
>
> 4.  **Weight Update (Gradient Descent Step)**:
>     -   Once all error gradients have been computed, the network's parameters are updated.
>     -   Each weight and bias is adjusted slightly in the opposite direction of its gradient. This is because the gradient points in the direction of the steepest increase in error, so moving against it helps decrease the error.
>     -   The magnitude of this adjustment is controlled by the learning rate.
>     -   Mathematically: `new_weight = old_weight - learning_rate * gradient_of_error_wrt_weight`.
>
> **Outcome of the Cycle**:
> After this single cycle on the mini-batch, the MLP's weights and biases are slightly modified, hopefully making the network perform a little better (i.e., produce a lower error) on similar data. This entire four-step process is then repeated for the next mini-batch, and continues for many "epochs" (passes through the entire training dataset) until the network's performance is satisfactory or stops improving.