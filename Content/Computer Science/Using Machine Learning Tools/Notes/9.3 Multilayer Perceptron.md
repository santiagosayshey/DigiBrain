> [!motivation] Single is Too Simple
>
> Single-layer perceptrons, composed of a single layer of Threshold Logic Units (TLUs), marked an important early development in neural networks. They demonstrated the **ability to learn and classify patterns, provided those patterns were linearly separable**—meaning a single straight line (or hyperplane in higher dimensions) could distinguish between the classes.
>
> However, this fundamental characteristic also exposed a significant limitation:
> - **Linear Separability Constraint**: **Many real-world problems involve data distributions that are not linearly separable**. Single-layer perceptrons are inherently unable to solve these more complex classification tasks.
>
> **The XOR Problem: A Case in Point**
> A classic illustration of this limitation is the XOR (exclusive OR) problem:
> -   **Inputs & Outputs**:
>     -   Input (0, 0) → Output 0
>     -   Input (0, 1) → Output 1
>     -   Input (1, 0) → Output 1
>     -   Input (1, 1) → Output 0
> -   **The Challenge**: If you visualize these four points on a 2D plane, with inputs as coordinates and outputs as class labels, no single straight line can separate the points that output '0' from those that output '1'.
>

> [!idea] Multi Layer Perceptron
>
> The Multilayer Perceptron (MLP) is an advancement from the single-layer perceptron, designed to overcome its limitations. The fundamental architectural difference is the inclusion of one or more **hidden layers** of neurons (which can be TLUs or other types of artificial neurons) between the input and output layers.
>
> -   **Structure**: An MLP typically consists of:
>     1.  An **input layer** that receives the initial features.
>     2.  One or more **hidden layers**. These layers are not directly connected to the external input or output; they process intermediate representations of the data.
>     3.  An **output layer** that produces the final classification or regression result.
>     Each neuron in one layer is typically fully connected to every neuron in the subsequent layer.
>
> ![[Pasted image 20250513042355.png|500]]
>
> **Solving the Motivation (Overcoming Single-Layer Limitations):**
> The introduction of hidden layers is precisely what allows MLPs to address the problem of non-linearly separable data, such as the XOR problem, which single-layer perceptrons cannot solve.
>
> -   **Hierarchical Feature Learning**: Neurons in the hidden layers learn to transform the input data into new, more abstract representations. Each successive layer can build upon the features learned by the previous ones.
> -   **Non-Linear Decision Boundaries**: Through these transformations across multiple layers, an MLP can learn complex, non-linear decision boundaries.
>     -   For the XOR problem, for instance, a hidden layer can learn to map the original inputs (which are not linearly separable) into a new feature space where they *become* linearly separable. The output layer can then easily make the final classification using these transformed features.
> -   **Increased Representational Power**: By adding depth (more layers), MLPs gain significantly more representational power than single-layer perceptrons, enabling them to model intricate patterns and solve a much wider array of complex problems.

| 3   | Consider |                        |                                                                                                                                                      |
| --- | -------- | ---------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------- |
