> [!motivation] The Problem of Multi-Step Data Processing
> 
> Machine learning workflows require multiple preprocessing steps before modeling - from handling missing values to feature scaling and encoding.
> 
> Without a structured approach to manage these steps, several problems commonly occur:
> 
> - **Test data leakage**: Information from test data influencing the training process
> - **Inconsistent transformations**: Different preprocessing applied during training vs. prediction
> - **Preprocessing complexity**: Managing multiple transformations becomes error-prone
> 
> For example, imagine manually preprocessing a dataset:
> 
> ```python
> # Wrong approach (causes data leakage)
> scaler = StandardScaler()
> X_scaled = scaler.fit_transform(X)  # Fits on ALL data
> X_train, X_test = train_test_split(X_scaled, y)  # Too late! Test data influenced scaling
> 
> # Later in production
> new_data = scaler.transform(new_sample)  # Using parameters influenced by test data
> ```
> 
> This approach leads to overly optimistic model performance estimates that won't translate to real-world data.

> [!idea] Machine Learning Pipelines
> 
> ML pipelines are structured workflows that chain together multiple data processing steps and a model into a single object that can be trained and used for predictions.
> 
> Key characteristics:
> 
> - **Fit once, transform many**: Learn parameters from training data only, then apply them to all other data
> - **Sequenced operations**: Apply transformations in a fixed, consistent order
> - **Single interface**: Train the entire sequence with one command and make predictions with another
> 
> Core functionality:
> 
> - Pipeline components learn their parameters (like imputation values or scaling factors) only from training data
> - These learned parameters are stored and reused when transforming new data
> - The entire sequence is treated as a cohesive unit rather than separate steps
> 
> This approach prevents data leakage by isolating test data from the training process while ensuring all data goes through identical transformations.

> [!example] Implementing a Pipeline in scikit-learn
> 
> A complete preprocessing and modeling pipeline in scikit-learn:
> 
> ```python
> from sklearn.pipeline import Pipeline
> from sklearn.impute import SimpleImputer
> from sklearn.preprocessing import StandardScaler, OneHotEncoder
> from sklearn.compose import ColumnTransformer
> from sklearn.ensemble import RandomForestClassifier
> 
> # Define preprocessing for numerical columns
> numerical_transformer = Pipeline(steps=[
>     ('imputer', SimpleImputer(strategy='median')),
>     ('scaler', StandardScaler())
> ])
> 
> # Define preprocessing for categorical columns
> categorical_transformer = Pipeline(steps=[
>     ('imputer', SimpleImputer(strategy='most_frequent')),
>     ('onehot', OneHotEncoder(handle_unknown='ignore'))
> ])
> 
> # Combine preprocessing steps
> preprocessor = ColumnTransformer(
>     transformers=[
>         ('num', numerical_transformer, numerical_cols),
>         ('cat', categorical_transformer, categorical_cols)
>     ])
> 
> # Create the full pipeline
> full_pipeline = Pipeline(steps=[
>     ('preprocessor', preprocessor),
>     ('classifier', RandomForestClassifier())
> ])
> 
> # Train and evaluate in one step
> full_pipeline.fit(X_train, y_train)
> score = full_pipeline.score(X_test, y_test)
> ```
> 
> This pipeline handles:
> 
> 1. Missing value imputation (different strategies for numerical and categorical)
> 2. Numerical scaling
> 3. Categorical encoding
> 4. Model training
> 
> When new data arrives, `full_pipeline.predict(new_data)` applies all preprocessing steps automatically before generating predictions.
