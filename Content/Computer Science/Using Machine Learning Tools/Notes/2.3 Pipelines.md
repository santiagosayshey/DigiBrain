> [!motivation] The Problem of Multi-Step Data Processing
> 
> Machine learning workflows require multiple preprocessing steps before modeling - from handling missing values to feature scaling and encoding.
> 
> When these steps are applied separately and manually, several critical issues arise:
> 
> - **Test data leakage**: Accidentally fitting preprocessors (like scalers) on both training and test data
> - **Inconsistent transformations**: Using different preprocessing parameters between training and prediction
> - **Order dependency**: Changing the sequence of operations produces different results
> 
> For example, consider a model trained on scaled data where the scaler was fit on the full dataset (including test data). In production, new data wouldn't have been part of that scaling calculation, creating a mismatch between training and prediction environments - the model would receive differently distributed inputs than it was trained on, leading to degraded performance.

> [!idea] Machine Learning Pipelines
> 
> ML pipelines are sequences of data processing components that can be executed as a single unit, ensuring consistent preprocessing across training, validation, testing, and production data.
> 
> Core benefits:
> 
> - **Consistency**: Apply identical transformations to all data splits
> - **Reproducibility**: Ensure the same preprocessing steps in the same order
> - **Modularity**: Easily add, remove, or reorder processing steps
> - **Encapsulation**: Package transformers and models together
> 
> Key components typically include:
> 
> - Data cleaning steps (missing value handling)
> - Feature engineering transformations
> - Feature selection algorithms
> - Scaling/normalization techniques
> - The machine learning model itself

> [!example] Implementing a Pipeline in scikit-learn
> 
> A complete preprocessing and modeling pipeline in scikit-learn:
> 
> ```python
> from sklearn.pipeline import Pipeline
> from sklearn.impute import SimpleImputer
> from sklearn.preprocessing import StandardScaler, OneHotEncoder
> from sklearn.compose import ColumnTransformer
> from sklearn.ensemble import RandomForestClassifier
> 
> # Define preprocessing for numerical columns
> numerical_transformer = Pipeline(steps=[
>     ('imputer', SimpleImputer(strategy='median')),
>     ('scaler', StandardScaler())
> ])
> 
> # Define preprocessing for categorical columns
> categorical_transformer = Pipeline(steps=[
>     ('imputer', SimpleImputer(strategy='most_frequent')),
>     ('onehot', OneHotEncoder(handle_unknown='ignore'))
> ])
> 
> # Combine preprocessing steps
> preprocessor = ColumnTransformer(
>     transformers=[
>         ('num', numerical_transformer, numerical_cols),
>         ('cat', categorical_transformer, categorical_cols)
>     ])
> 
> # Create the full pipeline
> full_pipeline = Pipeline(steps=[
>     ('preprocessor', preprocessor),
>     ('classifier', RandomForestClassifier())
> ])
> 
> # Train and evaluate in one step
> full_pipeline.fit(X_train, y_train)
> score = full_pipeline.score(X_test, y_test)
> ```
> 
> This pipeline handles:
> 
> 1. Missing value imputation (different strategies for numerical and categorical)
> 2. Numerical scaling
> 3. Categorical encoding
> 4. Model training
> 
> When new data arrives, `full_pipeline.predict(new_data)` applies all preprocessing steps automatically before generating predictions.
