> [!motivation] Generalisation Error
> 
> Machine learning models often perform well on training data but fail to maintain that performance on unseen data. This discrepancy points to various sources of error that prevent models from generalizing effectively to new examples. Understanding these error sources helps **identify targeted improvements in model design, hyperparameter selection, and data collection strategies.**
> 
> - Different sources of error contribute differently to poor generalization
> - Identifying specific error types enables more effective model improvements
> - Error analysis provides insights into whether to collect more data, change model complexity, or adjust training procedures

> [!idea] Variance and Overfitting
> 
> Variance errors occur when models are overly sensitive to fluctuations in the training data, capturing noise rather than underlying patterns.
> 
> - **Overfitting**: Model performs well on training data but poorly on unseen data due to learning noise
> - **High model complexity** often increases variance error (too many parameters relative to data amount)
> - **Irreducible error**: Random noise in the data that no model can predict
> 
> |Variance Indicators|Potential Solutions|
> |---|---|
> |Large gap between training and validation error|Regularization techniques (L1, L2)|
> |Model performs perfectly on training data|Early stopping|
> |Performance worsens with more complex models|Reduce model complexity|
> |Error increases with feature count|Feature selection/reduction|

> [!idea] Bias and Systematic Errors
> 
> Bias errors represent systematic deviations preventing models from capturing the true relationships in the data.
> 
> - **Underfitting**: Model is too simple to capture the underlying patterns
> - **Suboptimal model selection**: Using linear models for non-linear relationships
> - **Poor hyperparameter choices**: Learning rates too high/low or incorrect regularization strength
> - **Data representativeness issues**: Training data doesn't reflect the true data distribution
> 
> Bias errors typically manifest as poor performance on both training and validation sets, indicating the model lacks the capacity to learn the underlying pattern.
> 
> |Bias Sources|Detection Methods|
> |---|---|
> |Model architecture|Similar errors across training/validation|
> |Hyperparameters|Performance plateaus early in training|
> |Training procedure|Consistently high error metrics|
> |Data quality/coverage|Poor performance on specific subgroups|

> [!example] Bias and Variance in Practice
> 
> **High Variance (Overfitting) Examples:**
> 
> - **Decision Trees**: A deep, unpruned decision tree perfectly classifies each training example but fails on new data by creating extremely specific decision boundaries.
>     - Error pattern: Near-zero training error, high validation error
> - **Neural Networks**: An overparameterized neural network memorizes training examples instead of learning generalizable features.
>     - Signs: Training loss approaches zero while validation loss increases
> 
> **High Bias (Underfitting) Examples:**
> 
> - **Linear Regression on Non-linear Data**: Using linear regression to model housing prices that have exponential relationships with square footage.
>     - Error pattern: Both training and test errors remain high
> - **Simple Classifiers on Complex Problems**: Attempting to use logistic regression for image classification tasks.
>     - Signs: Performance plateaus at mediocre levels on all datasets
> 

> [!consider] Regularization Techniques for Controlling Variance
> 
> Regularization methods impose constraints on model parameters to prevent overfitting by limiting model complexity while maintaining expressive power.
> 
> **L1 Regularization (Lasso)**
> 
> - Adds absolute value of weights to loss function: $L = \text{Loss} + \lambda \sum |w_i|$
> - Encourages sparse models by driving some weights exactly to zero
> - Performs implicit feature selection by eliminating irrelevant features
> 
> **L2 Regularization (Ridge)**
> 
> - Adds squared magnitude of weights to loss function: $L = \text{Loss} + \lambda \sum w_i^2$
> - Penalizes large weight values without forcing them to exactly zero
> - Works well when many features contribute moderately to the outcome
> 
> **Other Effective Techniques**
> 
> - **Dropout**: Randomly disables neurons during training to prevent co-adaptation
> - **Early stopping**: Halts training when validation error begins to increase
> - **Data augmentation**: Artificially expands training data to improve generalization
> 
> |Technique|Best Used When|Effect on Model|
> |---|---|---|
> |L1 (Lasso)|Many irrelevant features exist|Creates sparse models|
> |L2 (Ridge)|All features potentially useful|Shrinks all weights proportionally|
> |Dropout|Using deep neural networks|Simulates ensemble learning|
> |Early stopping|Limited validation data available|Prevents overtraining on noise|