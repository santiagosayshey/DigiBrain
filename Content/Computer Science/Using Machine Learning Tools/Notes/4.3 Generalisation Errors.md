> [!motivation] Generalisation Error
> 
> Machine learning models often perform well on training data but fail to maintain that performance on unseen data. This discrepancy points to various sources of error that prevent models from generalizing effectively to new examples. Understanding these error sources helps **identify targeted improvements in model design, hyperparameter selection, and data collection strategies.**
> 
> - Different sources of error contribute differently to poor generalization
> - Identifying specific error types enables more effective model improvements
> - Error analysis provides insights into whether to collect more data, change model complexity, or adjust training procedures

> [!idea] Variance and Overfitting
> 
> Variance errors occur when models are overly sensitive to fluctuations in the training data, capturing noise rather than underlying patterns.
> 
> - **Overfitting**: Model performs well on training data but poorly on unseen data due to learning noise
> - **High model complexity** often increases variance error (too many parameters relative to data amount)
> - **Irreducible error**: Random noise in the data that no model can predict
> 
> |Variance Indicators|Potential Solutions|
> |---|---|
> |Large gap between training and validation error|Regularization techniques (L1, L2)|
> |Model performs perfectly on training data|Early stopping|
> |Performance worsens with more complex models|Reduce model complexity|
> |Error increases with feature count|Feature selection/reduction|

> [!idea] Bias and Systematic Errors
> 
> Bias errors represent systematic deviations preventing models from capturing the true relationships in the data.
> 
> - **Underfitting**: Model is too simple to capture the underlying patterns
> - **Suboptimal model selection**: Using linear models for non-linear relationships
> - **Poor hyperparameter choices**: Learning rates too high/low or incorrect regularization strength
> - **Data representativeness issues**: Training data doesn't reflect the true data distribution
> 
> Bias errors typically manifest as poor performance on both training and validation sets, indicating the model lacks the capacity to learn the underlying pattern.
> 
> |Bias Sources|Detection Methods|
> |---|---|
> |Model architecture|Similar errors across training/validation|
> |Hyperparameters|Performance plateaus early in training|
> |Training procedure|Consistently high error metrics|
> |Data quality/coverage|Poor performance on specific subgroups|

> [!example] Bias and Variance in Practice
> 
> **High Variance (Overfitting) Examples:**
> 
> - **Decision Trees**: A deep, unpruned decision tree perfectly classifies each training example but fails on new data by creating extremely specific decision boundaries.
>     - Error pattern: Near-zero training error, high validation error
> - **Neural Networks**: An overparameterized neural network memorizes training examples instead of learning generalizable features.
>     - Signs: Training loss approaches zero while validation loss increases
> 
> **High Bias (Underfitting) Examples:**
> 
> - **Linear Regression on Non-linear Data**: Using linear regression to model housing prices that have exponential relationships with square footage.
>     - Error pattern: Both training and test errors remain high
> - **Simple Classifiers on Complex Problems**: Attempting to use logistic regression for image classification tasks.
>     - Signs: Performance plateaus at mediocre levels on all datasets
> 

> [!consider] How Regularization Prevents Overfitting
> 
> **Core Mechanism**: Regularization adds a complexity penalty to the loss function that the model tries to minimize:
> 
> $TotalLoss = DataLoss + λ × ComplexityPenalty$
> 
> This creates a direct trade-off: the model must balance fitting training data against keeping parameters simple/small. Complex parameter combinations needed to memorize noise in training data become mathematically costly.
> 
> **L1 Regularization** implements this with the absolute value of weights:
> 
> - $Penalty = λ \sum |w_i|$
> - Drives weights for marginally useful features completely to zero
> - Example: A model trying to predict house prices might learn that a specific house color appeared valuable in training data. L1 would eliminate this weight entirely if the color only helped through random correlation.
> 
> **L2 Regularization** implements this with squared weights:
> 
> - $Penalty = λ \sum w_i^2$
> - Proportionally shrinks all weights, preventing any from becoming extremely large
> - Example: Without L2, a neural network might place enormous weight on a specific pixel position that happened to correlate with labels in training. L2 forces the model to distribute importance across many features.
> 
> Both prevent overfitting by making it mathematically expensive for the model to use its parameters to memorize training examples rather than learn generalizable patterns.