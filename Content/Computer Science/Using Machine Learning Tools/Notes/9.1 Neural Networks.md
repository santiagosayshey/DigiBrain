> [!motivation] Learning From Nature
>
> Throughout history, **human innovation has drawn inspiration from the natural world**. The flight of birds informed aircraft design. The hooks of burdock plants led to Velcro. The camouflage of chameleons inspired adaptive materials. Nature consistently offers elegant solutions to complex problems.
>
> One of today's **greatest challenges is building machines that can learn independently**. Traditional computing excels at following precise instructions but struggles with tasks humans find intuitive:
> - Recognizing faces across different lighting and angles
> - Understanding natural language with all its contextual nuances
> - Identifying meaningful patterns in massive, unstructured datasets
>
> **Our brains already solve these problems effortlessly**. The human neural system—with its billions of interconnected neurons—doesn't follow rigid rules. Instead, it:
> - Processes information in parallel, allowing for incredible efficiency when handling complex inputs
> - Adapts continuously through experience, strengthening useful connections while weakening others
> - Excels at pattern recognition and generalization from limited examples
>
> This biological inspiration has transformed our daily technology landscape. Voice assistants interpret our spoken requests. Photo applications categorize images without manual tagging. Recommendation systems predict our preferences across streaming platforms and online stores.
>
> The fundamental insight becomes clear: **if we want machines to learn like humans, perhaps we should design their architecture to process information more like our brains.**

> [!idea] Artificial Neurons
>
> **How Real Neurons Work (Simply Put):**
> In our brains, neurons are the communication cells that process information:
> - Each neuron receives signals from many other neurons through branching connections
> - Some incoming signals are excitatory (encouraging the neuron to fire)
> - Others are inhibitory (discouraging firing)
> - If the combined excitatory signals exceed a certain threshold, and no strong inhibitory signals are present, the neuron "fires" and sends its own signal to other neurons
> - These signals travel across gaps called synapses, whose strengths can change with experience
>
> The McCulloch-Pitts neuron represents the first mathematical model of this biological process as a computational unit. It functions as a binary threshold device:
>
> - **Inputs:** Binary signals (0 or 1) from other neurons
> - **Processing:** Sum excitatory inputs; check inhibitory inputs
> - **Output:** Binary (0 or 1) based on threshold comparison
>
> The neuron follows these simple rules:
> - If ANY inhibitory input is 1 → Output is 0
> - Otherwise: If sum of excitatory inputs ≥ threshold → Output is 1
> - Otherwise → Output is 0
>
> **Visual Representation:**
> ```
>   Excitatory Inputs
>     x₁ = 1 ----→ \
>     x₂ = 0 ----→  \
>     x₃ = 1 ----→   \    ┌───────┐
>                     ├───┤       │
>   Inhibitory Inputs     │  Σ≥θ? │----→ Output (0 or 1)
>     x₄ = 0 ----→   /    │       │
>     x₅ = 0 ----→  /     └───────┘
>                  /       Threshold θ = 2
>                 /
> ```
>
> **Example Calculation:**
> - Excitatory inputs: x₁ = 1, x₂ = 0, x₃ = 1 (sum = 2)
> - Inhibitory inputs: x₄ = 0, x₅ = 0 (none active)
> - Threshold (θ) = 2
> - Result: No inhibitory inputs are active AND sum of excitatory inputs (2) ≥ threshold (2)
> - Therefore: Output = 1 (neuron fires)
>
> This model demonstrated how networks of such simple units could theoretically compute any logical function, establishing the foundational concept for neural networks, despite lacking learning capabilities and weighted connections found in modern neural networks.

> [!consider]- From Biology to Binary: What Really Happens in Neurons
>
> **Excitatory vs. Inhibitory: A Chemical Difference**
> In biological neurons, the distinction between excitatory and inhibitory inputs isn't about electrical charge directly:
> - Both types use chemical messengers (neurotransmitters) that cross the gap (synapse) between neurons
> - **Excitatory neurotransmitters** (like glutamate) cause positive ions to flow into the receiving neuron, pushing it closer to firing
> - **Inhibitory neurotransmitters** (like GABA) either allow negative ions in or positive ions out, pushing the neuron away from firing
> - The McCulloch-Pitts model simplifies this complex electrochemical process into binary values
>
> **What Happens When Neurons Fire?**
> For most of your brain, there's no direct "if this neuron fires, do X function" relationship:
> - Instead, neurons form chains and networks where patterns matter
> - For example, to wave your hand:
>   - Planning neurons fire in sequence → activating motor cortex neurons
>   - Motor neurons send electrical signals down your spinal cord
>   - These connect to specific muscle neurons that release chemicals (acetylcholine)
>   - The chemicals trigger muscle contraction in your arm and hand
> 
> It's like a relay race where the "baton" (signal) passes through multiple runners (neurons) before reaching the finish line (muscle movement). The brain mostly works through these complex patterns and pathways rather than single neurons triggering specific functions.
>
> The McCulloch-Pitts model captured the basic threshold logic but missed this critical aspect of neural function: meaning comes from patterns and pathways, not individual cell activity.

> [!consider]- Neurons to Computation: The Bridge
>
> The McCulloch-Pitts model established a profound connection between neuroscience and computer science that continues to influence both fields:
>
> - **Computational Universality**: By showing that neural-inspired elements could implement basic logical gates (AND, OR, NOT), they proved such networks could theoretically compute anything a digital computer could.
>
> This linking of biological thinking to computational theory:
> - Suggested the brain itself might function as a logical computing device
> - Helped establish the theoretical foundation for artificial intelligence
> - Preceded von Neumann's work on digital computer architecture
>
> This early work bridged two seemingly different worlds—biology and mathematics—by recognizing that the fundamental unit of thought might also be the fundamental unit of computation, an insight that continues to drive neural network research today.

> [!idea] TLUs
>
> Threshold Logic Units (TLUs) are an advancement from the basic McCulloch-Pitts neuron. They introduce more refined computational capabilities by incorporating numerical inputs, weights, and a bias.
>
> Key features of TLUs include:
> - **Numerical Inputs & Weights**: Unlike the binary-only inputs of their predecessors, TLUs process numerical inputs. Each input ($x_i$) is multiplied by a corresponding weight ($w_i$), which signifies that input's relative importance.
> - **Bias Term**: A bias ($b$) is added to the weighted sum. This term allows the TLU to adjust its activation threshold, offering greater flexibility in its decision-making.
>
> The computation in a TLU proceeds in two stages:
> 1.  **Weighted Sum Calculation**: First, the TLU computes a linear combination of its inputs and their weights, adding the bias. This sum ($z$) is calculated as:
>     $$
>     z = (w_1 x_1 + w_2 x_2 + \dots + w_n x_n) + b
>     $$
> 2.  **Step Function Application**: The sum $z$ is then processed by a step function to produce the TLU's final output.
>
> **Understanding Step Functions:**
> A step function transforms a numerical input (the weighted sum $z$) into a discrete output, typically one of two values (e.g., 0 or 1).
> - **Purpose**: Its role is to make a decision. It converts the continuous sum $z$ into a definitive output, determining if the neuron "fires" or not. This mimics a classification process where an input is assigned to a specific category based on whether a threshold is met.
> - **Example: Heaviside Step Function**: A common example is the Heaviside step function, $H(z)$, defined as:
>     $$
>     H(z) = \begin{cases} 0 & \text{if } z < 0 \\ 1 & \text{if } z \ge 0 \end{cases}
>     $$
>     This function outputs 0 if the weighted sum is negative and 1 if it is zero or positive.
>
> **Use in Binary Classification:**
> The output of a TLU using a Heaviside step function effectively performs binary classification. It categorizes inputs into one of two classes based on whether the weighted sum of inputs surpasses the implicit threshold set by the function.

> [!example] Training TLUs
>
> A Threshold Logic Unit (TLU) can be trained to classify data, such as distinguishing Iris flower species using features like petal length and width.
>
> -   **Objective of Training**: The core of training a TLU is to determine the optimal numerical values for its weights ($w_i$) and bias ($b$). These parameters define how the TLU separates data.
>
> -   **Training Process (Conceptual)**:
>     -   Inputs are specific features (e.g., petal length $x_1$, petal width $x_2$).
>     -   Known labels (e.g., 1 for *Iris setosa*, 0 for others) are provided for training samples.
>     -   An algorithm (not detailed here) adjusts weights and bias so the TLU's output—derived from the weighted sum $z = w_1x_1 + w_2x_2 + b$ followed by a step function—increasingly matches these known labels.
>
> -   **Classification Post-Training**:
>     -   Once optimal weights and bias are found, the TLU can classify new, unseen Iris flowers.
>     -   It calculates $z$ using the new flower's measurements and the learned weights/bias.
>     -   The step function applied to $z$ then yields the classification (e.g., 1 or 0).
>
> In essence, training equips the TLU with the right parameters to draw a decision line (or plane in higher dimensions) that separates the data effectively for its given classification task.

| 3   | Idea     | Perceptrons     | composed of one or more TLUs organised in a single layer. every TLU connected to every input and provides the final outputs. draw a simple perceptron with 2 inputs and 3 outputs in a code block.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| --- | -------- | --------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 4   | Consider | Perceptron Math | Can we try to explain this analgously please? Thanks to the magic of linear algebra, Equation 10-2 can be used to efficiently compute the outputs of a layer of artificial neurons for several instances at once. Equation 10-2. Computing the outputs of a fully connected layer hW,b (X) = ϕ (XW + b) In this equation: As always, X represents the matrix of input features. It has one row per instance and one column per feature. The weight matrix W contains all the connection weights. It has one row per input and one column per neuron. The bias vector b contains all the bias terms: one per neuron. The function ϕ is called the activation function: when the artificial neurons are TLUs, it is a step function (but we will discuss other activation functions shortly).<br><br>NOTE In mathematics, the sum of a matrix and a vector is undefined. However, in Data Science, we allow “broadcasting”: adding a vector to a matrix means adding it to every row in the matrix. So XW + b first multiplies X by W—which results in a matrix with one row per instance and one column per output—then adds the vector b to every row of that matrix, which adds each bias term to the corresponding output, for every instance. Moreover, ϕ is applied itemwise to each item in the resulting matrix |