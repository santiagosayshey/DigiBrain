> [!idea] Gradient Descent - Simply Explained
> 
> Gradient descent is a method to find the best values for a model's parameters (like slope and intercept in a line).
> 
> **What it does**:
> 
> - It's like finding the lowest point in a valley by taking small steps downhill
> - Each step uses information about how steep the slope is in each direction
> 
> **How to do it**:
> 
> 1. Start with any parameter values (often zeros or random numbers)
> 2. Calculate how wrong your model is with these values
> 3. Figure out which direction to change each parameter to reduce the error
> 4. Take a small step in that direction
> 5. Repeat until you stop improving
> 
> **Practical implementation**:
> 
> - The "gradient" is just how much the error changes if you adjust each parameter
> - The "learning rate" is how big each step should be
> - Each "iteration" is one complete step in the process
> 
> **Real-world analog**: Imagine you're in a hilly area with fog, trying to find the lowest point. You can only feel the ground right around you to tell which way is downhill. You take a small step downhill, then reassess, over and over until you reach the bottom.

> [!example] Gradient Descent - A Simple Walkthrough
> 
> Let's train a model to predict house prices with gradient descent:
> 
> **Step 1**: Start with simple values
> 
> - Model: price = intercept + slope × sqft
> - Initial values: intercept = 0, slope = 100
> 
> **Step 2**: Try these values on our data
> 
> - For a 1000 sqft house: predicted price = 0 + 100 × 1000 = $100,000
> - But the actual price is $150,000, so we're off by $50,000
> - We do this for all houses and find we're consistently predicting too low
> 
> **Step 3**: Determine how to improve
> 
> - Since we're predicting too low, we need to increase both our intercept and slope
> - The calculations tell us exactly how much to change each
> 
> **Step 4**: Update our values
> 
> - New intercept = old intercept + small adjustment
> - New slope = old slope + small adjustment
> 
> **Step 5**: Repeat until our predictions are good
> 
> - With each iteration, our predictions get better
> - Eventually, we find that intercept ≈ $30,000 and slope ≈ $130 per sqft works best
> 
> **That's it!** Gradient descent just automates this process of making small, intelligent adjustments to find the best parameter values.

> [!idea] Gradient Descent Maths
> 
> Gradient descent uses calculus to find the best model parameters. Here's what the math actually means:
> 
> **The model** predicts values: $h_\theta(x) = \theta_0 + \theta_1x$
> 
> - This is just a line equation where $\theta_0$ is the y-intercept and $\theta_1$ is the slope
> 
> **The cost function** measures how wrong our predictions are: $J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2$
> 
> - This means: "Take each prediction, subtract the actual value, square it, add all these up, and divide by twice the number of examples"
> - Squaring makes all errors positive and emphasizes larger errors
> 
> **The gradient** tells us which way to adjust parameters:
> 
> For $\theta_0$: $\frac{\partial J}{\partial \theta_0} = \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})$
> 
> - This means: "Average all the prediction errors"
> - If predictions are too high (positive errors), decrease $\theta_0$
> - If predictions are too low (negative errors), increase $\theta_0$
> 
> For $\theta_1$: $\frac{\partial J}{\partial \theta_1} = \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}$
> 
> - This means: "Multiply each error by its corresponding x-value, then average"
> - This weighs errors by the size of the input value
> - Larger inputs have more influence on the slope adjustment
> 
> **The update rule** makes the actual adjustment: $\theta_j := \theta_j - \alpha \cdot \frac{\partial J}{\partial \theta_j}$
> 
> - This means: "Take a small step ($\alpha$) in the opposite direction of the gradient"
> - If the gradient is positive, we decrease the parameter
> - If the gradient is negative, we increase the parameter
> - The size of the change depends on both the learning rate ($\alpha$) and the size of the gradient

> [!consider] The Learning Rate in Gradient Descent
> 
> The learning rate ($\alpha$) controls how large of a step we take during each iteration of gradient descent.
> 
> **What the learning rate does**:
> 
> - It's a multiplier that scales the size of each parameter update
> - It determines how quickly or slowly we move toward the minimum
> - Mathematically: $\theta_j := \theta_j - \alpha \cdot \text{gradient}$
> 
> **Effects of different learning rates**:
> 
> - **Too large**:
>     - Parameters may overshoot the minimum
>     - Can cause oscillation or divergence (cost increases instead of decreases)
>     - Training becomes unstable or fails completely
> - **Too small**:
>     - Convergence becomes extremely slow
>     - May get stuck in plateaus where progress is minimal
>     - Requires many more iterations to reach the minimum
> - **Just right**:
>     - Steady progress toward the minimum
>     - Fast enough to be efficient but small enough to be stable
> 
> **Practical selection techniques**:
> 
> - Start with a small value (e.g., 0.001, 0.01, or 0.1) and adjust based on results
> - Monitor cost function during training: should decrease consistently
> - Adaptive methods (like Adam, RMSprop) automatically adjust learning rates
> 
> ![[Pasted image 20250324211532.png]]

> [!consider] Step Sizes in Gradient Descent
> 
> Steps in gradient descent are not fixed or linear. They vary based on both the learning rate and the gradient.
> 
> **How step sizes are determined**:
> 
> - Step size = Learning rate × Gradient magnitude
> - Formula: $\Delta\theta_j = -\alpha \cdot \frac{\partial J}{\partial \theta_j}$
> 
> **Natural variation in steps**:
> 
> - **Steep regions** (large gradient): Steps are naturally larger
> - **Flat regions** (small gradient): Steps become naturally smaller
> - This adaptive behavior helps gradient descent navigate efficiently
> 
> **Consequences of this behavior**:
> 
> - Steps automatically get smaller as we approach a minimum
> - Progress slows down naturally near convergence
> - Different parameters may change at different rates depending on their gradients
> 
> This natural variation in step sizes is a key feature of gradient descent - it automatically slows down when precision is needed and moves quickly when far from the minimum.

> [!idea] Stopping Criteria for Gradient Descent
> 
> Gradient descent is an iterative algorithm that needs to know when to stop. It doesn't run indefinitely.
> 
> **Common stopping criteria**:
> 
> - **Maximum iterations**: Stop after a predefined number of iterations
>     - Simple but may stop too early or waste computation
> - **Threshold on cost improvement**: Stop when the change in cost function becomes very small
>     - Formula: $|J(\theta^{(t+1)}) - J(\theta^{(t)})| < \epsilon$
>     - Shows convergence based on actual performance improvement
> - **Threshold on parameter changes**: Stop when parameters barely change between iterations
>     - Formula: $||\theta^{(t+1)} - \theta^{(t)}|| < \epsilon$
>     - Directly measures stability of the solution
> - **Gradient magnitude**: Stop when the gradient becomes very close to zero
>     - Formula: $||\nabla J(\theta)|| < \epsilon$
>     - Indicates we've reached a flat region (likely a minimum)
> 
> **In practice**:
> 
> - Multiple criteria are often used together
> - Early stopping may be used to prevent overfitting
> - Monitoring validation error can provide additional stopping signals
> 
> When gradient descent stops, we consider the algorithm to have "converged" to a solution.