> [!idea] Gradient Descent - Simply Explained
> 
> Gradient descent is a method to find the best values for a model's parameters (like slope and intercept in a line).
> 
> **What it does**:
> 
> - It's like finding the lowest point in a valley by taking small steps downhill
> - Each step uses information about how steep the slope is in each direction
> 
> **How to do it**:
> 
> 1. Start with any parameter values (often zeros or random numbers)
> 2. Calculate how wrong your model is with these values
> 3. Figure out which direction to change each parameter to reduce the error
> 4. Take a small step in that direction
> 5. Repeat until you stop improving
> 
> **Practical implementation**:
> 
> - The "gradient" is just how much the error changes if you adjust each parameter
> - The "learning rate" is how big each step should be
> - Each "iteration" is one complete step in the process
> 
> **Real-world analog**: Imagine you're in a hilly area with fog, trying to find the lowest point. You can only feel the ground right around you to tell which way is downhill. You take a small step downhill, then reassess, over and over until you reach the bottom.

> [!example] Gradient Descent - A Simple Walkthrough
> 
> Let's train a model to predict house prices with gradient descent:
> 
> **Step 1**: Start with simple values
> 
> - Model: price = intercept + slope × sqft
> - Initial values: intercept = 0, slope = 100
> 
> **Step 2**: Try these values on our data
> 
> - For a 1000 sqft house: predicted price = 0 + 100 × 1000 = $100,000
> - But the actual price is $150,000, so we're off by $50,000
> - We do this for all houses and find we're consistently predicting too low
> 
> **Step 3**: Determine how to improve
> 
> - Since we're predicting too low, we need to increase both our intercept and slope
> - The calculations tell us exactly how much to change each
> 
> **Step 4**: Update our values
> 
> - New intercept = old intercept + small adjustment
> - New slope = old slope + small adjustment
> 
> **Step 5**: Repeat until our predictions are good
> 
> - With each iteration, our predictions get better
> - Eventually, we find that intercept ≈ $30,000 and slope ≈ $130 per sqft works best
> 
> **That's it!** Gradient descent just automates this process of making small, intelligent adjustments to find the best parameter values.

> [!idea] Gradient Descent Maths
> 
> Gradient descent uses calculus to find the best model parameters. Here's what the math actually means:
> 
> **The model** predicts values: $h_\theta(x) = \theta_0 + \theta_1x$
> 
> - This is just a line equation where $\theta_0$ is the y-intercept and $\theta_1$ is the slope
> 
> **The cost function** measures how wrong our predictions are: $J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2$
> 
> - This means: "Take each prediction, subtract the actual value, square it, add all these up, and divide by twice the number of examples"
> - Squaring makes all errors positive and emphasizes larger errors
> 
> **The gradient** tells us which way to adjust parameters:
> 
> For $\theta_0$: $\frac{\partial J}{\partial \theta_0} = \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})$
> 
> - This means: "Average all the prediction errors"
> - If predictions are too high (positive errors), decrease $\theta_0$
> - If predictions are too low (negative errors), increase $\theta_0$
> 
> For $\theta_1$: $\frac{\partial J}{\partial \theta_1} = \frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}$
> 
> - This means: "Multiply each error by its corresponding x-value, then average"
> - This weighs errors by the size of the input value
> - Larger inputs have more influence on the slope adjustment
> 
> **The update rule** makes the actual adjustment: $\theta_j := \theta_j - \alpha \cdot \frac{\partial J}{\partial \theta_j}$
> 
> - This means: "Take a small step ($\alpha$) in the opposite direction of the gradient"
> - If the gradient is positive, we decrease the parameter
> - If the gradient is negative, we increase the parameter
> - The size of the change depends on both the learning rate ($\alpha$) and the size of the gradient

> [!consider] The Learning Rate in Gradient Descent
> 
> The learning rate ($\alpha$) controls how large of a step we take during each iteration of gradient descent.
> 
> **What the learning rate does**:
> 
> - It's a multiplier that scales the size of each parameter update
> - It determines how quickly or slowly we move toward the minimum
> - Mathematically: $\theta_j := \theta_j - \alpha \cdot \text{gradient}$
> 
> **Effects of different learning rates**:
> 
> - **Too large**:
>     - Parameters may overshoot the minimum
>     - Can cause oscillation or divergence (cost increases instead of decreases)
>     - Training becomes unstable or fails completely
> - **Too small**:
>     - Convergence becomes extremely slow
>     - May get stuck in plateaus where progress is minimal
>     - Requires many more iterations to reach the minimum
> - **Just right**:
>     - Steady progress toward the minimum
>     - Fast enough to be efficient but small enough to be stable
> 
> **Practical selection techniques**:
> 
> - Start with a small value (e.g., 0.001, 0.01, or 0.1) and adjust based on results
> - Monitor cost function during training: should decrease consistently
> - Adaptive methods (like Adam, RMSprop) automatically adjust learning rates
> 
> ![[Pasted image 20250324211532.png]]

> [!consider] Step Sizes in Gradient Descent
> 
> Steps in gradient descent are not fixed or linear. They vary based on both the learning rate and the gradient.
> 
> **How step sizes are determined**:
> 
> - Step size = Learning rate × Gradient magnitude
> - Formula: $\Delta\theta_j = -\alpha \cdot \frac{\partial J}{\partial \theta_j}$
> 
> **Natural variation in steps**:
> 
> - **Steep regions** (large gradient): Steps are naturally larger
> - **Flat regions** (small gradient): Steps become naturally smaller
> - This adaptive behavior helps gradient descent navigate efficiently
> 
> **Consequences of this behavior**:
> 
> - Steps automatically get smaller as we approach a minimum
> - Progress slows down naturally near convergence
> - Different parameters may change at different rates depending on their gradients
> 
> This natural variation in step sizes is a key feature of gradient descent - it automatically slows down when precision is needed and moves quickly when far from the minimum.

> [!idea] Stopping Criteria for Gradient Descent
> 
> Gradient descent is an iterative algorithm that needs to know when to stop. It doesn't run indefinitely.
> 
> **Common stopping criteria**:
> 
> - **Maximum iterations**: Stop after a predefined number of iterations
>     - Simple but may stop too early or waste computation
> - **Threshold on cost improvement**: Stop when the change in cost function becomes very small
>     - Formula: $|J(\theta^{(t+1)}) - J(\theta^{(t)})| < \epsilon$
>     - Shows convergence based on actual performance improvement
> - **Threshold on parameter changes**: Stop when parameters barely change between iterations
>     - Formula: $||\theta^{(t+1)} - \theta^{(t)}|| < \epsilon$
>     - Directly measures stability of the solution
> - **Gradient magnitude**: Stop when the gradient becomes very close to zero
>     - Formula: $||\nabla J(\theta)|| < \epsilon$
>     - Indicates we've reached a flat region (likely a minimum)
> 
> **In practice**:
> 
> - Multiple criteria are often used together
> - Early stopping may be used to prevent overfitting
> - Monitoring validation error can provide additional stopping signals
> 
> When gradient descent stops, we consider the algorithm to have "converged" to a solution.


> [!idea] Stochastic Gradient Descent
> 
> Stochastic Gradient Descent (SGD) is a variation of gradient descent that uses a single random data point to compute the gradient at each step.
> 
> **Key differences from Batch Gradient Descent**:
> 
> - Uses only one training example per iteration (randomly selected)
> - Updates parameters more frequently
> - Formula: $\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta; x^{(i)}, y^{(i)})$
> 
> **Advantages**:
> 
> - Much faster for large datasets
> - Can escape local minima more easily due to noise
> - Requires less memory (processes one example at a time)
> - Often converges faster in terms of real time
> 
> **Disadvantages**:
> 
> - Parameter updates have high variance (noisy)
> - May never settle exactly at the minimum
> - Usually requires a decreasing learning rate schedule
> 
> **Implementation steps**:
> 
> 1. Shuffle training data randomly
> 2. For each example in the shuffled dataset:
>     - Calculate error on just that example
>     - Update parameters using that single example's gradient
> 3. Repeat for multiple epochs (passes through the dataset)
> 
> **Real-world analog**: Instead of carefully surveying the entire valley before each step, you randomly check a single spot near your feet to decide which way to move. This is less accurate for individual steps but lets you take many more steps in the same amount of time.

> [!consider] Finding the Global Minimum
> 
> Finding the true global minimum in gradient descent is challenging because we can only see the local landscape around our current position.
> 
> **The challenge**:
> 
> - Gradient descent only guarantees finding a local minimum
> - We can't know for certain if we've found the global minimum
> - For non-convex functions, multiple minima exist
> 
> **How we approach this problem**:
> 
> - **Convex functions**: For convex cost functions (like MSE in linear regression), any local minimum is the global minimum, so gradient descent will find it
>     
> - **Non-convex functions**: For more complex models (like neural networks):
>     
>     - Run multiple times with different random initializations
>     - Use techniques that add noise to escape local minima
>     - Apply momentum to help carry past shallow local minima
>     - Use global optimization techniques (simulated annealing, genetic algorithms)
> 
> **Practical indicators**:
> 
> - Very low cost value
> - Similar results from different initializations
> - Smooth convergence of the learning curve
> 
> **Important note**: In many machine learning applications, finding the absolute global minimum isn't always necessary. A good local minimum often provides excellent performance, and the search for the perfect global minimum may lead to overfitting.


> [!consider] Batch vs. Stochastic Gradient Descent: A Simple Comparison
> 
> **Batch Gradient Descent** (the simple version):
> 
> - Uses all training examples to calculate each update
> - Like checking the entire hiking trail before taking each step
> - Makes steady, predictable progress toward the minimum
> - Formula: "Average the gradients from all examples, then take one step"
> 
> **Stochastic Gradient Descent**:
> 
> - Uses just one random example to calculate each update
> - Like checking just one nearby spot before each step
> - Takes many more steps, but each is less accurate
> - Formula: "Calculate gradient from one random example, take a step, repeat with a new example"
> 
> **Key differences in simple terms**:
> 
> - **Speed**: Stochastic is typically much faster for large datasets (many smaller steps vs. few larger steps)
> - **Smoothness**: Batch follows a smooth path; Stochastic takes a noisy, zigzag path
> - **Memory**: Batch needs to process all data at once; Stochastic can handle one example at a time
> - **Randomness**: Stochastic introduces randomness that can help escape local minima
> 
> **Middle ground**: Mini-batch gradient descent uses small random subsets of data (e.g., 32-128 examples), combining benefits of both approaches.
> 
> ```image_goes_here
> Side-by-side comparison showing:
> 1. Batch GD: One large, precise step calculated from all data points
> 2. Stochastic GD: Multiple small, noisy steps, each using a single data point
> ```