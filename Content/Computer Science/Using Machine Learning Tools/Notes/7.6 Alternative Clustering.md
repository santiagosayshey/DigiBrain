> [!consider] Other Clustering Algorithms in Scikit-Learn
>
> Beyond K-Means and DBSCAN, Scikit-Learn offers several other clustering algorithms, each with different strengths and weaknesses:
>
> * **Agglomerative Clustering**:
>     * **How it works**: Builds a hierarchy of clusters from the bottom up. Starts with each instance as its own cluster and iteratively merges the closest pair of clusters until only one remains.
>     * **Output**: Produces a cluster tree (dendrogram) showing the merge hierarchy, allowing flexibility in choosing the number of clusters after fitting.
>     * **Pros**: Can capture clusters of various shapes and works with various distance metrics.
>     * **Cons**: Does not scale well to large datasets unless a connectivity matrix (specifying neighbors) is provided.
>
> * **BIRCH** (Balanced Iterative Reducing and Clustering using Hierarchies):
>     * **Designed for**: Very large datasets, especially when the number of features is relatively small (e.g., < 20).
>     * **How it works**: Builds a compact summary tree structure (CF-Tree) during training, storing enough information to assign points to clusters without keeping all data in memory.
>     * **Pros**: Fast (potentially faster than K-Means on large datasets with low dimensionality), memory-efficient.
>     * **Cons**: Primarily designed for lower-dimensional data.
>
> * **Mean-Shift**:
>     * **How it works**: Starts by placing a circle (defined by a `bandwidth` radius) around each data point. It iteratively shifts each circle towards the mean of the points within it, effectively moving towards areas of higher density. Points whose circles converge to the same location form a cluster.
>     * **Pros**: Can find arbitrarily shaped clusters, does not require specifying the number of clusters beforehand, relies on only one main hyperparameter (`bandwidth`).
>     * **Cons**: Tends to segment clusters that have internal density variations, does not scale well computationally to large datasets.
>
> Exploring these alternatives can be beneficial depending on the specific characteristics of your dataset (size, shape of clusters, density variations) and computational constraints.