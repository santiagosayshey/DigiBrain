> [!motivation] Beyond K-Means: Handling Complex Cluster Shapes
>
> While K-Means is efficient, it struggles when clusters have non-spherical shapes, varying sizes, or different densities, as it relies on minimizing distance to a central point (centroid). This limitation necessitates algorithms that define clusters based on different principles. DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is one such algorithm that identifies clusters based on the density of data points.

> [!idea] DBSCAN: Density-Based Clustering
>
> DBSCAN identifies clusters by defining them as continuous areas containing a high density of data points, separated by areas with low density. Unlike K-Means, it doesn't assume clusters are spherical or centered around a specific point.
>
> Here's the core logic:
> 1.  **Check Neighborhood Density**: For each data point, the algorithm examines a small region around it, called its **Îµ-neighborhood**. This includes all points within a specified distance `eps` from the original point.
> 2.  **Identify Core Points**: It counts how many points fall within this `eps`-neighborhood. If the count reaches a specified minimum number, `min_samples` (including the point itself), then that original point is marked as a **core instance**. Core instances signify points located in dense regions of the data.
> 3.  **Grow Clusters**: Clusters are formed starting from core instances. All points within the `eps`-neighborhood of a core instance become part of the same cluster. Crucially, if any of those neighboring points are *also* core instances, *their* neighbors are included too. This process continues, effectively connecting nearby dense regions into a single cluster, allowing clusters to form complex shapes.
> 4.  **Label Anomalies**: Any data point that is *not* a core instance and does *not* fall within the `eps`-neighborhood of any core instance is considered noise or an **anomaly**.
>
> This density-based approach allows DBSCAN to find arbitrarily shaped clusters and identify outliers naturally, relying on the `eps` (neighborhood size) and `min_samples` (density threshold) parameters. It works well when clusters are clearly separated by lower-density areas.

> [!example] DBSCAN Implementation and Prediction Workaround
>
> DBSCAN is available in Scikit-Learn. Here's an example using the `make_moons` dataset, which has non-spherical clusters:
>
> 1.  **Clustering**: Instantiate `DBSCAN` with `eps` and `min_samples`, then fit to the data.
> 
> ```python
> from sklearn.cluster import DBSCAN
> from sklearn.datasets import make_moons
> import numpy as np
>
> X, y = make_moons(n_samples=1000, noise=0.05, random_state=42) # Example data
>
> # Instantiate and fit DBSCAN
> dbscan = DBSCAN(eps=0.2, min_samples=5) # Note: eps=0.05 in text yields poor result
> dbscan.fit(X)
> ```
>
> 2.  **Results**: Access the clustering results:
>     * `labels_`: An array containing the cluster index for each instance. `-1` indicates an anomaly/outlier.
>     * `core_sample_indices_`: Indices of instances identified as core points.
>     * `components_`: The actual data points identified as core instances.
> 
> ```python
> # labels = dbscan.labels_ # Cluster index per instance (-1 for anomalies)
> # core_indices = dbscan.core_sample_indices_ # Indices of core points
> # core_points = dbscan.components_ # Coordinates of core points
> ```
> ![[Pasted image 20250428152431.png|500]]
>
> 3.  **Prediction Workaround**: DBSCAN itself does not have a `predict` method for new instances. A common workaround is to train a classifier on the results from DBSCAN, typically using the core instances as training data.
> 
> ```python
> from sklearn.neighbors import KNeighborsClassifier
>
> # Train a classifier (e.g., KNN) on core points and their labels
> knn = KNeighborsClassifier(n_neighbors=50) # Example K value
> # Fit KNN using core points coordinates and their corresponding labels
> knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])
>
> # Now use the trained KNN to predict clusters for new instances
> X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])
> predicted_labels = knn.predict(X_new)
> # predicted_labels example: array([1, 0, 1, 0])
> # probabilities = knn.predict_proba(X_new) # Optional: get probabilities
> ```
> ```image_goes_here
> Caption: Figure 9-15. Decision boundary of KNN classifier trained on DBSCAN results.
> Description: A plot showing the moons dataset clusters as identified by DBSCAN (using the good `eps` value). A decision boundary line, learned by the KNeighborsClassifier trained on the DBSCAN core points, separates the two cluster regions. Four new points (X_new) are shown as crosses, indicating which region (cluster) the KNN classifier assigns them to.
> ```
> *Note: To classify new points as anomalies using this workaround, one could check the distance to the nearest neighbor found by `knn.kneighbors()` and assign -1 if the distance exceeds a threshold like `eps`*.

