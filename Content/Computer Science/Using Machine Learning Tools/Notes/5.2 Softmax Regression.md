> [!motivation] Beyond Binary Classification
> 
> While logistic regression excels at binary classification tasks, many real-world problems involve choosing among multiple categories. Consider these scenarios:
> 
> - A medical diagnostic system needs to determine which of several possible diseases a patient might have based on symptoms
> - An image recognition system must identify different species of animals from photographs
> - A document classifier needs to sort text into multiple categories like business, sports, politics, or entertainment
> 
> Attempting to solve these problems with binary classifiers requires building multiple one-vs-rest models, which becomes inefficient and may lead to ambiguous predictions where an instance is assigned to multiple classes or none at all.
> 
> A more elegant approach would directly model the probability distribution across all possible classes, allowing the model to weigh evidence for each class simultaneously and ensure the probabilities form a coherent whole, summing to exactly 1.

> [!idea] Softmax Regression
> 
> Softmax Regression (also called Multinomial Logistic Regression) extends logistic regression to handle multiple classes directly through a two-step process:
> 
> **Step 1: Compute a score for each class** For each class k, calculate a score using a class-specific parameter vector θ^(k):
> 
> $s_k(x) = (θ^{(k)})^T x$
> 
> This creates a separate linear model for each class, resulting in K scores for K classes.
> 
> **Step 2: Convert scores to probabilities using the softmax function**
> 
> $\hat{p}_k = \frac{\exp(s_k(x))}{\sum_{j=1}^{K}\exp(s_j(x))}$
> 
> The softmax function:
> 
> - Takes the exponential of each score (making all values positive)
> - Normalizes these values so they sum to 1
> - Creates a proper probability distribution across all classes
> 
> **Making predictions** The model predicts the class with the highest probability:
> 
> $\hat{y} = \text{argmax}_k(s_k(x))$
> 
> This simplifies to choosing the class with the highest score, since the softmax function preserves the ranking of values.
> 
> **Training the model** The model is trained by minimizing the cross-entropy loss function:
> 
> $J(Θ) = -\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}y_k^{(i)}\log(\hat{p}_k^{(i)})$
> 
> Where y_k^(i) is typically 1 for the correct class and 0 for all others, making the loss function focus on maximizing the log probability of the correct class.

> [!example] Iris Species Classification
> 
> The classic iris dataset provides an excellent demonstration of Softmax Regression for multi-class classification. In this example, we'll classify iris flowers into three species based on their petal measurements.
> 
> ```python
> from sklearn.datasets import load_iris
> from sklearn.linear_model import LogisticRegression
> from sklearn.model_selection import train_test_split
> 
> # Load and prepare the data
> iris = load_iris(as_frame=True)
> X = iris.data[["petal length (cm)", "petal width (cm)"]].values
> y = iris["target"]  # 0=setosa, 1=versicolor, 2=virginica
> 
> # Split into training and test sets
> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
> 
> # Train the model with multi-class support
> # Note: scikit-learn's LogisticRegression automatically uses softmax 
> # when trained on more than two classes
> softmax_reg = LogisticRegression(C=30, random_state=42)
> softmax_reg.fit(X_train, y_train)
> 
> # Make predictions for a sample with petal length=5cm, petal width=2cm
> softmax_reg.predict([[5, 2]])  # Returns [2] (Iris virginica)
> 
> # Get probability estimates for each class
> softmax_reg.predict_proba([[5, 2]]).round(2)  
> # Returns [[0.00, 0.04, 0.96]] (96% confidence in Iris virginica)
> ```
> 
> **Visualizing the decision boundaries:**
> 
> ![[Pasted image 20250426104501.png|500]]
> 
> The visualization reveals several key insights:
> 
> - The decision boundaries between any two classes are linear (straight lines)
> - The colored regions show where each class has the highest probability
> - The curved lines within regions show probability contours (e.g., the line labeled 0.75 indicates points where Iris versicolor has a 75% estimated probability)
> - Each class dominates in different regions of the feature space: setosa (yellow) has small petals, versicolor (blue) has medium-sized petals, and virginica (green) has large petals
> - Where all decision boundaries meet, all three classes have equal probability (33%)

> [!consider] Practical Considerations
> 
> **Relationship to binary logistic regression:** Softmax Regression reduces to binary logistic regression when K=2. The mathematics simplify to the familiar sigmoid function, making it a true generalization.
> 
> **Linear decision boundaries:** While Softmax Regression handles multiple classes directly, its decision boundaries remain linear. For complex non-linear boundaries, consider kernel methods or neural networks with softmax output layers.
> 
> **Computational efficiency:** Compared to training K separate binary classifiers, Softmax Regression:
> 
> - Is more computationally efficient during training
> - Ensures probabilities sum to exactly 1 across all classes
> - Often yields better calibrated probabilities
> 
> **Regularization:** Like logistic regression, softmax models benefit from regularization:
> 
> - L2 regularization (as in the example with parameter C=30) helps prevent overfitting
> - Smaller values of C increase regularization strength
> 
> **Implementation details:** In scikit-learn, LogisticRegression automatically applies Softmax Regression when:
> 
> - The target variable contains more than two classes
> - solver="lbfgs" (the default) or other solvers that support multinomial loss
> - multi_class="multinomial" is specified (though this is automatic with appropriate solvers)
> 
> **Appropriate use cases:** Softmax Regression works best for mutually exclusive classes (a sample belongs to exactly one class). For multi-label problems where instances can belong to multiple classes simultaneously, binary relevance approaches are more appropriate.