
> [!idea] Approaches to ML
> 
> Machine learning algorithms follow two fundamental approaches that differ in how they process and learn from data.
> 
> **Example-based learning:**
> 
> - Makes predictions by **measuring similarity between new data and stored examples**
> - Retains training data in memory to use during prediction
> - Example: k-Nearest Neighbors classifier determining house prices by averaging the values of 5 most similar properties in its database, where "similarity" is calculated using distance metrics on features like square footage, location, and age
> - Example: A spam filter using Case-Based Reasoning that flags an email as spam because it shares 85% of its key phrase patterns with previously identified spam messages
> 
> **Model-based learning:**
> 
> - Creates a mathematical model that captures patterns in training data
> - Example: Linear Regression for house price prediction, which learns specific coefficients (e.g., +$120 per square foot, -$5,000 per mile from downtown, -$10,000 per decade of age)
> - Example: A Neural Network for image recognition that builds hierarchical representations—first detecting edges, then shapes, then object parts, finally whole objects—through millions of learned parameters
> 
> **Practical differences:**
> 
> - Example-based methods typically require minimal training time but more memory and slower predictions
> - Model-based methods require more upfront computation but result in compact models with faster prediction times
> - Example-based methods often excel with small, noise-free datasets while model-based methods can better handle large, complex data

> [!consider] Generalization and ML Approaches
> 
> Generalization refers to a model's **ability to perform well on previously unseen data rather than just memorizing training examples.**
> 
> **Generalization trade-offs:**
> 
> |Aspect|Example-based|Model-based|
> |---|---|---|
> |Overfitting risk|Lower (with sufficient examples)|Higher (complex models can memorize noise)|
> |Handling outliers|Naturally robust when using multiple neighbors|May need special techniques (regularization)|
> |Data requirements|Typically needs more data|Can work with less data if model assumptions match reality|
> |Interpretability|Often more transparent (similar examples)|Varies widely (linear models simple, deep networks complex)|
> 
> **Key generalization concepts:**
> 
> - Bias-variance tradeoff balances simplicity against flexibility
> - Training-validation split helps measure generalization
> - Regularization techniques prevent models from becoming too complex
> 
> Effective generalization connects back to our initial problem of static programs – the goal is creating systems that adapt to new situations without explicit programming for every case.
