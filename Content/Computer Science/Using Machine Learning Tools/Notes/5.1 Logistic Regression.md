> [!motivation] Classification Problems
> 
> Linear regression predicts continuous values, but many real-world problems require binary or categorical outcomes:
> 
> - Medical diagnosis (disease present/absent)
> - Spam detection (spam/not spam)
> - Credit approval (approve/deny)
> 
> When applying linear regression to binary outcomes (coded as 0/1):
> 
> - Predictions can fall outside meaningful range (< 0 or > 1)
> - Equal changes in input produce equal changes in output, regardless of current value
> - Residuals are non-normally distributed and heteroscedastic
> 
> These limitations make linear models fundamentally unsuitable for classification tasks.

> [!idea] Logistic Function Transformation
> 
> The logistic function (sigmoid) transforms a linear prediction into a probability between 0 and 1:
> 
> $$p(x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_n x_n)}} = \frac{1}{1 + e^{-z}}$$
> 
> Key properties:
> 
> - Output always between 0 and 1
> - S-shaped curve with steepest slope at z=0
> - Asymptotic behavior as z approaches ±∞
> 
> ![[Pasted image 20250407100105.png|400]]
> 
> The transformation creates a non-linear decision boundary while maintaining linear relationships in the log-odds space:
> 
> $$\log\left(\frac{p(x)}{1-p(x)}\right) = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n$$
> 
> This is called the logit transformation, connecting linear models to probability space.

> [!example] Linear vs. Logistic Models
> 
> Consider predicting credit default (1=default, 0=no default) based on credit card balance:
> 
> |Balance ($)|Linear Model|Logistic Model|
> |---|---|---|
> |0|-0.1|0.05|
> |500|0.1|0.15|
> |1000|0.3|0.30|
> |1500|0.5|0.50|
> |2000|0.7|0.70|
> |2500|0.9|0.85|
> |3000|1.1|0.95|
> 
> - Linear model produces invalid probabilities (negative at low balances, above 1 at high balances)
> - Logistic model correctly constrains predictions to [0,1]
> - Logistic model shows diminishing effect of balance changes at extreme values

> [!consider] Conceptual View of the Sigmoid Transformation
> 
> Think of the sigmoid function as a "squeezing" mechanism that takes any linear value and maps it to a probability between 0 and 1:
> 
> - For large negative values (e.g., -5 or lower), the output approaches 0
> - For large positive values (e.g., +5 or higher), the output approaches 1
> - Around zero, there's a smooth transition zone
> 
> This transformation has several conceptual advantages:
> 
> - **Natural probability interpretation**: Any linear combination of variables gets "compressed" into a valid probability
> - **Diminishing returns**: As you move toward the extremes, additional changes have less impact (adding more evidence to a near-certain case has little effect)
> - **Graceful handling of outliers**: Extreme input values can't produce extreme predictions beyond 0 or 1
> 
> The sigmoid essentially says: "Take all your linear evidence, sum it up (the z value), and I'll tell you how likely the outcome is based on that evidence."


> [!example] How Logistic Regression Works Mathematically
> 
> Let's use a simple student example to understand the mathematics of logistic regression.
> 
> **The Problem with Linear Models**
> 
> A linear model for predicting exam success based on hours studied ($x$) would be: $y = \beta_0 + \beta_1 x$
> 
> With parameters $\beta_0 = -0.5$ and $\beta_1 = 0.25$:
> 
> - For 0 hours: $y = -0.5 + 0.25(0) = -0.5$ (negative probability!)
> - For 4 hours: $y = -0.5 + 0.25(4) = 0.5$ (reasonable)
> - For 8 hours: $y = -0.5 + 0.25(8) = 1.5$ (over 100%!)
> 
> **The Logistic Transformation Solution**
> 
> We keep the same linear model but pass it through the sigmoid function: $p(x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x)}}$
> 
> With the same parameters:
> 
> - For 0 hours: $p = \frac{1}{1 + e^{0.5}} = 0.38$
> - For 4 hours: $p = \frac{1}{1 + e^{-0.5}} = 0.62$
> - For 8 hours: $p = \frac{1}{1 + e^{-1.5}} = 0.82$
> 
> All outputs are now valid probabilities between 0 and 1.
> 
> **What the Parameters Mean**
> 
> - $\beta_0 = -0.5$ is the "baseline" log-odds when no studying occurs
> - $\beta_1 = 0.25$ means each hour of study increases the log-odds by 0.25
> - The 50% threshold occurs when $\beta_0 + \beta_1 x = 0$, which is at $x = 2$ hours
> 
> The power of logistic regression is that it transforms the unbounded linear equation into bounded probabilities while preserving the interpretability of how each variable affects the outcome.