> [!motivation] The Need for Probabilistic Classification
> 
> Many real-world classification problems require more than just a yes/no decision - they need an assessment of certainty or risk.
> 
> **Limitations of linear models for classification:**
> 
> - Linear models produce unbounded outputs that don't naturally represent probabilities
> - Direct thresholding of raw scores doesn't provide confidence measures
> - Decision-making often requires knowing not just the classification but also its likelihood
> 
> Consider medical diagnosis, fraud detection, or customer churn prediction - in these cases, knowing that a patient has a 95% chance of having a condition provides much more actionable information than simply being told "positive" or "negative" without any confidence measure.
> 
> An ideal classification approach would combine the simplicity and interpretability of linear models with the ability to output well-calibrated probabilities that can inform decision-making and risk assessment.

> [!idea] Logistic Regression
> 
> Logistic Regression transforms a linear model into a probabilistic classifier by applying the sigmoid function to the weighted sum of inputs.
> 
> ![[Pasted image 20250426102650.png|500]]
> 
> **Core mechanism:**
> 
> - Computes a linear combination of input features: $z = \theta^T x = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + ... + \theta_n x_n$
> - Transforms this score into a probability using the sigmoid function: $\hat{p} = \sigma(z) = \frac{1}{1 + e^{-z}}$
> - Makes predictions using a threshold (typically 0.5): $\hat{y} = 1$ if $\hat{p} \geq 0.5$, otherwise $\hat{y} = 0$
> 
> **Key properties:**
> 
> - The sigmoid function maps any real number to a value between 0 and 1
> - When input is 0, output is exactly 0.5
> - Positive inputs produce probabilities above 0.5, negative inputs below 0.5
> - The steepness of the function in the middle creates a decision boundary
> 
> **Mathematical interpretation:**
> 
> - The model estimates log-odds (logit) of the positive class
> - The coefficients represent the change in log-odds when a feature increases by one unit
> - Positive coefficients increase the probability of the positive class
> - Negative coefficients decrease the probability of the positive class
> 
> Despite its name, Logistic Regression is a classification model, not a regression model. It serves as the foundation for many probabilistic classification approaches and can be extended to multi-class problems through techniques like one-vs-rest classification.

> [!consider] Training Logistic Regression Models
> 
> The training process for logistic regression centers on finding the parameter values that maximize the likelihood of the observed data.
> 
> **The cost function challenge:**
> 
> Unlike linear regression, logistic regression doesn't have a simple squared error cost function. Instead, it uses a specialized cost function called the log loss (or cross-entropy loss):
> 
> $J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} [y^{(i)}\log(\hat{p}^{(i)}) + (1-y^{(i)})\log(1-\hat{p}^{(i)})]$
> 
> **Why this function works:**
> 
> - When $y = 1$ (positive instance): Cost = $-\log(\hat{p})$
>     - If model predicts $\hat{p} \approx 1$ (correct): Cost approaches 0
>     - If model predicts $\hat{p} \approx 0$ (incorrect): Cost approaches infinity
> - When $y = 0$ (negative instance): Cost = $-\log(1-\hat{p})$
>     - If model predicts $\hat{p} \approx 0$ (correct): Cost approaches 0
>     - If model predicts $\hat{p} \approx 1$ (incorrect): Cost approaches infinity
> 
> **Optimization approach:**
> 
> - No closed-form solution exists (unlike linear regression's normal equation)
> - Gradient descent is typically used for optimization
> - The cost function is convex, guaranteeing convergence to the global minimum
> - The gradient for parameter $\theta_j$ is: $\frac{\partial J}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} (\hat{p}^{(i)} - y^{(i)})x_j^{(i)}$
> 
> This mathematical formulation elegantly encodes our objective: we want the model to assign high probabilities to positive instances and low probabilities to negative instances. The logarithmic nature of the cost function creates increasingly severe penalties as the model's confidence increases in the wrong direction.

> [!example] Classifying Iris Flowers with Logistic Regression
> 
> The famous iris dataset provides an excellent demonstration of logistic regression in action. Let's walk through building a classifier to identify Iris virginica flowers based on their petal measurements.
> 
> ![[Pasted image 20250426103910.png|500]]
> 
> **Understanding the dataset:**
> 
> First, we load the iris dataset and explore its structure:
> 
> ```python
> from sklearn.datasets import load_iris
> iris = load_iris(as_frame=True)
> 
> # Examine the dataset
> list(iris)  # Returns available attributes
> iris.data.head(3)  # View first 3 rows of feature data
> iris.target.head(3)  # View first 3 target values
> iris.target_names  # View class names
> ```
> 
> The dataset contains measurements of sepals and petals from three iris species. Each flower has four measurements, and we'll initially focus on just the petal width to detect Iris virginica.
> 
> **Training the model:**
> 
> ```python
> from sklearn.linear_model import LogisticRegression
> from sklearn.model_selection import train_test_split
> 
> # Extract feature (petal width) and target (is it virginica?)
> X = iris.data[["petal width (cm)"]].values
> y = iris.target_names[iris.target] == 'virginica'  # Creates boolean array
> 
> # Split into training and test sets
> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
> 
> # Train the model
> log_reg = LogisticRegression(random_state=42)
> log_reg.fit(X_train, y_train)
> ```
> 
> This code:
> 
> - Extracts petal width as our single feature
> - Creates a boolean target array where True = virginica, False = not virginica
> - Splits data into training and test sets (25% for testing by default)
> - Initializes and trains the logistic regression model
> 
> ![[Pasted image 20250426103922.png|500]]
> 
> **Visualizing probability estimates:**
> 
> After training, we can examine how the model makes predictions across different petal widths:
> 
> ```python
> import numpy as np
> import matplotlib.pyplot as plt
> 
> # Generate a range of petal widths to visualize
> X_new = np.linspace(0, 3, 1000).reshape(-1, 1)
> 
> # Get probability estimates for each value
> y_proba = log_reg.predict_proba(X_new)
> 
> # Find the decision boundary (where probability = 50%)
> decision_boundary = X_new[y_proba[:, 1] >= 0.5][0, 0]
> 
> # Plot the probabilities
> plt.plot(X_new, y_proba[:, 0], "b--", linewidth=2, label="Not Iris virginica proba")
> plt.plot(X_new, y_proba[:, 1], "g-", linewidth=2, label="Iris virginica proba")
> plt.plot([decision_boundary, decision_boundary], [0, 1], "k:", linewidth=2, 
>          label="Decision boundary")
> ```
> 
> The visualization shows:
> 
> - Blue dashed line: Probability of NOT being virginica
> - Green solid line: Probability of being virginica
> - Vertical dotted line: Decision boundary at ~1.65 cm
> - Blue squares at bottom: Non-virginica flower samples
> - Green triangles at top: Virginica flower samples
> 
> At the decision boundary, both probabilities equal 50%. If petal width > 1.65 cm, the model predicts virginica; otherwise, it predicts non-virginica.
> 
> ```python
> # Making predictions with the model
> decision_boundary  # Returns 1.65...
> log_reg.predict([[1.7], [1.5]])  # Returns [True, False]
> ```
> 
> ![[Pasted image 20250426103936.png|500]]
> 
> **Using multiple features:**
> 
> We can improve the model by using both petal width and length:
> 
> ```python
> # Train on two features
> X_2D = iris.data[["petal length (cm)", "petal width (cm)"]].values
> y = iris.target_names[iris.target] == 'virginica'
> X_train_2D, X_test_2D, y_train, y_test = train_test_split(X_2D, y, random_state=42)
> 
> log_reg_2D = LogisticRegression(random_state=42)
> log_reg_2D.fit(X_train_2D, y_train)
> ```
> 
> The third image shows the resulting decision boundary in the 2D feature space. The straight line represents points where the model estimates a 50% probability. Parallel lines represent other probability levels (15%, 30%, etc.).
> 
> **Key implementation details:**
> 
> - `LogisticRegression` in scikit-learn applies ℓ₂ regularization by default
> - The `C` parameter controls regularization strength (inverse of alpha in other models)
> - Higher C values mean less regularization
> - The model outputs class probabilities through `predict_proba()` and binary predictions through `predict()`
> 
> This example demonstrates how logistic regression creates a smooth probability curve that transitions between classes, making it ideal for binary classification problems where you need both decisions and confidence levels.