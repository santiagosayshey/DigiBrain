> [!motivation] Classification Problems
> 
> Linear regression predicts continuous values, but many real-world problems require binary or categorical outcomes:
> 
> - Medical diagnosis (disease present/absent)
> - Spam detection (spam/not spam)
> - Credit approval (approve/deny)
> 
> When applying linear regression to binary outcomes (coded as 0/1):
> 
> - Predictions can fall outside meaningful range (< 0 or > 1)
> - Equal changes in input produce equal changes in output, regardless of current value
> - Residuals are non-normally distributed and heteroscedastic
> 
> These limitations make linear models fundamentally unsuitable for classification tasks.

> [!idea] Logistic Function Transformation
> 
> The logistic function (sigmoid) transforms a linear prediction into a probability between 0 and 1:
> 
> $$p(x) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 x_1 + ... + \beta_n x_n)}} = \frac{1}{1 + e^{-z}}$$
> 
> Key properties:
> 
> - Output always between 0 and 1
> - S-shaped curve with steepest slope at z=0
> - Asymptotic behavior as z approaches ±∞
> 
> ![[Pasted image 20250407100105.png|400]]
> 
> The transformation creates a non-linear decision boundary while maintaining linear relationships in the log-odds space:
> 
> $$\log\left(\frac{p(x)}{1-p(x)}\right) = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n$$
> 
> This is called the logit transformation, connecting linear models to probability space.

> [!example] Linear vs. Logistic Models
> 
> Consider predicting credit default (1=default, 0=no default) based on credit card balance:
> 
> |Balance ($)|Linear Model|Logistic Model|
> |---|---|---|
> |0|-0.1|0.05|
> |500|0.1|0.15|
> |1000|0.3|0.30|
> |1500|0.5|0.50|
> |2000|0.7|0.70|
> |2500|0.9|0.85|
> |3000|1.1|0.95|
> 
> - Linear model produces invalid probabilities (negative at low balances, above 1 at high balances)
> - Logistic model correctly constrains predictions to [0,1]
> - Logistic model shows diminishing effect of balance changes at extreme values
