> [!motivation] Motivation
> 
> **Making sense of data often requires grouping similar items together**. Imagine you have thousands of emails - some are spam, others are important business communications. How do you automatically sort them?
> 
> Similarly, consider:
> 
> - Medical diagnostics: Is this cell malignant or benign?
> - Customer behavior: Will this customer churn or stay?
> - Image recognition: Is this image a cat or a dog?
> 
> In each case, we need to assign items to discrete categories based on their characteristics. This assignment process needs to be:
> 
> - Accurate
> - Scalable
> - Consistent

> [!idea] Classification
> 
> Classification is a supervised learning technique that **predicts which category an observation belongs to based on labeled training data.** These categories usually have no meaningful order. 
> 
> **How it works:**
> 
> - The algorithm learns from labeled examples (training data)
> - It identifies patterns that distinguish different classes
> - When given new data, it assigns the most likely category
> 
> **Types of classification problems:**
> 
> - Binary classification: two possible outcomes (spam/not spam)
> - Multi-class classification: three or more discrete categories (cat/dog/horse)
> - Multi-label classification: each instance can belong to multiple categories simultaneously
> 
> **Common classification algorithms:**
> 
> |Algorithm|Strengths|Typical Applications|
> |---|---|---|
> |Logistic Regression|Simple, interpretable|Binary outcomes, risk scoring|
> |Decision Trees|Handles non-linear data, interpretable|Rule-based decisions|
> |Random Forest|Resistant to overfitting, handles missing values|Complex classification tasks|
> |Support Vector Machines|Effective in high-dimensional spaces|Text classification, image recognition|
> |Neural Networks|Captures complex patterns|Image, speech, and natural language processing|
> 
> ![[Pasted image 20250324131007.png|600]]

> [!example] Email Spam Detection Case Study
> 
> A major email provider implemented a classification system to filter unwanted messages from users' inboxes.
> 
> **Training data preparation:**
> 
> - Collected 100,000 emails manually labeled as "spam" or "not spam"
> - These labeled examples teach the model to recognize patterns
> 
> **Feature extraction process:**
> 
> - **Engineered manually by data scientists:**
>     - Data scientists explicitly define which features to extract
>     - Word frequencies, email metadata, and structural elements don't emerge automatically
>     - The system counts specific words because it's told to look for them
>     - Engineers must update feature extraction rules as spam tactics evolve
> 
> **Random Forest classifier explained:**
> 
> - A collection of decision trees (100 in this case) that work together
> - Each decision tree:
>     - Resembles a flowchart with yes/no questions about features
>     - Example path: "Does email contain 'free'? → Yes → Is sender domain new? → Yes → Classify as spam"
>     - Different trees examine different feature combinations
> - The "forest" combines all tree predictions:
>     - If 78/100 trees classify as spam, the confidence is 78%
>     - System uses a threshold (e.g., >50%) to make final decision
> 
> **System performance:**
> 
> - 99.1% accuracy on new emails (test data)
> - Processing pipeline handles emails in under 50ms
> - When users correct misclassifications, these examples are added to training data
> 
> **Continuous improvement:**
> 
> - Model retrained weekly with newly labeled examples
> - Engineers regularly update feature extraction methods

> [!idea] Stochastic Gradient Descent (SGD)
> 
> Stochastic Gradient Descent is an **optimization method that minimizes classification errors by iteratively adjusting a model's parameters in small steps.**
> 
> **What it actually does:**
> 
> - Takes a small random subset (mini-batch) of training examples
> - Measures how wrong the model's predictions are on these examples
> - Identifies which direction to change each parameter to reduce errors
> - Moves parameters a small step in that direction
> - Repeats with different mini-batches until predictions improve
> 
> **Practical example:**
> 
> For a spam filter using logistic regression:
> 
> 1. The model has parameters (weights) for features like "free" or "urgent"
> 2. On a batch of 32 emails, the model makes several wrong predictions
> 3. SGD calculates that increasing the weight for "free" would reduce errors
> 4. It updates: weight_free = 0.2 → 0.26
> 5. After thousands of such updates across all weights, the model learns to classify correctly
> 
> **Key properties:**
> 
> - Uses randomness to escape poor solutions
> - Learning rate determines step size for updates
> - Works well with large datasets that don't fit in memory
> - Converges faster than processing all data at once