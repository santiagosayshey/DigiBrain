> [!motivation] The Challenge of Limited Labeled Data
>
> In many real-world machine learning scenarios, unlabeled data is abundant, but obtaining labeled data is expensive and time-consuming, often requiring manual expert effort. Training supervised models on only a small fraction of labeled instances typically results in suboptimal performance.
> * **Example:** Training a Logistic Regression classifier on only 50 labeled examples from the digits dataset might yield low accuracy (e.g., ~75%) compared to training on the full labeled training set (e.g., ~91%).
> * **Problem:** How can we effectively leverage the vast amount of unlabeled data alongside the few labeled examples to improve model performance without extensive manual labeling? This is the core goal of semi-supervised learning.

> [!idea] Semi-Supervised Learning via Clustering
>
> Clustering can be a powerful tool in semi-supervised learning by helping to understand the structure of the entire dataset (labeled and unlabeled). This structure can then be used to improve the supervised learning process:
>
> 1.  **Cluster the Data**: First, apply a clustering algorithm (like K-Means) to the full training dataset (ignoring the few labels initially) to partition it into $k$ clusters.
> 2.  **Active Learning Strategy**: Instead of labeling random instances, identify instances that are most representative of the discovered clusters (e.g., the points closest to each cluster centroid). Manually label only these few representative instances. Training a supervised model on these strategically chosen samples often yields significantly better results than using randomly chosen ones.
> 3.  **Label Propagation Strategy**: After labeling the representative instances (one per cluster), propagate that label to *all* other instances belonging to the same cluster based on the K-Means result (`kmeans.labels_`). This creates a fully labeled (though potentially noisy) dataset derived from the initial few labels and the data's structure. Training the supervised model on this larger, propagated dataset can further boost performance.
> 4.  **(Optional) Outlier Removal**: Performance might be further improved by identifying and excluding instances that are very far from their assigned cluster centroid before training on the propagated labels, as these might be outliers or points near ambiguous boundaries.

> [!example] Clustering for Semi-Supervised Learning (Scikit-Learn)
>
> Let's illustrate using the digits dataset, assuming only 50 initial labels (`n_labeled`).
>
> 5.  **Baseline Model**: Train a classifier on the few labeled instances.
>     ```python
>     from sklearn.datasets import load_digits
>     from sklearn.linear_model import LogisticRegression
>     import numpy as np
>
>     X_digits, y_digits = load_digits(return_X_y=True)
>     X_train, y_train = X_digits[:1400], y_digits[:1400]
>     X_test, y_test = X_digits[1400:], y_digits[1400:]
>
>     n_labeled = 50
>     log_reg_base = LogisticRegression(max_iter=10_000)
>     log_reg_base.fit(X_train[:n_labeled], y_train[:n_labeled])
>     baseline_score = log_reg_base.score(X_test, y_test)
>     # baseline_score might be around 0.748
>     ```
>
> 6.  **Cluster & Find Representatives (Active Learning)**: Cluster the *entire* training set. Find the instance closest to each centroid. (Assume these `k` representatives are then manually labeled correctly -> `y_representative_digits`).
>     ```python
>     from sklearn.cluster import KMeans
>
>     k = 50 # Number of clusters (e.g., same as labeled instances)
>     kmeans = KMeans(n_clusters=k, random_state=42)
>     # Get distances from each point to each centroid
>     X_digits_dist = kmeans.fit_transform(X_train)
>     # Find index of point closest to each centroid (axis=0)
>     representative_digit_idx = np.argmin(X_digits_dist, axis=0)
>     # Get the representative images
>     X_representative_digits = X_train[representative_digit_idx]
>     # Manually label these representatives (example labels)
>     y_representative_digits = np.array([1, 3, 6, 0, 7, 9, 2, 4, 8, 8, #... (needs 50 labels)
>                                         #... assumed to be manually assigned correctly ...
>                                         5, 1, 9, 9, 3, 7]) # Example from text
>
>     log_reg_repr = LogisticRegression(max_iter=10_000)
>     log_reg_repr.fit(X_representative_digits, y_representative_digits)
>     representative_score = log_reg_repr.score(X_test, y_test)
>     # representative_score might improve significantly, e.g., to 0.849
>     ```
>
> 7.  **Propagate Labels**: Assign the label of each representative digit to all instances in its corresponding cluster.
>     ```python
>     y_train_propagated = np.empty(len(X_train), dtype=np.int64)
>     for i in range(k):
>         # Assign the representative's label to all instances in cluster 'i'
>         y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]
>
>     log_reg_prop = LogisticRegression(max_iter=10_000) # Re-initialize model
>     log_reg_prop.fit(X_train, y_train_propagated)
>     propagated_score = log_reg_prop.score(X_test, y_test)
>     # propagated_score might improve further, e.g., to 0.894
>     ```
>
> 8.  **(Optional) Propagate with Outlier Removal**: Refine the propagated set by removing instances furthest from their centroids before training (code omitted for brevity, but involves calculating distances and percentile cutoffs per cluster). Training on this filtered set can sometimes yield the best results (e.g., ~90.9% accuracy in the text example).
>
> *Note: Scikit-Learn also provides dedicated classes like `LabelSpreading`, `LabelPropagation`, and `SelfTrainingClassifier` in `sklearn.semi_supervised` for different semi-supervised approaches*.