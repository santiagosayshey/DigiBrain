> [!motivation] The Challenge of Limited Labeled Data
>
> In many real-world machine learning scenarios, unlabeled data is abundant, but obtaining labeled data is expensive and time-consuming, often requiring manual expert effort. Training supervised models on only a small fraction of labeled instances typically results in suboptimal performance.
> * **Example:** Training a Logistic Regression classifier on only 50 labeled examples from the digits dataset might yield low accuracy (e.g., ~75%) compared to training on the full labeled training set (e.g., ~91%).
> * **Problem:** How can we effectively leverage the vast amount of unlabeled data alongside the few labeled examples to improve model performance without extensive manual labeling? This is the core goal of semi-supervised learning.

> [!idea] Semi-Supervised Learning via Clustering
>
> Clustering can be a powerful tool in semi-supervised learning by helping to understand the structure of the entire dataset (labeled and unlabeled). This structure can then be used to improve the supervised learning process:
>
> 1.  **Cluster the Data**: First, apply a clustering algorithm (like K-Means) to the full training dataset (ignoring the few labels initially) to partition it into $k$ clusters.
> 2.  **Active Learning Strategy**: Instead of labeling random instances, identify instances that are most representative of the discovered clusters (e.g., the points closest to each cluster centroid). Manually label only these few representative instances. Training a supervised model on these strategically chosen samples often yields significantly better results than using randomly chosen ones.
> 3.  **Label Propagation Strategy**: After labeling the representative instances (one per cluster), propagate that label to *all* other instances belonging to the same cluster based on the K-Means result (`kmeans.labels_`). This creates a fully labeled (though potentially noisy) dataset derived from the initial few labels and the data's structure. Training the supervised model on this larger, propagated dataset can further boost performance.
> 4.  **(Optional) Outlier Removal**: Performance might be further improved by identifying and excluding instances that are very far from their assigned cluster centroid before training on the propagated labels, as these might be outliers or points near ambiguous boundaries.

> [!example] Demonstrating Clustering's Benefit in Semi-Supervised Learning
>
> This example uses the digits dataset to show how K-Means clustering improves a Logistic Regression model when only 50 initial labels (`n_labeled`) are available.
>
> 1.  **Baseline Performance**: First, train a classifier using only the 50 known labels to see the starting point.
>     ```python
>     from sklearn.datasets import load_digits
>     from sklearn.linear_model import LogisticRegression
>     from sklearn.cluster import KMeans
>     import numpy as np
>
>     X_digits, y_digits = load_digits(return_X_y=True)
>     X_train, y_train = X_digits[:1400], y_digits[:1400]
>     X_test, y_test = X_digits[1400:], y_digits[1400:]
>
>     n_labeled = 50
>     log_reg_base = LogisticRegression(max_iter=10_000)
>     log_reg_base.fit(X_train[:n_labeled], y_train[:n_labeled])
>     baseline_score = log_reg_base.score(X_test, y_test)
>     print(f"Baseline score (50 random labels): {baseline_score:.4f}")
>     ```
>     *Result:* The accuracy is quite low (e.g., ~74.8% in the text's example), highlighting the limitation of having few random labels.
>
> 2.  **Clustering for Active Learning**: Cluster the *entire* training set (using unlabeled data structure). Identify representative instances (closest to centroids) and manually label *only these* 50 representatives. Train the model on these strategically chosen labels.
>     ```python
>     k = 50 # Number of clusters = number of labels we have
>     kmeans = KMeans(n_clusters=k, random_state=42)
>     X_digits_dist = kmeans.fit_transform(X_train) # Cluster & get distances
>     representative_digit_idx = np.argmin(X_digits_dist, axis=0) # Indices of reps
>     X_representative_digits = X_train[representative_digit_idx] # Get rep data points
>
>     # Assume y_representative_digits holds the correct manual labels for these 50 reps
>     # y_representative_digits = np.array([...]) # (Manual labeling step)
>
>     log_reg_repr = LogisticRegression(max_iter=10_000)
>     log_reg_repr.fit(X_representative_digits, y_representative_digits)
>     representative_score = log_reg_repr.score(X_test, y_test)
>     print(f"Representative score (50 representative labels): {representative_score:.4f}")
>     ```
>     *Result:* Accuracy jumps significantly (e.g., to ~84.9% in the text)! Labeling representative instances found via clustering provides much more value than labeling random ones.
>
> 3.  **Label Propagation**: Leverage the clustering further. Assign the label of each representative instance to all other instances within the same cluster. Train the model on this larger, pseudo-labeled dataset.
>     ```python
>     y_train_propagated = np.empty(len(X_train), dtype=np.int64)
>     for i in range(k):
>         y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]
>
>     log_reg_prop = LogisticRegression(max_iter=10_000)
>     log_reg_prop.fit(X_train, y_train_propagated)
>     propagated_score = log_reg_prop.score(X_test, y_test)
>     print(f"Propagated score (full pseudo-labeled set): {propagated_score:.4f}")
>     ```
>     *Result:* Another significant boost in accuracy (e.g., to ~89.4%) by using the cluster structure to expand the labeled set.
>
> 4.  **Refinement by Removing Outliers**: Improve the propagated set by removing the 1% of instances in each cluster that are furthest from their centroid, then train again.
>     ```python
>     percentile_closest = 99
>     X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
>     partially_propagated_mask = np.ones_like(X_cluster_dist, dtype=bool) # Start with all true
>     for i in range(k):
>         in_cluster = (kmeans.labels_ == i)
>         cluster_dist = X_cluster_dist[in_cluster]
>         if len(cluster_dist) > 0: # Avoid empty clusters
>             cutoff_distance = np.percentile(cluster_dist, percentile_closest)
>             above_cutoff = (X_cluster_dist > cutoff_distance)
>             # Mark outliers to be removed from the mask
>             partially_propagated_mask[in_cluster & above_cutoff] = False
>
>     X_train_partially_propagated = X_train[partially_propagated_mask]
>     y_train_partially_propagated = y_train_propagated[partially_propagated_mask]
>
>     log_reg_partial_prop = LogisticRegression(max_iter=10_000)
>     log_reg_partial_prop.fit(X_train_partially_propagated, y_train_partially_propagated)
>     partial_propagated_score = log_reg_partial_prop.score(X_test, y_test)
>     print(f"Partial Propagated score (outliers removed): {partial_propagated_score:.4f}")
>     ```
>     *Result:* Performance improves again (e.g., to ~90.9%), potentially matching or even slightly exceeding the accuracy achieved when training on the fully labeled dataset, demonstrating the power of combining clustering with limited labels and outlier removal.
>
> *Note: Scikit-Learn also provides dedicated classes like `LabelSpreading`, `LabelPropagation`, and `SelfTrainingClassifier` in `sklearn.semi_supervised` for alternative semi-supervised approaches*.