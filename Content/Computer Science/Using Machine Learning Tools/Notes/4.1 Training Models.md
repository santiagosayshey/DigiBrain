> [!motivation] Why Do We Need Model Training?
> 
> Models represent relationships between variables in data. Consider a linear model that predicts house prices based on square footage:
> 
> $\text{price} = \theta_0 + \theta_1 \times \text{sqft}$
> 
> With different parameter values ($\theta_0$, $\theta_1$), we get dramatically different predictions:
> 
> |Parameters|Prediction for 1500 sqft|
> |---|---|
> |$\theta_0 = 50000, \theta_1 = 100$|$$200,000$|
> |$\theta_0 = 10000, \theta_1 = 200$|$$310,000$|
> 
> How do we find the best parameters that make our model accurately reflect real-world data? This is where model training comes in.

> [!idea] Training Models
> 
> Training a model means systematically adjusting the model's parameters to minimize the errors between its predictions and the actual data.
> 
> **Cost Function**: A mathematical formula that measures how wrong a model's predictions are compared to actual data. Lower cost = better model.
> 
> **The Cost Function Graph**:
> 
> - X-axis: Model parameter values (like slope and intercept in linear regression)
> - Y-axis: Cost value (error measurement)
> - Each point on the graph represents how good (or bad) a specific parameter value is
> 
> **Key Features of the Cost Landscape**:
> 
> - **Global minimum**: The lowest point on the entire graph - represents the best possible parameter values
> - **Local minima**: Deceptive dips that look like the lowest point in their neighborhood but aren't the absolute lowest
> - **Valleys**: Regions where cost is decreasing
> - **Hills**: Regions where cost is increasing
> 
> Training algorithms navigate this landscape by:
> 
> 1. Starting at a random position on the graph
> 2. Determining which direction points downhill
> 3. Taking steps in that direction
> 4. Continuing until reaching a point where all directions go uphill (a minimum)
> 
> ```image_goes_here
> A 2D curve showing cost (y-axis) vs. one parameter (x-axis). The curve has multiple valleys with one deeper than others. Label the deepest valley as "global minimum" and a shallower valley as "local minimum". Show a ball at a high point and arrows indicating its potential path downhill.
> ```

> [!example] Training a Simple Linear Model
> 
> Let's train a linear model to predict housing prices using square footage.
> 
> **Data**:
> 
> |Square Footage|Actual Price|
> |---|---|
> |1000|$150,000|
> |1500|$210,000|
> |2000|$290,000|
> |2500|$350,000|
> 
> **Model**: $\text{price} = \theta_0 + \theta_1 \times \text{sqft}$
> 
> **Training process**:
> 
> 1. Start with initial parameters: $\theta_0 = 0, \theta_1 = 100$
> 2. Calculate predictions and errors:
>     - For 1000 sqft: Predict $100,000$, Error = $50,000$
>     - For 1500 sqft: Predict $150,000$, Error = $60,000$
>     - ...
> 3. Calculate loss (MSE): $\frac{1}{4}((50,000)^2 + (60,000)^2 + ...)$
> 4. Update parameters using gradient descent
> 5. Repeat until convergence
> 
> Final parameters: $\theta_0 = 30,000, \theta_1 = 130$ give us predictions closer to actual prices.