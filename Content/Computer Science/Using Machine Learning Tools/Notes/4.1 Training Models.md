> [!motivation] Why Do We Need Model Training?
> 
> Models represent relationships between variables in data. Consider a linear model that predicts house prices based on square footage:
> 
> $\text{price} = \theta_0 + \theta_1 \times \text{sqft}$
> 
> With different parameter values ($\theta_0$, $\theta_1$), we get dramatically different predictions:
> 
> |Parameters|Prediction for 1500 sqft|
> |---|---|
> |$\theta_0 = 50000, \theta_1 = 100$|$$200,000$|
> |$\theta_0 = 10000, \theta_1 = 200$|$$310,000$|
> 
> How do we find the best parameters that make our model accurately reflect real-world data? This is where model training comes in.

> [!idea] Training Models
> 
> Training a model means systematically adjusting the model's parameters to minimize the errors between its predictions and the actual data.
> 
> **Cost Function**: A mathematical formula that measures how wrong a model's predictions are compared to actual data. Lower cost = better model.
> 
> **The Cost Function Graph**:
> 
> - X-axis: Model parameter values (like slope and intercept in linear regression)
> - Y-axis: Cost value (error measurement)
> - Each point on the graph represents how good (or bad) a specific parameter value is
> 
> **Key Features of the Cost Landscape**:
> 
> - **Global minimum**: The lowest point on the entire graph - represents the best possible parameter values
> - **Local minima**: Deceptive dips that look like the lowest point in their neighborhood but aren't the absolute lowest
> - **Valleys**: Regions where cost is decreasing
> - **Hills**: Regions where cost is increasing
> 
> Training algorithms navigate this landscape by:
> 
> 1. Starting at a random position on the graph
> 2. Determining which direction points downhill
> 3. Taking steps in that direction
> 4. Continuing until reaching a point where all directions go uphill (a minimum)
> 
>![[Pasted image 20250324205107.png|500]]
>
>**Simple Explanation**: This graph shows, for every value of a parameter, what its cost (error) would be. The parameter value with the lowest cost is the best one - it gives us the most accurate model.

> [!example] Training a Simple Linear Model
> 
> Let's see how the cost function landscape works with a linear model for housing prices.
> 
> **Data**:
> 
> |Square Footage|Actual Price|
> |---|---|
> |1000|$150,000|
> |1500|$210,000|
> |2000|$290,000|
> |2500|$350,000|
> 
> **Model**: $\text{price} = \theta_0 + \theta_1 \times \text{sqft}$
> 
> **Cost function**: Mean Squared Error (MSE) $J(\theta_0, \theta_1) = \frac{1}{4}\sum_{i=1}^{4}(y_i - (\theta_0 + \theta_1 \times x_i))^2$
> 
> **Cost landscape exploration**:
> 
> |Parameters|Predictions|MSE Cost|
> |---|---|---|
> |$\theta_0 = 0, \theta_1 = 100$|$100K, 150K, 200K, 250K$|$4.125B$|
> |$\theta_0 = 50K, \theta_1 = 100$|$150K, 200K, 250K, 300K$|$1.125B$|
> |$\theta_0 = 30K, \theta_1 = 130$|$160K, 225K, 290K, 355K$|$127.5M$|
> 
> **Optimization process**:
> 
> 1. Our gradient descent algorithm starts at a random point (e.g., $\theta_0 = 0, \theta_1 = 100$)
> 2. Calculates which direction reduces the cost fastest
> 3. Takes a step in that direction (updates both parameters together)
> 4. Repeats until reaching a minimum
> 
> **Result**: After optimization converges, we find $\theta_0 \approx 30,000, \theta_1 \approx 130$
> 
> 
> This example shows we must optimize both parameters together, as changing one affects the optimal value of the other.