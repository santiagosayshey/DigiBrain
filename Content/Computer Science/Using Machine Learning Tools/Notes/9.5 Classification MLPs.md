> [!idea] Configuring MLPs for Classification Tasks
>
> Multilayer Perceptrons (MLPs) are highly versatile and can be effectively adapted for various classification tasks, including binary, multilabel binary, and multiclass classification. The key to tailoring an MLP for classification lies primarily in the configuration of its output layer and the choice of loss function.
>
> **Output Layer Configuration is Key:**
>
> -   **Binary Classification** (e.g., spam vs. ham):
>     -   **Output Neurons**: Typically, a single output neuron is used.
>     -   **Activation Function**: The sigmoid activation function ($\sigma(z) = 1 / (1 + \exp(-z))$) is applied to this neuron. Its output (between 0 and 1) is interpreted as the estimated probability of the instance belonging to the positive class.
>
> -   **Multilabel Binary Classification** (e.g., an email being "spam/ham" AND "urgent/non-urgent"):
>     -   **Output Neurons**: One output neuron is dedicated to each binary label. For the email example, you would have two output neurons.
>     -   **Activation Function**: Each output neuron uses the sigmoid activation function independently. The output of each neuron is the probability for its corresponding label.
>     -   *Note*: The output probabilities for different labels do not necessarily sum to 1, as an instance can possess multiple labels simultaneously (e.g., an email can be "ham" and "urgent").
>
> -   **Multiclass Classification** (mutually exclusive classes, e.g., digit recognition 0-9):
>     -   **Output Neurons**: One output neuron is used for each class (e.g., 10 neurons for digit recognition).
>     -   **Activation Function**: The **softmax** activation function is applied across all output neurons. Softmax ensures that the outputs are probabilities (each between 0 and 1) and that they sum up to 1 across all classes, reflecting the mutually exclusive nature of the classes.

> [!example] Example: MLP for Iris Dataset Classification
>
> This example demonstrates building a simple Multilayer Perceptron for a classification task using Scikit-Learn's `MLPClassifier` on the well-known Iris dataset. The Iris dataset is relatively simple and almost linearly separable, making a basic MLP structure sufficient.
>
> Key steps include loading the data, scaling features (important for MLP performance), defining the `MLPClassifier` with a simple hidden layer, and training it within a pipeline.
>
> **Scikit-Learn Implementation:**
>
> ```python
> from sklearn.datasets import load_iris
> from sklearn.model_selection import train_test_split
> from sklearn.neural_network import MLPClassifier
> from sklearn.pipeline import make_pipeline
> from sklearn.preprocessing import StandardScaler
> from sklearn.metrics import accuracy_score
>
> # Load the Iris dataset (3 classes)
> iris = load_iris()
> X = iris.data
> y = iris.target
>
> # Split data into training and testing sets
> X_train, X_test, y_train, y_test = train_test_split(
>     X, y, random_state=42, test_size=0.3, stratify=y
> )
>
> # Create a pipeline:
> # 1. Scale features using StandardScaler
> # 2. Classify using MLPClassifier
> # For this simpler task, a single hidden layer with 10 neurons is used.
> # max_iter is increased to ensure convergence for this example.
> clf_pipeline = make_pipeline(
>     StandardScaler(),
>     MLPClassifier(hidden_layer_sizes=[10], max_iter=1000, random_state=42)
> )
>
> # Train the classifier pipeline
> clf_pipeline.fit(X_train, y_train)
>
> # Make predictions on the test set
> y_pred = clf_pipeline.predict(X_test)
>
> # Evaluate the model's accuracy
> accuracy = accuracy_score(y_test, y_pred)
> # print(f"Test Accuracy: {accuracy:.4f}")
> # Expected accuracy would be high (e.g., >0.9) for this dataset.
> ```
>
> **Explanation:**
> -   The code first loads the Iris dataset and splits it.
> -   A `StandardScaler` is used because feature scaling is crucial for MLPs.
> -   The `MLPClassifier` is configured with a single hidden layer containing 10 neurons. By default, it uses the ReLU activation function for hidden layers and optimizes the cross-entropy loss function using the 'adam' solver.
> -   These steps are combined into a `Pipeline` for streamlined workflow.
> -   After training, the model's performance can be evaluated using metrics like accuracy. For the Iris dataset, a simple MLP like this can achieve high accuracy.