> [!summary] Key DNN Training Guidelines
>
> This provides a consolidation of practical guidelines and starting configurations for training Deep Neural Networks effectively, though experimentation based on the specific task is always encouraged.
> - **Default DNN Configuration**: For most cases, start with He initialization, ReLU (for shallower networks) or Swish (for deeper ones) activation, Batch Normalization (if deep), Early Stopping for regularization (with Weight Decay like AdamW if further regularization is needed), Nesterov Accelerated Gradients (NAG) or AdamW as the optimizer, and Performance or 1cycle learning rate scheduling.
> - **Self-Normalizing Network Configuration**: If using a simple stack of dense layers suitable for self-normalization, opt for LeCun initialization, SELU activation, no explicit normalization layer (as it's self-normalizing), Alpha Dropout if regularization is needed, NAG optimizer, and Performance or 1cycle learning rate scheduling.
> - **Essential Pre-steps**: Always normalize your input features. Crucially, try to reuse parts of a pretrained network (transfer learning) if a similar model exists. If not, consider unsupervised pretraining if you have ample unlabeled data, or pretraining on an auxiliary task if you have relevant labeled data for such a task.
> - **Achieving Sparse Models**: If a model with many zero weights (sparse model) is needed for efficiency, use L1 regularization or tools from the TensorFlow Model Optimization Toolkit (TF-MOT). Note that these techniques will typically break self-normalization.
> - **Optimizing for Low-Latency Predictions**: For models that need to make predictions very quickly, consider using fewer layers, fast activation functions (like ReLU or Leaky ReLU), folding Batch Normalization layers into preceding layers after training, ensuring the model is sparse, and potentially reducing float precision (e.g., to 16-bit or 8-bit, possibly using TF-MOT).
> - **Considerations for Risk-Sensitive Applications**: If your application is risk-sensitive or if inference latency is not a primary concern, using Monte Carlo (MC) Dropout can boost performance, provide more reliable probability estimates, and offer valuable uncertainty measures.
> - **Final Thoughts**: These guidelines provide strong starting points. While Keras offers great convenience for many tasks, TensorFlowâ€™s lower-level API is available when you need more fine-grained control, such as for custom loss functions or training algorithms.

