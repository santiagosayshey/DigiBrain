> [!motivation] Static Programs
> 
> Traditional software development relies on rigid, rule-based programs with explicitly coded logic.
> 
> When requirements change:
> 
> - Developers must manually update code
> - New edge cases require additional rules
> - Complexity increases exponentially with program size
> - Testing and validation cycles become lengthy
> 
> This approach becomes unsustainable as applications scale or operate in dynamic environments. **Systems break when encountering scenarios not explicitly programmed for,** leading to maintenance debt and reduced adaptability.

> [!idea] Machine Learning
> 
> Machine learning enables computers to **learn patterns from data without being explicitly programmed for every scenario.**
> 
> **Core concept:** Systems **derive rules automatically from examples rather than following pre-defined instructions.**
> 
> How it addresses static program limitations:
> 
> - Adapts to new patterns without code rewrites
> - Generalizes to unseen examples
> - Handles complexity through probabilistic approaches
> - Updates behavior by training on new data
> 
> Machine learning shifts the paradigm from "tell the computer what to do" to **"show the computer what good outcomes look like"** and letting it determine the optimal approach.

> [!consider] The "Teach a Man to Fish" Analogy
> 
> The proverb "Give a man a fish, and you feed him for a day. Teach a man to fish, and you feed him for a lifetime" parallels the shift from static programming to machine learning.
> 
> **Static Programming** = Giving a fish
> 
> - Solves specific problems with direct solutions
> - Requires new code for each new scenario
> 
> **Machine Learning** = Teaching to fish
> 
> - Provides frameworks for solving classes of problems
> - System continues to improve with experience
> - Adapts to changing conditions without constant intervention
> 
> This analogy highlights why machine learning offers more sustainable solutions for complex, evolving problem spaces where explicit programming would be impractical.

> [!idea] Supervised Learning
> 
> Supervised learning **trains models on labeled data (input-output pairs) to predict outputs for new inputs.**
> 
> **Two primary types:**
> 
> |Type|Purpose|Output|Example|
> |---|---|---|---|
> |Classification|Predicts which discrete category an input belongs to|Discrete label from a finite set (e.g., "yes"/"no", "cat"/"dog"/"bird")|Email filter that decides if a message belongs to "spam," "primary," or "promotions" categories based on content and sender information|
> |Regression|Predicts a continuous numerical value on a scale|Any real number within a range (e.g., $250,321 or 98.6°F)|House price predictor that takes features like location, square footage, and age to output an exact dollar amount ($472,890) rather than a price category|
> 
> **Key difference:**
> 
> - Classification divides inputs into separate, distinct groups (like sorting objects into labeled boxes)
> - Regression places inputs on a continuous spectrum (like measuring position on a number line)
> 
> 
> **Process:**
> 
> - Model is trained on labeled examples
> - Learning algorithm finds patterns connecting inputs to outputs
> - Model performance is evaluated on held-out test data

> [!idea] Unsupervised Learning
> 
> Unsupervised learning **finds patterns in data without labeled outputs or predefined categories.** The algorithm must discover structure on its own.
> 
> **Three main types:**
> 
> |Type|Purpose|Example|
> |---|---|---|
> |Clustering|Groups similar data points based on inherent similarities|Customer segmentation system that analyzes purchase history and browsing behavior to identify natural customer groups (like "budget shoppers," "luxury buyers," and "seasonal purchasers") without predefined categories|
> |Dimension Reduction|Compresses data to fewer dimensions while preserving important information|Image processing system that takes 1000-pixel images and reduces them to 50 key features that capture the same visual information, making pattern recognition faster without significant loss of accuracy|
> |Anomaly Detection|Identifies unusual patterns that don't conform to expected behavior|Credit card fraud detection system that learns normal spending patterns and flags transactions that deviate significantly, like unusual locations or purchase amounts, without being explicitly told what fraud looks like|
> 
> **Key characteristics:**
> 
> - Works with unlabeled data where "correct answers" aren't provided
> - Discovers hidden structures or relationships autonomously
> - Often used as a preprocessing step or for exploratory data analysis
> 
> **Common algorithms:**
> 
> - Clustering: K-means, Hierarchical Clustering, DBSCAN
> - Dimension Reduction: Principal Component Analysis (PCA), t-SNE
> - Anomaly Detection: Isolation Forest, One-Class SVM, Autoencoders

> [!idea] Reinforcement Learning
> 
> Reinforcement learning **trains agents to make sequences of decisions by interacting with an environment and receiving feedback** in the form of rewards or penalties.
> 
> **Key components:**
> 
> |Component|Description|
> |---|---|
> |Agent|The decision-maker that learns through trial and error|
> |Environment|The world in which the agent operates|
> |Actions|Choices the agent can make|
> |States|Different situations the agent can find itself in|
> |Rewards|Numerical feedback signals indicating success or failure|
> |Policy|The strategy that determines which actions to take in which states|
> 
> **Core process:**
> 
> - Agent observes current state of environment
> - Agent selects an action based on its policy
> - Environment transitions to a new state
> - Agent receives a reward signal
> - Agent updates its policy to maximize future rewards
> 
> **Example:** A game-playing AI learns to master chess not through studying labeled examples of good/bad moves, but by playing thousands of games against itself, receiving a positive reward only when it wins, and gradually discovering effective strategies through experimentation.
> 
> **Common algorithms:**
> 
> - Q-Learning
> - Deep Q Networks (DQN)
> - Proximal Policy Optimization (PPO)
> - Soft Actor-Critic (SAC)

> [!idea] Approaches to ML
> 
> Machine learning algorithms follow two fundamental approaches that differ in how they process and learn from data.
> 
> **Example-based learning:**
> 
> - Makes predictions by **measuring similarity between new data and stored examples**
> - Retains training data in memory to use during prediction
> - Example: k-Nearest Neighbors classifier determining house prices by averaging the values of 5 most similar properties in its database, where "similarity" is calculated using distance metrics on features like square footage, location, and age
> - Example: A spam filter using Case-Based Reasoning that flags an email as spam because it shares 85% of its key phrase patterns with previously identified spam messages
> 
> **Model-based learning:**
> 
> - Creates a mathematical model that captures patterns in training data
> - Example: Linear Regression for house price prediction, which learns specific coefficients (e.g., +$120 per square foot, -$5,000 per mile from downtown, -$10,000 per decade of age)
> - Example: A Neural Network for image recognition that builds hierarchical representations—first detecting edges, then shapes, then object parts, finally whole objects—through millions of learned parameters
> 
> **Practical differences:**
> 
> - Example-based methods typically require minimal training time but more memory and slower predictions
> - Model-based methods require more upfront computation but result in compact models with faster prediction times
> - Example-based methods often excel with small, noise-free datasets while model-based methods can better handle large, complex data

> [!consider] Generalization and ML Approaches
> 
> Generalization refers to a model's **ability to perform well on previously unseen data rather than just memorizing training examples.**
> 
> **Generalization trade-offs:**
> 
> |Aspect|Example-based|Model-based|
> |---|---|---|
> |Overfitting risk|Lower (with sufficient examples)|Higher (complex models can memorize noise)|
> |Handling outliers|Naturally robust when using multiple neighbors|May need special techniques (regularization)|
> |Data requirements|Typically needs more data|Can work with less data if model assumptions match reality|
> |Interpretability|Often more transparent (similar examples)|Varies widely (linear models simple, deep networks complex)|
> 
> **Key generalization concepts:**
> 
> - Bias-variance tradeoff balances simplicity against flexibility
> - Training-validation split helps measure generalization
> - Regularization techniques prevent models from becoming too complex
> 
> Effective generalization connects back to our initial problem of static programs – the goal is creating systems that adapt to new situations without explicit programming for every case.

> [!consider] Problems with Data
> 
> **Data quality and characteristics significantly impact machine learning model performance,** often creating challenges that technical improvements alone cannot solve.
> 
> **Quantity issues:**
> 
> - The "how much is enough?" question depends on problem complexity and model type
> - Simple linear models might need hundreds of examples, while deep neural networks require millions
> - Example: A speech recognition system trained on 10 hours of audio might recognize basic commands but fail at conversational speech, while one trained on 10,000 hours works across accents and environments
> 
> **Representativeness problems:**
> 
> - Training data must reflect the full distribution of future cases
> - Example: A facial recognition system trained primarily on young adult faces will perform poorly on elderly or child faces
> - Example: A loan approval model trained on historical data may perpetuate existing biases if protected attributes correlate with approval decisions
> 
> |Data Issue|Impact|Mitigation Strategy|
> |---|---|---|
> |Class imbalance|Model biases toward majority class|Oversampling minority class, synthetic data generation|
> |Concept drift|Model performance degrades over time|Continuous monitoring, periodic retraining|
> |Selection bias|Model learns patterns specific to how data was collected|Careful sampling design, external validation|
> 
> **Real-world data complications:**
> 
> - Noise: Random variation in measurements (e.g., sensor fluctuations in IoT devices)
> - Missing values: Incomplete records requiring imputation or special handling
> - Outliers: Extreme values that may represent errors or legitimate rare cases
> - Irrelevant features: Attributes that introduce noise rather than signal
> - Multicollinearity: Highly correlated features that complicate model interpretation

> [!consider] Problems with Models
> 
> Machine learning models face two fundamental challenges that represent opposite ends of the complexity spectrum.
> 
> **Overfitting:**
> 
> - Model learns noise and random fluctuations in training data rather than underlying patterns
> - Signs: Excellent performance on training data but poor performance on new data
> - Example: A decision tree with many branches perfectly classifying every training example by essentially memorizing the dataset rather than learning general rules
> 
> **Underfitting:**
> 
> - Model is too simple to capture the underlying structure of the data
> - Signs: Poor performance on both training and new data
> - Example: Using linear regression to model housing prices when the relationship between square footage and price follows a non-linear curve
> 
> |Problem|Causes|Solutions|
> |---|---|---|
> |Overfitting|Too many features relative to data points; excessive model complexity; noise in training data|Regularization (L1/L2); pruning complex models; early stopping; cross-validation; more training data|
> |Underfitting|Overly simple model architecture; insufficient features; excessive regularization|Feature engineering; more complex models; reduced regularization; polynomial features|
> 
> **Balancing approaches:**
> 
> - Data-centric: Collect more clean, representative data; perform feature selection
> - Model-centric: Adjust model complexity through hyperparameter tuning
> - Validation-centric: Use proper cross-validation to find optimal complexity level
> 
> The ideal model exists at the sweet spot between overfitting and underfitting—complex enough to capture true patterns but simple enough to generalize beyond training examples.


