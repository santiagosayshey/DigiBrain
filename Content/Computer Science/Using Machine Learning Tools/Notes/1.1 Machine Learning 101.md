> [!motivation] Static Programs
> 
> Traditional software development relies on rigid, rule-based programs with explicitly coded logic.
> 
> When requirements change:
> 
> - Developers must manually update code
> - New edge cases require additional rules
> - Complexity increases exponentially with program size
> - Testing and validation cycles become lengthy
> 
> This approach becomes unsustainable as applications scale or operate in dynamic environments. **Systems break when encountering scenarios not explicitly programmed for,** leading to maintenance debt and reduced adaptability.

> [!idea] Machine Learning
> 
> Machine learning enables computers to **learn patterns from data without being explicitly programmed for every scenario.**
> 
> **Core concept:** Systems **derive rules automatically from examples rather than following pre-defined instructions.**
> 
> How it addresses static program limitations:
> 
> - Adapts to new patterns without code rewrites
> - Generalizes to unseen examples
> - Handles complexity through probabilistic approaches
> - Updates behavior by training on new data
> 
> Machine learning shifts the paradigm from "tell the computer what to do" to **"show the computer what good outcomes look like"** and letting it determine the optimal approach.

> [!consider] The "Teach a Man to Fish" Analogy
> 
> The proverb "Give a man a fish, and you feed him for a day. Teach a man to fish, and you feed him for a lifetime" parallels the shift from static programming to machine learning.
> 
> **Static Programming** = Giving a fish
> 
> - Solves specific problems with direct solutions
> - Requires new code for each new scenario
> 
> **Machine Learning** = Teaching to fish
> 
> - Provides frameworks for solving classes of problems
> - System continues to improve with experience
> - Adapts to changing conditions without constant intervention
> 
> This analogy highlights why machine learning offers more sustainable solutions for complex, evolving problem spaces where explicit programming would be impractical.

> [!idea] Supervised Learning
> 
> Supervised learning **trains models on labeled data (input-output pairs) to predict outputs for new inputs.**
> 
> **Two primary types:**
> 
> |Type|Purpose|Output|Example|
> |---|---|---|---|
> |Classification|Predicts which discrete category an input belongs to|Discrete label from a finite set (e.g., "yes"/"no", "cat"/"dog"/"bird")|Email filter that decides if a message belongs to "spam," "primary," or "promotions" categories based on content and sender information|
> |Regression|Predicts a continuous numerical value on a scale|Any real number within a range (e.g., $250,321 or 98.6°F)|House price predictor that takes features like location, square footage, and age to output an exact dollar amount ($472,890) rather than a price category|
> 
> **Key difference:**
> 
> - Classification divides inputs into separate, distinct groups (like sorting objects into labeled boxes)
> - Regression places inputs on a continuous spectrum (like measuring position on a number line)
> 
> 
> **Process:**
> 
> - Model is trained on labeled examples
> - Learning algorithm finds patterns connecting inputs to outputs
> - Model performance is evaluated on held-out test data

> [!idea] Unsupervised Learning
> 
> Unsupervised learning **finds patterns in data without labeled outputs or predefined categories.** The algorithm must discover structure on its own.
> 
> **Three main types:**
> 
> |Type|Purpose|Example|
> |---|---|---|
> |Clustering|Groups similar data points based on inherent similarities|Customer segmentation system that analyzes purchase history and browsing behavior to identify natural customer groups (like "budget shoppers," "luxury buyers," and "seasonal purchasers") without predefined categories|
> |Dimension Reduction|Compresses data to fewer dimensions while preserving important information|Image processing system that takes 1000-pixel images and reduces them to 50 key features that capture the same visual information, making pattern recognition faster without significant loss of accuracy|
> |Anomaly Detection|Identifies unusual patterns that don't conform to expected behavior|Credit card fraud detection system that learns normal spending patterns and flags transactions that deviate significantly, like unusual locations or purchase amounts, without being explicitly told what fraud looks like|
> 
> **Key characteristics:**
> 
> - Works with unlabeled data where "correct answers" aren't provided
> - Discovers hidden structures or relationships autonomously
> - Often used as a preprocessing step or for exploratory data analysis
> 
> **Common algorithms:**
> 
> - Clustering: K-means, Hierarchical Clustering, DBSCAN
> - Dimension Reduction: Principal Component Analysis (PCA), t-SNE
> - Anomaly Detection: Isolation Forest, One-Class SVM, Autoencoders

> [!idea] Reinforcement Learning
> 
> Reinforcement learning **trains agents to make sequences of decisions by interacting with an environment and receiving feedback** in the form of rewards or penalties.
> 
> **Key components:**
> 
> |Component|Description|
> |---|---|
> |Agent|The decision-maker that learns through trial and error|
> |Environment|The world in which the agent operates|
> |Actions|Choices the agent can make|
> |States|Different situations the agent can find itself in|
> |Rewards|Numerical feedback signals indicating success or failure|
> |Policy|The strategy that determines which actions to take in which states|
> 
> **Core process:**
> 
> - Agent observes current state of environment
> - Agent selects an action based on its policy
> - Environment transitions to a new state
> - Agent receives a reward signal
> - Agent updates its policy to maximize future rewards
> 
> **Example:** A game-playing AI learns to master chess not through studying labeled examples of good/bad moves, but by playing thousands of games against itself, receiving a positive reward only when it wins, and gradually discovering effective strategies through experimentation.
> 
> **Common algorithms:**
> 
> - Q-Learning
> - Deep Q Networks (DQN)
> - Proximal Policy Optimization (PPO)
> - Soft Actor-Critic (SAC)

> [!idea] Approaches to ML
> 
> Machine learning algorithms follow two fundamental approaches that differ in how they process and learn from data.
> 
> **Example-based learning:**
> 
> - Makes predictions by **measuring similarity between new data and stored examples**
> - Retains training data in memory to use during prediction
> - Example: k-Nearest Neighbors classifier determining house prices by averaging the values of 5 most similar properties in its database, where "similarity" is calculated using distance metrics on features like square footage, location, and age
> - Example: A spam filter using Case-Based Reasoning that flags an email as spam because it shares 85% of its key phrase patterns with previously identified spam messages
> 
> **Model-based learning:**
> 
> - Creates a mathematical model that captures patterns in training data
> - Example: Linear Regression for house price prediction, which learns specific coefficients (e.g., +$120 per square foot, -$5,000 per mile from downtown, -$10,000 per decade of age)
> - Example: A Neural Network for image recognition that builds hierarchical representations—first detecting edges, then shapes, then object parts, finally whole objects—through millions of learned parameters
> 
> **Practical differences:**
> 
> - Example-based methods typically require minimal training time but more memory and slower predictions
> - Model-based methods require more upfront computation but result in compact models with faster prediction times
> - Example-based methods often excel with small, noise-free datasets while model-based methods can better handle large, complex data

> [!consider] Generalization and ML Approaches
> 
> Generalization refers to a model's **ability to perform well on previously unseen data rather than just memorizing training examples.**
> 
> **Generalization trade-offs:**
> 
> |Aspect|Example-based|Model-based|
> |---|---|---|
> |Overfitting risk|Lower (with sufficient examples)|Higher (complex models can memorize noise)|
> |Handling outliers|Naturally robust when using multiple neighbors|May need special techniques (regularization)|
> |Data requirements|Typically needs more data|Can work with less data if model assumptions match reality|
> |Interpretability|Often more transparent (similar examples)|Varies widely (linear models simple, deep networks complex)|
> 
> **Key generalization concepts:**
> 
> - Bias-variance tradeoff balances simplicity against flexibility
> - Training-validation split helps measure generalization
> - Regularization techniques prevent models from becoming too complex
> 
> Effective generalization connects back to our initial problem of static programs – the goal is creating systems that adapt to new situations without explicit programming for every case.

> [!consider] Problems with Data
> 
> **Data quality and characteristics significantly impact machine learning model performance,** often creating challenges that technical improvements alone cannot solve.
> 
> **Quantity issues:**
> 
> - The "how much is enough?" question depends on problem complexity and model type
> - Simple linear models might need hundreds of examples, while deep neural networks require millions
> - Example: A speech recognition system trained on 10 hours of audio might recognize basic commands but fail at conversational speech, while one trained on 10,000 hours works across accents and environments
> 
> **Representativeness problems:**
> 
> - Training data must reflect the full distribution of future cases
> - Example: A facial recognition system trained primarily on young adult faces will perform poorly on elderly or child faces
> - Example: A loan approval model trained on historical data may perpetuate existing biases if protected attributes correlate with approval decisions
> 
> |Data Issue|Impact|Mitigation Strategy|
> |---|---|---|
> |Class imbalance|Model biases toward majority class|Oversampling minority class, synthetic data generation|
> |Concept drift|Model performance degrades over time|Continuous monitoring, periodic retraining|
> |Selection bias|Model learns patterns specific to how data was collected|Careful sampling design, external validation|
> 
> **Real-world data complications:**
> 
> - Noise: Random variation in measurements (e.g., sensor fluctuations in IoT devices)
> - Missing values: Incomplete records requiring imputation or special handling
> - Outliers: Extreme values that may represent errors or legitimate rare cases
> - Irrelevant features: Attributes that introduce noise rather than signal
> - Multicollinearity: Highly correlated features that complicate model interpretation

> [!consider] Problems with Models
> 
> Machine learning models can go wrong in two main ways - they can be too simple or too complex for the problem.
> 
> **Overfitting:**
> 
> - Model tries to memorize training examples instead of learning general patterns
> - Signs: Works perfectly on training data but fails on new examples
> - Example: A model that memorizes that "John Smith always buys coffee on Tuesdays" rather than learning that "people often buy coffee in the morning"
> 
> **Underfitting:**
> 
> - Model is too simple to capture important patterns in the data
> - Signs: Performs poorly even on the training data
> - Example: Using a straight line to predict house prices when size, location, and age all matter in complex ways
> 
> |Problem|Simple Explanation|How to Fix|
> |---|---|---|
> |Overfitting|The model is overthinking - like memorizing test answers without understanding the subject|Use simpler models; get more training data; focus on the most important features|
> |Underfitting|The model is too simplistic - like using only addition when calculus is needed|Try more complex models; add more relevant features; remove restrictions on the model|
> 
> **Finding the right balance:**
> 
> - Too simple = misses important patterns
> - Too complex = gets distracted by noise and irrelevant details
> - Just right = captures true patterns that generalize to new situations
> 
> A good model is like a good summary - it captures the important points without including every minor detail.

> [!idea] Selecting a Model
> 
> Choosing the right machine learning model is more practical than theoretical - **test a few options and pick the one that works best on new data.**
> 
> **Simple selection process:**
> 
> - Choose 2-3 model types to try (start with simpler ones)
> - Split your data into two parts: training data and test data
> - Train each model on the training data
> - Test how well each model performs on the test data (data it hasn't seen)
> - Select the model that performs best on the test data
> 
> **Example:** For predicting house prices, you might try:
> 
> - A simple linear model (draws a straight line through data)
> - A decision tree (creates yes/no decision rules)
> 
> After training both, you test them on houses they haven't seen before:
> 
> - Linear model: Average prediction error of $45,000
> - Decision tree: Average prediction error of $32,000
> 
> You would select the decision tree since it generalizes better to new data.
> 
> **Key insight:** The best model isn't necessarily the most complex one - it's the one that makes the most accurate predictions on new data without memorizing the training examples.

> [!idea] Model Validation
> 
> Model validation ensures your machine learning model will work well on new data it hasn't seen before.
> 
> **Basic data splitting:**
> 
> - **Training set** (typically 70-80% of data): Used to teach the model patterns
> - **Test set** (typically 20-30% of data): Used to evaluate how well the model generalizes
> 
> **Why this matters:** Without splitting the data, you can't tell if your model is actually learning useful patterns or just memorizing examples.
> 
> **Example:** Imagine teaching a child to recognize dogs by showing them 100 dog photos. If you test them using those same 100 photos, you don't know if they truly understand what a dog is or just memorized those specific images. Testing with new dog photos reveals if they've truly learned.
> 
> **Simple validation approach:**
> 
> 1. Randomly divide your dataset (e.g., 80% for training, 20% for testing)
> 2. Train your model using only the training data
> 3. Test your model on the test data
> 4. Measure how well it performs on this unseen data
> 
> **Common mistake to avoid:** Never use your test data during training or model selection - it defeats the purpose of testing on "unseen" data and gives you an overly optimistic view of your model's performance.

> [!consider] The Validation Set: A Third Dataset
> 
> When building machine learning models, a third dataset called the "validation set" provides additional benefits beyond the basic training-test split.
> 
> **What it is:**
> 
> - A separate portion of data used during the model development process
> - Not used for direct training, not used for final evaluation
> - Typically 10-20% of your original data
> 
> **Why you need it:** The validation set helps you make decisions about your model before final testing. It acts as a "practice test" that lets you:
> 
> - Compare different model types
> - Tune model settings (like tree depth or learning rate)
> - Make changes to your approach without "using up" your test data
> 
> **Three-way split example:**
> 
> - Training set (60%): Teach the model patterns
> - Validation set (20%): Make model decisions and improvements
> - Test set (20%): Final evaluation only once
> 
> **Real-world scenario:** Imagine you're building a spam filter. You train 5 different models on your training data. All 5 models work differently. Which should you choose? You check each one against the validation set to pick the best performer. Only after selecting and finalizing your model do you check its performance on the test set to get an honest estimate of how it will work in production.
> 
> This approach prevents you from inadvertently "cheating" by repeatedly using the test set during development, which would give you an overly optimistic performance estimate.