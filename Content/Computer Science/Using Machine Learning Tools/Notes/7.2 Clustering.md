> [!motivation] The Need for Grouping Unlabeled Data
> 
> Humans intuitively group similar objects, like recognizing plants of the same species, even without formal botanical knowledge. This natural ability highlights a common data analysis need.
> 
> - Consider datasets like the Iris dataset shown below. When labels (species) are present (left panel), classification algorithms can assign instances to predefined categories.
> - However, much data lacks these labels (right panel). In such cases, classification methods cannot be applied.
> - The challenge arises: how can we automatically discover inherent groupings within unlabeled data?
> 
> ![[Pasted image 20250428125444.png|500]]
> 
> This necessitates methods that can identify structure without supervision.

> [!idea] Clustering
> 
> Clustering is an unsupervised learning task focused on **identifying similar instances within a dataset and assigning them to groups, known as clusters**. Unlike classification, it does not use predefined labels.
> 
> - **Objective**: To partition data such that instances within a cluster are similar to each other, and instances in different clusters are dissimilar.
> - **Algorithm Variability**: Different clustering algorithms define "clusters" differently. Some seek dense regions (DBSCAN), others group around central points (K-Means), and some build hierarchies of clusters.
> - **Feature Utilization**: Algorithms can leverage multiple features, potentially revealing clusters not obvious in lower-dimensional views (like the Iris dataset example).
> 
> Clustering has diverse applications:
> 
> - Customer segmentation for targeted marketing.
> - Exploratory data analysis to understand dataset structure.
> - Dimensionality reduction via cluster affinity vectors.
> - Feature engineering by adding cluster affinity as features.
> - Anomaly detection (outliers often have low affinity to all clusters).
> - Semi-supervised learning by propagating labels within clusters.
> - Search engines for finding similar items (e.g., images).
> - Image segmentation by grouping pixels of similar color.

> [!idea] K-Means Algorithm
> 
> K-Means is an iterative algorithm designed to **partition a dataset into a predefined number (k) of distinct, non-overlapping clusters.** It is particularly effective for identifying "blob-like" groups of instances.
> 
> ![[Pasted image 20250428132156.png|300]]
> 
> - **Objective**: Minimize the variance within each cluster. It achieves this by finding k central points (centroids) and assigning each data point to the cluster associated with the nearest centroid.
> - **Requirement**: The number of clusters, k, must be specified by the user before running the algorithm.
> - **Outcome**: The process results in assigning a cluster index (0 to kâˆ’1) to each instance. The decision boundaries between clusters form a Voronoi tessellation around the final centroids/
> 
> 
> 
> ![[Pasted image 20250428132258.png|300]]

> [!example] K-Means Implementation (Scikit-Learn)
>
> The K-Means algorithm can be implemented efficiently using Scikit-Learn. The process involves instantiating the model with the desired number of clusters ($k$) and then fitting it to the data.
>
> ```python
> from sklearn.cluster import KMeans
> from sklearn.datasets import make_blobs
> import numpy as np
>
> # Example data (replace [...] with actual parameters)
> # X, y = make_blobs([...])
> # Note: y contains true cluster IDs but is not used by K-Means
>
> k = 5
> kmeans = KMeans(n_clusters=k, random_state=42)
> # Fit model to data X and predict cluster index for each instance in X
> y_pred = kmeans.fit_predict(X)
> ```
>
> Key outputs and methods after fitting:
>
> * **Cluster Assignments (`labels_`)**: Stores the cluster index assigned to each instance of the training data `X`.
>     - This is a NumPy array with the same length as `X`.
>     - Each element is an integer from $0$ to $k-1$, indicating the cluster assigned to the corresponding instance.
>     - It is equivalent to the `y_pred` returned by `fit_predict(X)`.
> ```python
> # Access predicted labels for training data:
> training_labels = kmeans.labels_
> # print(training_labels)
> # Example: array([4, 0, 1, ..., 2, 1, 0], dtype=int32)
> ```
>
> * **Centroid Coordinates (`cluster_centers_`)**: Provides the coordinates of the final $k$ cluster centroids.
>     - This is a NumPy array with shape `(k, n_features)`, where `n_features` is the number of features in `X`.
>     - Each row contains the feature values defining the center of a cluster.
> ```python
> # Access centroid coordinates:
> centroids = kmeans.cluster_centers_
> # print(centroids)
> # Example: array([[-2.80, 1.80], [ 0.21, 2.26], ...])
> ```
>
> * **Predicting on New Data (`predict`)**: Assigns new data points to the closest cluster based on the learned centroids.
>     - Takes an array `X_new` of new instances as input.
>     - Returns a NumPy array containing the predicted cluster index (from $0$ to $k-1$) for each instance in `X_new`.
> ```python
> # Define new data points
> X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
> # Predict the cluster for each new point
> new_labels = kmeans.predict(X_new)
> # print(new_labels)
> # Example Output (from text): array([1, 1, 2, 2], dtype=int32)
> ```
>
> This demonstrates the basic workflow: initialize `KMeans` with $k$, use `fit_predict` on training data to find centroids and assign labels, inspect `labels_` and `cluster_centers_`, and use `predict` for new instances.