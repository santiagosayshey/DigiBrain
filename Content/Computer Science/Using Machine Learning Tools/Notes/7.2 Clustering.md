> [!motivation] The Need for Grouping Unlabeled Data
> 
> Humans intuitively group similar objects, like recognizing plants of the same species, even without formal botanical knowledge. This natural ability highlights a common data analysis need.
> 
> - Consider datasets like the Iris dataset shown below. When labels (species) are present (left panel), classification algorithms can assign instances to predefined categories.
> - However, much data lacks these labels (right panel). In such cases, classification methods cannot be applied.
> - The challenge arises: how can we automatically discover inherent groupings within unlabeled data?
> 
> ![[Pasted image 20250428125444.png|500]]
> 
> This necessitates methods that can identify structure without supervision.

> [!idea] Clustering
> 
> Clustering is an unsupervised learning task focused on **identifying similar instances within a dataset and assigning them to groups, known as clusters**. Unlike classification, it does not use predefined labels.
> 
> - **Objective**: To partition data such that instances within a cluster are similar to each other, and instances in different clusters are dissimilar.
> - **Algorithm Variability**: Different clustering algorithms define "clusters" differently. Some seek dense regions (DBSCAN), others group around central points (K-Means), and some build hierarchies of clusters.
> - **Feature Utilization**: Algorithms can leverage multiple features, potentially revealing clusters not obvious in lower-dimensional views (like the Iris dataset example).
> 
> Clustering has diverse applications:
> 
> - Customer segmentation for targeted marketing.
> - Exploratory data analysis to understand dataset structure.
> - Dimensionality reduction via cluster affinity vectors.
> - Feature engineering by adding cluster affinity as features.
> - Anomaly detection (outliers often have low affinity to all clusters).
> - Semi-supervised learning by propagating labels within clusters.
> - Search engines for finding similar items (e.g., images).
> - Image segmentation by grouping pixels of similar color.

> [!idea] K-Means Algorithm
> 
> K-Means is an iterative algorithm designed to **partition a dataset into a predefined number (k) of distinct, non-overlapping clusters.** It is particularly effective for identifying "blob-like" groups of instances.
> 
> ![[Pasted image 20250428132156.png|300]]
> 
> - **Objective**: Minimize the variance within each cluster. It achieves this by finding k central points (centroids) and assigning each data point to the cluster associated with the nearest centroid.
> - **Requirement**: The number of clusters, k, must be specified by the user before running the algorithm.
> - **Outcome**: The process results in assigning a cluster index (0 to kâˆ’1) to each instance. The decision boundaries between clusters form a Voronoi tessellation around the final centroids/
> 
> 
> 
> ![[Pasted image 20250428132258.png|300]]

> [!example] K-Means Implementation (Scikit-Learn)
>
> K-Means in Scikit-Learn helps group unlabeled data. You tell it how many groups ($k$) to find, and it assigns each data point to one group.
>
> ```python
> from sklearn.cluster import KMeans
> from sklearn.datasets import make_blobs
> import numpy as np
>
> # Example data (X has features, e.g., petal length/width for flowers)
> # X, y = make_blobs([...]) # y (true groups) is not used by KMeans
>
> k = 3 # Let's say we want to find 3 groups (clusters)
> kmeans = KMeans(n_clusters=k, random_state=42)
>
> # Run K-Means: finds centers and assigns each point in X to a cluster
> kmeans.fit(X)
> ```
>
> After fitting, you can inspect the results stored in the `kmeans` object:
>
> * The `labels_` attribute shows the cluster assignment for each data point used during training:
> ```python
> # Get the cluster assignments (a NumPy array):
> assigned_clusters = kmeans.labels_
> # Example: array([0, 0, 0, ..., 1, 1, 1, ..., 2, 2, 2], dtype=int32)
> ```
> This array tells you which cluster ID (e.g., 0, 1, or 2 if $k=3$) was assigned to the first data point in `X`, the second, the third, and so on. It reveals the actual grouping discovered by the algorithm in your training data. Conceptually, if you plotted the data, all points with label '0' would form one visual group, label '1' another, etc.
>
> * The `cluster_centers_` attribute stores the coordinates calculated for the center point of each cluster:
> ```python
> # Get the coordinates of the cluster centers (NumPy array):
> group_centers = kmeans.cluster_centers_
> # Example: array([ [center0_feat1, center0_feat2],
> #                  [center1_feat1, center1_feat2],
> #                  [center2_feat1, center2_feat2] ])
> ```
> These coordinates (one row per cluster) define the final calculated center for each group (cluster 0, cluster 1, cluster 2, etc.) based on the features of the data points assigned to them.
>
> * The `predict` method assigns new, unseen data points to the closest learned cluster centroid:
> ```python
> # Example new data points
> X_new = np.array([[petal_len1, petal_wid1], [petal_len2, petal_wid2]])
> # Find the cluster ID for each new point:
> predicted_groups = kmeans.predict(X_new)
> # Example: array([0, 2], dtype=int32)
> ```
> This returns an array showing the predicted cluster ID (e.g., 0, 1, or 2) for each of the new data points you provided, based on which existing cluster center they are nearest to. It allows you to classify new data using the discovered groups.