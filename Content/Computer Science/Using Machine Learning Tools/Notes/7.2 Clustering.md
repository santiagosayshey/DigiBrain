> [!motivation] The Need for Grouping Unlabeled Data
> 
> Humans intuitively group similar objects, like recognizing plants of the same species, even without formal botanical knowledge. This natural ability highlights a common data analysis need.
> 
> - Consider datasets like the Iris dataset shown below. When labels (species) are present (left panel), classification algorithms can assign instances to predefined categories.
> - However, much data lacks these labels (right panel). In such cases, classification methods cannot be applied.
> - The challenge arises: how can we automatically discover inherent groupings within unlabeled data?
> 
> ![[Pasted image 20250428125444.png|500]]
> 
> This necessitates methods that can identify structure without supervision.

> [!idea] Clustering
> 
> Clustering is an unsupervised learning task focused on **identifying similar instances within a dataset and assigning them to groups, known as clusters**. Unlike classification, it does not use predefined labels.
> 
> - **Objective**: To partition data such that instances within a cluster are similar to each other, and instances in different clusters are dissimilar.
> - **Algorithm Variability**: Different clustering algorithms define "clusters" differently. Some seek dense regions (DBSCAN), others group around central points (K-Means), and some build hierarchies of clusters.
> - **Feature Utilization**: Algorithms can leverage multiple features, potentially revealing clusters not obvious in lower-dimensional views (like the Iris dataset example).
> 
> Clustering has diverse applications:
> 
> - Customer segmentation for targeted marketing.
> - Exploratory data analysis to understand dataset structure.
> - Dimensionality reduction via cluster affinity vectors.
> - Feature engineering by adding cluster affinity as features.
> - Anomaly detection (outliers often have low affinity to all clusters).
> - Semi-supervised learning by propagating labels within clusters.
> - Search engines for finding similar items (e.g., images).
> - Image segmentation by grouping pixels of similar color.

> [!idea] K-Means Algorithm
> 
> K-Means is an iterative algorithm designed to **partition a dataset into a predefined number (k) of distinct, non-overlapping clusters.** It is particularly effective for identifying "blob-like" groups of instances.
> 
> ![[Pasted image 20250428132156.png|300]]
> 
> - **Objective**: Minimize the variance within each cluster. It achieves this by finding k central points (centroids) and assigning each data point to the cluster associated with the nearest centroid.
> - **Requirement**: The number of clusters, k, must be specified by the user before running the algorithm.
> - **Outcome**: The process results in assigning a cluster index (0 to kâˆ’1) to each instance. The decision boundaries between clusters form a Voronoi tessellation around the final centroids/
> 
> 
> 
> ![[Pasted image 20250428132258.png|300]]

You are absolutely correct! My apologies, I missed explicitly adding the `python` language identifier consistently to all code blocks in the previous example. This is important for correct syntax highlighting and can affect how the code block is interpreted when copied.

Let me provide the revised `example` callout with the `python` identifier correctly placed after the opening backticks for all code snippets.

Ah, thank you for showing me exactly how it's rendering! That clarifies the issue. It seems your markdown environment is separating the language identifier ("Python") onto its own line above the code block fence (` ``` `) when used within the callout structure (`>`).

Let's try removing the language identifier altogether to ensure the code block structure itself renders correctly. This means we'll lose the explicit Python syntax highlighting, but the code should appear within a standard block.

> [!example] K-Means Implementation (Scikit-Learn)
> 
> The K-Means algorithm can be implemented efficiently using Scikit-Learn. The process involves instantiating the model with the desired number of clusters ($k$) and then fitting it to the data.
> 
> ```
> from sklearn.cluster import KMeans
> from sklearn.datasets import make_blobs
> 
> # Example data (replace [...] with actual parameters)
> # X, y = make_blobs([...])
> # Note: y contains true cluster IDs but is not used by K-Means
> 
> k = 5
> kmeans = KMeans(n_clusters=k, random_state=42)
> y_pred = kmeans.fit_predict(X)
> ```
> 
> - After fitting, the cluster assignments (labels) for the training data `X` are available:
>     
>     ```
>     # Access predicted labels:
>     kmeans.labels_
>     # (y_pred is equivalent to kmeans.labels_)
>     ```
>     
> - The coordinates of the $k$ cluster centroids found by the algorithm can also be accessed:
>     
>     ```
>     # Access centroid coordinates:
>     kmeans.cluster_centers_
>     ```
>     
> - The trained `kmeans` model can assign new instances to the nearest cluster centroid:
>     
>     ```
>     import numpy as np
>     X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
>     kmeans.predict(X_new)
>     # Example Output (from text): array([1, 1, 2, 2], dtype=int32)
>     ```
>     
> 
> This demonstrates the basic workflow: initialize `KMeans` with $k$, use `fit_predict` on training data, and then use `predict` for new instances or inspect `labels_` and `cluster_centers_`.

Please let me know if this version renders the code blocks correctly embedded within the callout on your end.

Please check if copying the code blocks from this response works correctly with the `python` language identifier included.