> [!motivation] The Need for Grouping Unlabeled Data
> 
> Humans intuitively group similar objects, like recognizing plants of the same species, even without formal botanical knowledge. This natural ability highlights a common data analysis need.
> 
> - Consider datasets like the Iris dataset shown below. When labels (species) are present (left panel), classification algorithms can assign instances to predefined categories.
> - However, much data lacks these labels (right panel). In such cases, classification methods cannot be applied.
> - The challenge arises: how can we automatically discover inherent groupings within unlabeled data?
> 
> ![[Pasted image 20250428125444.png|500]]
> 
> This necessitates methods that can identify structure without supervision.

> [!idea] Clustering
> 
> Clustering is an unsupervised learning task focused on **identifying similar instances within a dataset and assigning them to groups, known as clusters**. Unlike classification, it does not use predefined labels.
> 
> - **Objective**: To partition data such that instances within a cluster are similar to each other, and instances in different clusters are dissimilar.
> - **Algorithm Variability**: Different clustering algorithms define "clusters" differently. Some seek dense regions (DBSCAN), others group around central points (K-Means), and some build hierarchies of clusters.
> - **Feature Utilization**: Algorithms can leverage multiple features, potentially revealing clusters not obvious in lower-dimensional views (like the Iris dataset example).
> 
> Clustering has diverse applications:
> 
> - Customer segmentation for targeted marketing.
> - Exploratory data analysis to understand dataset structure.
> - Dimensionality reduction via cluster affinity vectors.
> - Feature engineering by adding cluster affinity as features.
> - Anomaly detection (outliers often have low affinity to all clusters).
> - Semi-supervised learning by propagating labels within clusters.
> - Search engines for finding similar items (e.g., images).
> - Image segmentation by grouping pixels of similar color.

> [!idea] K-Means Algorithm
> 
> K-Means is an iterative algorithm designed to **partition a dataset into a predefined number (k) of distinct, non-overlapping clusters.** It is particularly effective for identifying "blob-like" groups of instances.
> 
> ![[Pasted image 20250428132156.png|300]]
> 
> - **Objective**: Minimize the variance within each cluster. It achieves this by finding k central points (centroids) and assigning each data point to the cluster associated with the nearest centroid.
> - **Requirement**: The number of clusters, k, must be specified by the user before running the algorithm.
> - **Outcome**: The process results in assigning a cluster index (0 to kâˆ’1) to each instance. The decision boundaries between clusters form a Voronoi tessellation around the final centroids/
> 
> 
> 
> Code snippet
> 
> ```
> Caption: Figure 9-3. K-Means decision boundaries.
> Description: The same scatter plot as Figure 9-2, but now with decision boundaries shown as lines, dividing the space into regions. Each region corresponds to a cluster. The center of each cluster (centroid) is marked with an 'X'. This illustrates the Voronoi tessellation created by K-Means.
> ```

> [!example] K-Means Implementation (Scikit-Learn)
> 
> Training a K-Means model using Scikit-Learn involves specifying k and fitting the model to the data.
> 
> Python
> 
> ```
> from sklearn.cluster import KMeans
> from sklearn.datasets import make_blobs
> import numpy as np
> 
> # Example data (e.g., blobs like Figure 9-2)
> # X, y = make_blobs(...) # y is not used for training
> 
> # 1. Specify the number of clusters (k)
> k = 5
> 
> # 2. Instantiate the model
> kmeans = KMeans(n_clusters=k, random_state=42)
> 
> # 3. Fit to data and predict cluster labels
> y_pred = kmeans.fit_predict(X)
> 
> # --- Post-Training Information ---
> 
> # Cluster assignments for training data:
> # kmeans.labels_
> # Example: array([4, 0, 1, ..., 2, 1, 0], dtype=int32)
> 
> # Coordinates of cluster centroids:
> # kmeans.cluster_centers_
> # Example: array([[-2.80, 1.80], [ 0.21, 2.26], ...])
> 
> # Predict cluster for new data:
> # X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
> # new_labels = kmeans.predict(X_new)
> # Example: array([1, 1, 2, 2], dtype=int32)
> ```
> 
> - The `fit_predict` method returns the cluster index assigned to each instance in the input data `X`.
> - The trained `kmeans` object stores the centroid coordinates (`cluster_centers_`) and the labels assigned to the training data (`labels_`).
> - The `predict` method assigns new instances to the cluster whose centroid is nearest.