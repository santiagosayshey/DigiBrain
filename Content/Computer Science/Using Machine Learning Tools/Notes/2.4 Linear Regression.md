> [!motivation] Linear Regression
> 
> Linear regression serves as a fundamental statistical method for modeling relationships between variables. Understanding it thoroughly is valuable because:
> 
> - It provides the foundation for many advanced machine learning algorithms and techniques
> - It demonstrates core concepts in statistical modeling (parameter estimation, hypothesis testing)
> - Its simplicity makes it an excellent starting point for understanding more complex models
> - The mathematics behind linear regression reappears in numerous other statistical methods

> [!idea] Linear Regression
> 
> Linear regression **finds patterns in data by drawing the best possible straight line through data points.** It answers the question: "How does one thing change when something else changes?"
> 
> Think of it like this:
> 
> - You have dots scattered on a graph (your data points)
> - You want to draw one straight line that comes as close as possible to all dots
> - This line becomes your prediction tool for new data
> 
> **The Process**
> 
> Linear regression works by:
> 
> 1. Trying different straight lines through the data
> 2. Measuring how far each dot is from each potential line
> 3. Picking the line where the dots are closest overall
> 
> **What Makes It "Linear"**
> 
> The "linear" part means:
> 
> - The relationship follows a straight line
> - When input increases by a fixed amount, output always changes by the same amount
> - The effects of different inputs simply add together
> 
> **What It Tells You**
> 
> The resulting line tells you:
> 
> - The starting point (where the line crosses the y-axis)
> - The rate of change (the slope of the line)
> - How strongly the inputs and outputs are related
> 
> **Real-World Example**
> 
> For house prices based on size:
> 
> - The line might show that each additional square foot adds about $150 to the price
> - The starting point might be $50,000 (theoretical price of a 0 sq ft house)
> - This relationship lets you estimate prices for houses not in your original data

> [!idea] Mathematical Framework of Linear Regression
> 
> **Starting Point: The Linear Model**
> 
> $$y = wx + b$$
> 
> This equation is our prediction formula:
> 
> - We input an x value (like house size)
> - Multiply it by w (the slope)
> - Add b (the y-intercept)
> - Get our prediction for y (like house price)
> 
> **Step 1: Measure How Wrong Our Predictions Are**
> 
> For each data point, we calculate the error: $$\text{error} = \text{actual y} - \text{predicted y} = y_i - (wx_i + b)$$
> 
> We square these errors and add them up: $$\text{Total Error} = \sum_{i=1}^{n}(y_i - (wx_i + b))^2$$
> 
> Squaring does two things:
> 
> - Makes all errors positive (negative and positive errors don't cancel out)
> - Penalizes large errors more heavily than small ones
> 
> **Step 2: Find the Values of w and b That Minimize This Error**
> 
> This is calculus in action. We find where the derivative equals zero:
> 
> For w: $$\frac{\partial}{\partial w}\sum_{i=1}^{n}(y_i - (wx_i + b))^2 = 0$$ For b: $$\frac{\partial}{\partial b}\sum_{i=1}^{n}(y_i - (wx_i + b))^2 = 0$$
> 
> When we solve these equations:
> 
> $$w = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}$$
> 
> This formula actually means:
> 
> - The numerator measures how x and y change together (covariance)
> - The denominator measures how much x varies on its own (variance)
> - So w = how much y tends to change when x changes
> 
> And:
> 
> $$b = \bar{y} - w\bar{x}$$
> 
> This ensures our line passes through the "center" of our data points (the point of averages).
> 
> **Step 3: Evaluate How Well Our Line Fits the Data**
> 
> $$R^2 = 1 - \frac{\text{Error of our model}}{\text{Error if we just guessed the average y}}$$
> 
> - If R² = 1: Our line perfectly predicts all points
> - If R² = 0: Our line is no better than just guessing the average
> 
> **Multiple Variables: Same Idea, More Dimensions**
> 
> With multiple features (x₁, x₂, etc.), we use matrices to handle all calculations at once:
> 
> $$\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$
> 
> This is the same process, just written compactly to handle all variables simultaneously.

> [!example] Linear Regression Example
> 
> **Problem**: Predicting house prices based on square footage
> 
> **Data**:
> 
> |House|Size (sq ft)|Price ($1000s)|
> |---|---|---|
> |1|1400|232|
> |2|1600|252|
> |3|1700|290|
> |4|1875|314|
> |5|1100|186|
> 
> **1. Calculate the model parameters**
> 
> Using the OLS formula, we get:
> 
> - $\beta_0$ (intercept) = 32.7
> - $\beta_1$ (slope) = 0.15
> 
> Our model: $\hat{y} = 32.7 + 0.15x$
> 
> **2. Make predictions**
> 
> For a house with 1500 sq ft: $\hat{y} = 32.7 + 0.15 \times 1500 = 257.7$ thousand dollars
> 
> **3. Visualize the model**
> 
> ```image_goes_here
> A scatter plot showing house size (x-axis) vs. price (y-axis) with the 5 data points plotted. A straight line represents the linear regression model passing through the points with equation y = 32.7 + 0.15x. The line should show the best fit through the scattered points.
> ```
> 
> **4. Assess performance**
> 
> - R-squared = 0.94 (94% of price variation is explained by square footage)
> - Mean squared error (MSE) = 85.3