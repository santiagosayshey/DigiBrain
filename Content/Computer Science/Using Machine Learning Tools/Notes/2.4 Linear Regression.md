> [!motivation] Linear Regression
> 
> Linear regression serves as a fundamental statistical method for modeling relationships between variables. Understanding it thoroughly is valuable because:
> 
> - It provides the foundation for many advanced machine learning algorithms and techniques
> - It demonstrates core concepts in statistical modeling (parameter estimation, hypothesis testing)
> - Its simplicity makes it an excellent starting point for understanding more complex models
> - The mathematics behind linear regression reappears in numerous other statistical methods

> [!idea] Linear Regression
> 
> Linear regression models relationships between variables using a straight line. It predicts how a dependent variable (y) changes when an independent variable (x) changes.
> 
> **Core Concept**
> 
> The model has the form:
> 
> $$y = wx + b$$
> 
> Where:
> 
> - $w$ is the coefficient (slope) that shows how much y changes when x increases by 1
> - $b$ is the intercept that shows the value of y when x is 0
> 
> Linear regression finds the values of w and b that create the "best-fitting" line through our data points. This line minimizes the overall distance between predicted values and actual values.
> 
> **Finding the Best-Fitting Line**
> 
> We measure how well a line fits by calculating the error for each data point:
> 
> $$error = actual\ y - predicted\ y$$
> 
> To find the best line, we minimize the sum of squared errors across all points. Squaring ensures we penalize all deviations regardless of direction.
> 
> $$Total\ Error = \sum(y_i - (wx_i + b))^2$$
> 
> Using calculus to minimize this error gives us formulas for optimal values:
> 
> $$w = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$$
> 
> $$b = \bar{y} - w\bar{x}$$
> 
> These formulas capture how x and y vary together relative to how much x varies on its own.
> 
> **Multiple Regression**
> 
> With multiple input variables, we extend to:
> 
> $$y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$
> 
> The same principles apply, but we use matrix algebra to handle all calculations simultaneously.
> 
> **Evaluating Model Quality**
> 
> We measure model performance with $R^2$ (0 to 1), which represents the proportion of output variance explained by the model. Higher is better.

> [!consider] The Intercept Term (b) in Linear Regression
> 
> The intercept term $b$ (sometimes called $\beta_0$ or the bias term) represents the predicted value of $y$ when all input features equal zero.
> 
> **What it means in practice:**
> 
> - It's the "starting point" or "base value" before considering the effect of any features
> - It shifts the entire regression line (or plane, or hyperplane) up or down
> - Without an intercept, we would force our line through the origin (0,0), which rarely matches real data
> 
> **Examples:**
> 
> In house price prediction:
> 
> - If $b = 50,000$ and all other features are zero, the model predicts a house price of $50,000
> - This might represent the "bare land value" before considering size, bedrooms, etc.
> 
> In height prediction based on age:
> 
> - If $b = 45$ (cm) and age is 0, the model predicts a height of 45 cm
> - This might represent the average height at birth
> 
> **Important considerations:**
> 
> - The intercept doesn't always have a meaningful real-world interpretation, especially when zero values for features aren't realistic
> - In some cases, we might force $b = 0$ (no intercept) if it makes theoretical sense for the specific problem
> - The intercept absorbs systematic errors in our model, helping minimize overall prediction error



> [!idea] Multiple Linear Regression
> 
> With multiple input variables, linear regression becomes:
> 
> $$y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$
> 
> Where:
> 
> - $y$ is the predicted output
> - $x_1, x_2, ..., x_n$ are different input features
> - $w_1, w_2, ..., w_n$ are the coefficients for each feature
> - $b$ is the intercept term
> 
> **How It Works**
> 
> 1. Each feature gets its own coefficient ($w_i$) that represents its individual contribution to the prediction
> 2. We still find the values of all $w_i$ and $b$ that minimize the sum of squared errors
> 3. The mathematics is handled using matrix operations rather than individual calculations
> 
> **Making Predictions**
> 
> Just like with simple linear regression, making a prediction involves plugging values into the equation:
> 
> For example, if predicting house price based on:
> 
> - Square footage ($x_1$)
> - Number of bedrooms ($x_2$)
> - Age of house ($x_3$)
> 
> With coefficients:
> 
> - $w_1 = 100$ (each square foot adds $100)
> - $w_2 = 15,000$ (each bedroom adds $15,000)
> - $w_3 = -500$ (each year of age reduces value by $500)
> - $b = 50,000$ (base price)
> 
> Then for a 2,000 sq ft, 3-bedroom, 10-year-old house:
> 
> $y = 100 \times 2,000 + 15,000 \times 3 + (-500) \times 10 + 50,000$ $y = 200,000 + 45,000 - 5,000 + 50,000 = 290,000$
> 
> The predicted price would be $290,000.

