> [!motivation] Linear Regression
> 
> Linear regression serves as a fundamental statistical method for modeling relationships between variables. Understanding it thoroughly is valuable because:
> 
> - It provides the foundation for many advanced machine learning algorithms and techniques
> - It demonstrates core concepts in statistical modeling (parameter estimation, hypothesis testing)
> - Its simplicity makes it an excellent starting point for understanding more complex models
> - The mathematics behind linear regression reappears in numerous other statistical methods

> [!idea] Linear Regression
> 
> Linear regression models relationships between variables using a straight line. It predicts how a dependent variable (y) changes when an independent variable (x) changes.
> 
> **Core Concept**
> 
> The model has the form:
> 
> $$y = wx + b$$
> 
> Where:
> 
> - $w$ is the coefficient (slope) that shows how much y changes when x increases by 1
> - $b$ is the intercept that shows the value of y when x is 0
> 
> Linear regression finds the values of w and b that create the "best-fitting" line through our data points. This line minimizes the overall distance between predicted values and actual values.
> 
> **Finding the Best-Fitting Line**
> 
> We measure how well a line fits by calculating the error for each data point:
> 
> $$error = actual\ y - predicted\ y$$
> 
> To find the best line, we minimize the sum of squared errors across all points. Squaring ensures we penalize all deviations regardless of direction.
> 
> $$Total\ Error = \sum(y_i - (wx_i + b))^2$$
> 
> Using calculus to minimize this error gives us formulas for optimal values:
> 
> $$w = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$$
> 
> $$b = \bar{y} - w\bar{x}$$
> 
> These formulas capture how x and y vary together relative to how much x varies on its own.
> 
> **Multiple Regression**
> 
> With multiple input variables, we extend to:
> 
> $$y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$
> 
> The same principles apply, but we use matrix algebra to handle all calculations simultaneously.
> 
> **Evaluating Model Quality**
> 
> We measure model performance with $R^2$ (0 to 1), which represents the proportion of output variance explained by the model. Higher is better.

> [!example] Linear Regression Example
> 
> **Problem**: Predicting house prices based on square footage
> 
> **Data**:
> 
> |House|Size (sq ft)|Price ($1000s)|
> |---|---|---|
> |1|1400|232|
> |2|1600|252|
> |3|1700|290|
> |4|1875|314|
> |5|1100|186|
> 
> **1. Calculate the model parameters**
> 
> Using the OLS formula, we get:
> 
> - $\beta_0$ (intercept) = 32.7
> - $\beta_1$ (slope) = 0.15
> 
> Our model: $\hat{y} = 32.7 + 0.15x$
> 
> **2. Make predictions**
> 
> For a house with 1500 sq ft: $\hat{y} = 32.7 + 0.15 \times 1500 = 257.7$ thousand dollars
> 
> **3. Visualize the model**
> 
> ```image_goes_here
> A scatter plot showing house size (x-axis) vs. price (y-axis) with the 5 data points plotted. A straight line represents the linear regression model passing through the points with equation y = 32.7 + 0.15x. The line should show the best fit through the scattered points.
> ```
> 
> **4. Assess performance**
> 
> - R-squared = 0.94 (94% of price variation is explained by square footage)
> - Mean squared error (MSE) = 85.3