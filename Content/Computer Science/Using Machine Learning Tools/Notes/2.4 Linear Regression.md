> [!motivation] Linear Regression
> 
> Linear regression serves as a fundamental statistical method for modeling relationships between variables. Understanding it thoroughly is valuable because:
> 
> - It provides the foundation for many advanced machine learning algorithms and techniques
> - It demonstrates core concepts in statistical modeling (parameter estimation, hypothesis testing)
> - Its simplicity makes it an excellent starting point for understanding more complex models
> - The mathematics behind linear regression reappears in numerous other statistical methods

> [!idea] Linear Regression
> 
> Linear regression finds the straight line (or hyperplane) that best fits a set of data points.
> 
> **What It Actually Does**
> 
> At its core, linear regression:
> 
> - Draws a straight line through data points that minimizes the distance between the line and each point
> - Explicitly models the relationship: "when X changes by this much, Y tends to change by that much"
> - Finds exactly how much each input feature contributes to the output
> 
> **Mathematical Representation**
> 
> For a simple case with one feature: $$y = wx + b$$
> 
> What this actually means:
> 
> - $w$ is how much y changes when x increases by 1 unit
> - $b$ is the starting value of y when x = 0
> 
> **The Optimization Process**
> 
> Linear regression works by:
> 
> 1. Measuring the vertical distance (error) from each data point to a proposed line
> 2. Squaring each error (penalizing large errors more heavily, making math easier)
> 3. Adding up all squared errors to get the "sum of squared errors" (SSE)
> 4. Finding the line that makes this sum as small as possible
> 
> $$SSE = \sum_{i=1}^{n}(y_i - (wx_i + b))^2$$
> 
> The line with the lowest SSE is the "best fit" line.
> 
> **Finding the Best Parameters**
> 
> To find the optimal $w$ and $b$, we:
> 
> - Take derivatives of the SSE with respect to $w$ and $b$
> - Set these derivatives to zero (finding the minimum)
> - Solve the resulting equations
> 
> This gives us:
> 
> - $w$ = how strongly x and y vary together, divided by how much x varies on its own
> - $b$ = the average y-value minus $w$ times the average x-value
> 
> **Multiple Features**
> 
> With multiple features, we do the same thing but in higher dimensions:
> 
> - Instead of a line, we find a hyperplane
> - Each feature gets its own weight showing its individual contribution
> - The same principle applies: minimize the sum of squared errors
> 
> $$y = w_1x_1 + w_2x_2 + ... + w_dx_d + b$$
> 
> **In Matrix Form**
> 
> The whole process can be expressed compactly using matrices:
> 
> - $\mathbf{X}$ = matrix of input features (each row is a data point, each column is a feature)
> - $\mathbf{y}$ = vector of output values
> - $\mathbf{w}$ = vector of weights to be found
> 
> The solution is: $\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$
> 
> This directly computes the weights that minimize the squared error across all data points in one step.

> [!example] Linear Regression Example
> 
> **Problem**: Predicting house prices based on square footage
> 
> **Data**:
> 
> |House|Size (sq ft)|Price ($1000s)|
> |---|---|---|
> |1|1400|232|
> |2|1600|252|
> |3|1700|290|
> |4|1875|314|
> |5|1100|186|
> 
> **1. Calculate the model parameters**
> 
> Using the OLS formula, we get:
> 
> - $\beta_0$ (intercept) = 32.7
> - $\beta_1$ (slope) = 0.15
> 
> Our model: $\hat{y} = 32.7 + 0.15x$
> 
> **2. Make predictions**
> 
> For a house with 1500 sq ft: $\hat{y} = 32.7 + 0.15 \times 1500 = 257.7$ thousand dollars
> 
> **3. Visualize the model**
> 
> ```image_goes_here
> A scatter plot showing house size (x-axis) vs. price (y-axis) with the 5 data points plotted. A straight line represents the linear regression model passing through the points with equation y = 32.7 + 0.15x. The line should show the best fit through the scattered points.
> ```
> 
> **4. Assess performance**
> 
> - R-squared = 0.94 (94% of price variation is explained by square footage)
> - Mean squared error (MSE) = 85.3