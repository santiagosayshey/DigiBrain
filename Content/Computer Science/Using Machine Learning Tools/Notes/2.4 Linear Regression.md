> [!motivation] Linear Regression
> 
> Linear regression serves as a fundamental statistical method for modeling relationships between variables. Understanding it thoroughly is valuable because:
> 
> - It provides the foundation for many advanced machine learning algorithms and techniques
> - It demonstrates core concepts in statistical modeling (parameter estimation, hypothesis testing)
> - Its simplicity makes it an excellent starting point for understanding more complex models
> - The mathematics behind linear regression reappears in numerous other statistical methods

> [!idea] Linear Regression
> 
> Linear regression models relationships between variables using a straight line. It predicts how a dependent variable (y) changes when an independent variable (x) changes. Essentially we are trying to **find a line of best fit that minimizes the error of the actual value compared to the model value.** 
> 
> **Core Concept**
> 
> The model has the form:
> 
> $$y = wx + b$$
> 
> Where:
> 
> - $w$ is the coefficient (slope) that shows how much y changes when x increases by 1
> - $b$ is the intercept that shows the value of y when x is 0
> 
> Linear regression finds the values of w and b that create the "best-fitting" line through our data points. This line minimizes the overall distance between predicted values and actual values.
> 
> **Finding the Best-Fitting Line**
> 
> We measure how well a line fits by calculating the error for each data point:
> 
> $$error = actual\ y - predicted\ y$$
> 
> To find the best line, we minimize the sum of squared errors across all points. Squaring ensures we penalize all deviations regardless of direction.
> 
> $$Total\ Error = \sum(y_i - (wx_i + b))^2$$
> 
> Using calculus to minimize this error gives us formulas for optimal values:
> 
> $$w = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$$
> 
> $$b = \bar{y} - w\bar{x}$$
> 
> These formulas capture how x and y vary together relative to how much x varies on its own.

> [!consider] The Intercept Term (b) in Linear Regression
> 
> The intercept term $b$ (sometimes called $\beta_0$ or the bias term) represents the predicted value of $y$ when all input features equal zero.
> 
> **What it means in practice:**
> 
> - It's the "starting point" or "base value" before considering the effect of any features
> - It shifts the entire regression line (or plane, or hyperplane) up or down
> - Without an intercept, we would force our line through the origin (0,0), which rarely matches real data
> 
> **Examples:**
> 
> In house price prediction:
> 
> - If $b = 50,000$ and all other features are zero, the model predicts a house price of $50,000
> - This might represent the "bare land value" before considering size, bedrooms, etc.
> 
> In height prediction based on age:
> 
> - If $b = 45$ (cm) and age is 0, the model predicts a height of 45 cm
> - This might represent the average height at birth
> 
> **Important considerations:**
> 
> - The intercept doesn't always have a meaningful real-world interpretation, especially when zero values for features aren't realistic
> - In some cases, we might force $b = 0$ (no intercept) if it makes theoretical sense for the specific problem
> - The intercept absorbs systematic errors in our model, helping minimize overall prediction error

> [!idea] Multiple Linear Regression
> 
> With multiple input variables, linear regression becomes:
> 
> $$y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$
> 
> Where:
> 
> - $y$ is the predicted output
> - $x_1, x_2, ..., x_n$ are different input features
> - $w_1, w_2, ..., w_n$ are the coefficients for each feature
> - $b$ is the intercept term
> 
> **How It Works**
> 
> 1. Each feature gets its own coefficient ($w_i$) that represents its individual contribution to the prediction
> 2. We still find the values of all $w_i$ and $b$ that minimize the sum of squared errors
> 3. The mathematics is handled using matrix operations rather than individual calculations
> 
> **Making Predictions**
> 
> Just like with simple linear regression, making a prediction involves plugging values into the equation:
> 
> For example, if predicting house price based on:
> 
> - Square footage ($x_1$)
> - Number of bedrooms ($x_2$)
> - Age of house ($x_3$)
> 
> With coefficients:
> 
> - $w_1 = 100$ (each square foot adds $100)
> - $w_2 = 15,000$ (each bedroom adds $15,000)
> - $w_3 = -500$ (each year of age reduces value by $500)
> - $b = 50,000$ (base price)
> 
> Then for a 2,000 sq ft, 3-bedroom, 10-year-old house:
> 
> $y = 100 \times 2,000 + 15,000 \times 3 + (-500) \times 10 + 50,000$ $y = 200,000 + 45,000 - 5,000 + 50,000 = 290,000$
> 
> The predicted price would be $290,000.

> [!idea] How Linear Regression Finds Coefficients
> 
> Linear regression finds the optimal coefficients through several methods:
> 
> **1. Ordinary Least Squares (OLS) - Analytical Solution**
> 
> The most common approach uses calculus to find the exact solution:
> 
> - We set up an equation to minimize the sum of squared errors
> - Take derivatives with respect to each coefficient and set them to zero
> - Solve the resulting system of equations
> 
> For simple linear regression, this gives us the formulas:
> 
> $$w = \frac{\sum(x_i - \bar{x})(y_i - \bar{y})}{\sum(x_i - \bar{x})^2}$$
> 
> $$b = \bar{y} - w\bar{x}$$
> 
> For multiple regression, we use matrix algebra:
> 
> $$\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$$
> 
> **2. Gradient Descent - Iterative Approach**
> 
> When datasets are very large, we may use an iterative approach:
> 
> - Start with random or zero coefficients
> - Calculate the error gradient (direction of steepest increase)
> - Update coefficients in the opposite direction (to decrease error)
> - Repeat until convergence (minimal change in error)
> 
> Update rule: $w_{\text{new}} = w_{\text{old}} - \alpha \frac{\partial \text{Error}}{\partial w}$
> 
> Where $\alpha$ is the learning rate that controls step size.
> 
> **3. Regularized Methods**
> 
> To prevent overfitting, especially with many features:
> 
> - Ridge Regression: Adds penalty proportional to squared coefficient values
> - Lasso: Adds penalty proportional to absolute coefficient values (can zero out coefficients)
> - Elastic Net: Combines both penalties
> 
> These methods balance fitting the data with keeping coefficients small or sparse.
> 
> **Software Implementation**
> 
> In practice, we use libraries like scikit-learn in Python:
> 
> ```python
> from sklearn.linear_model import LinearRegression
> model = LinearRegression()
> model.fit(X, y)  # X contains features, y contains targets
> coefficients = model.coef_  # Gets w values
> intercept = model.intercept_  # Gets b value
> ```

> [!consider] Beyond Straight Lines: Polynomial and Non-Linear Features
> 
> Linear regression is not limited to fitting straight lines. The "linear" in linear regression refers to the model being linear in the parameters (coefficients), not necessarily in the input features.
> 
> **Polynomial Regression**
> 
> We can transform our features to model non-linear relationships while still using linear regression:
> 
> $$y = w_0 + w_1x + w_2x^2 + w_3x^3 + ... + w_nx^n$$
> 
> This is still a linear regression model because it's linear in the parameters $(w_0, w_1, w_2, ...)$ even though it's non-linear in $x$.
> 
> **Other Non-Linear Transformations**
> 
> We can apply various transformations to features:
> 
> - Logarithmic: $y = w_0 + w_1\log(x)$
> - Exponential: $y = w_0 + w_1e^x$
> - Trigonometric: $y = w_0 + w_1\sin(x) + w_2\cos(x)$
> 
> **Feature Engineering**
> 
> Linear regression can model complex relationships through feature engineering:
> 
> - Creating interaction terms: $x_1 \times x_2$
> - Adding domain-specific transformations
> - Combining multiple transformations
> 
> **Key Insight**
> 
> The power of linear regression extends beyond fitting straight lines. By transforming features appropriately, we can model a wide range of relationships while maintaining the mathematical simplicity and interpretability of linear regression.