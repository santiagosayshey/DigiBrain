> [!motivation] Why Measure Regression Performance
> 
> When building predictive models, we need objective ways to:
> 
> - Quantify how well our predictions match actual values
> - Compare different models to select the best one
> - Tune hyperparameters to optimize performance
> - Detect overfitting by comparing training and validation metrics
> 
> Different metrics capture different aspects of model performance, allowing us to select metrics aligned with our specific goals.

> [!idea] Root Mean Squared Error (RMSE)
> 
> RMSE measures the **average magnitude of prediction errors, with larger errors penalized more heavily due to squaring.**
> 
> - Heavily weights outliers due to squared terms
> - Uses same units as the target variable (after taking square root)
> - Commonly used default metric for regression problems
> - Higher values indicate worse performance
> 
> **Mathematical Definition**: $$\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$$
> 
> This formula:
> 
> 1. Calculates the difference between each predicted value ($\hat{y}_i$) and actual value ($y_i$)
> 2. Squares each difference to make all values positive and penalize larger errors
> 3. Averages these squared differences
> 4. Takes the square root to return to the original units of measurement

> [!idea] Mean Absolute Error (MAE)
> 
> MAE measures the **average absolute difference between predicted and actual values.**
> 
> - More robust to outliers than RMSE
> - Easier to interpret directly as average error magnitude
> - Uses same units as the target variable
> - Preferred when outliers should not have disproportionate influence
> 
> **Mathematical Definition**: $$\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$
> 
> This formula:
> 
> 1. Calculates the absolute difference between each predicted value ($\hat{y}_i$) and actual value ($y_i$)
> 2. Takes the average of these absolute differences
> 3. Provides a linear penalty for errors (unlike RMSE's quadratic penalty)

> [!idea] Median and Maximum Error
> 
> These metrics focus on specific aspects of the error distribution rather than averages.
> 
> **Median Absolute Error**
> 
> - Calculates the median of all absolute differences between predictions and actual values
> - Extremely robust to outliers
> - Useful when dataset contains anomalies or extreme values
> 
> **Maximum Error**
> 
> - Reports the largest absolute error in the dataset
> - Helps identify worst-case prediction scenarios
> - Important in applications where the worst prediction matters most
> 
> **Mathematical Definitions**: $$\text{Median Absolute Error} = \text{median}(|y_1 - \hat{y}_1|, |y_2 - \hat{y}_2|, ..., |y_n - \hat{y}_n|)$$
> 
> $$\text{Maximum Error} = \max(|y_1 - \hat{y}_1|, |y_2 - \hat{y}_2|, ..., |y_n - \hat{y}_n|)$$
> 
> These formulas select specific points from the distribution of absolute errors instead of calculating averages.

> [!idea] R² (Coefficient of Determination)
> 
> R² measures the proportion of variance in the dependent variable explained by the independent variables.
> 
> - Ranges from 0 to 1 for most models (can be negative for poorly fitted models)
> - Value of 1 indicates perfect predictions
> - Value of 0 indicates model performs no better than predicting the mean
> - Scale-free metric, making it useful for comparing across different datasets
> 
> **Mathematical Definition**: $$R^2 = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}$$
> 
> This formula:
> 
> 1. Calculates the ratio of the model's squared error sum to the total variance in the data
> 2. Subtracts this ratio from 1
> 3. Effectively compares model performance to a baseline model that always predicts the mean
> 
> **Correlation Coefficient**
> 
> The correlation coefficient (Pearson's r) measures the linear relationship between predicted and actual values. $$r = \frac{\sum_{i=1}^{n}(y_i - \bar{y})(\hat{y}_i - \bar{\hat{y}})}{\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2 \sum_{i=1}^{n}(\hat{y}_i - \bar{\hat{y}})^2}}$$
> 
> For linear regression with a single variable, $r^2 = R^2$. The formula measures how closely predictions and actual values move together, ranging from -1 (perfect negative correlation) to 1 (perfect positive correlation).

> [!consider] Choosing the Right Metric
> 
> The choice of metric should align with the specific goals of your regression task:
> 
> |Metric|When to Use|Example|
> |---|---|---|
> |RMSE|When larger errors are disproportionately undesirable|**Housing Price Prediction**: In real estate valuation, a $50,000 error is more than twice as bad as a $25,000 error. RMSE would penalize the larger error more heavily, making it suitable for scenarios where large errors significantly impact business decisions.|
> |MAE|When you need a metric that's robust to outliers|**Retail Sales Forecasting**: When predicting daily store sales where occasional special events (like Black Friday) create legitimate outliers, MAE provides a better measure of typical performance without being skewed by these special cases.|
> |Median Error|When your dataset contains significant anomalies|**Sensor Reading Prediction**: In industrial equipment with occasional sensor malfunctions that create erroneous readings, median error gives a more reliable picture of model performance on normal operations by completely ignoring extreme errors.|
> |Maximum Error|When worst-case performance is critical|**Medical Dosage Prediction**: When predicting proper medication dosages, even a single large error could be life-threatening. Maximum error helps identify potential worst-case scenarios that need to be addressed, even if average performance is good.|
> |R²|When you need a scale-free metric for comparison across datasets|**Multi-Product Demand Forecasting**: When building models to predict demand for products with vastly different price ranges (e.g., $5 items vs. $500 items), R² allows you to compare model quality across products regardless of their absolute values.|
> 
> **Practical Illustration**: Consider three house price prediction models:
> 
> - Model A has low average errors but severely underestimates one luxury property
> - Model B has slightly higher average errors but performs reasonably across all properties
> - Model C has the lowest average errors but completely misses a high-value property
> 
> For a general real estate agency, Model B might be preferred based on MAE. For luxury specialists, Model A might be better based on RMSE. For a comprehensive evaluation, multiple metrics together provide the clearest picture of model performance.