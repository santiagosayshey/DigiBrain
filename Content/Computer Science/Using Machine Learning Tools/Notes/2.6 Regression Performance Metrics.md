> [!motivation] Why Measure Regression Performance
> 
> When building predictive models, we need objective ways to:
> 
> - Quantify how well our predictions match actual values
> - Compare different models to select the best one
> - Tune hyperparameters to optimize performance
> - Detect overfitting by comparing training and validation metrics
> 
> Different metrics capture different aspects of model performance, allowing us to select metrics aligned with our specific goals.

> [!idea] Root Mean Squared Error (RMSE)
> 
> RMSE measures the **average magnitude of prediction errors, with larger errors penalized more heavily due to squaring.**
> 
> - Heavily weights outliers due to squared terms
> - Uses same units as the target variable (after taking square root)
> - Commonly used default metric for regression problems
> - Higher values indicate worse performance
> 
> **Mathematical Definition**: $$\text{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}$$
> 
> This formula:
> 
> 1. Calculates the difference between each predicted value ($\hat{y}_i$) and actual value ($y_i$)
> 2. Squares each difference to make all values positive and penalize larger errors
> 3. Averages these squared differences
> 4. Takes the square root to return to the original units of measurement

> [!idea] Mean Absolute Error (MAE)
> 
> MAE measures the average absolute difference between predicted and actual values.
> 
> - More robust to outliers than RMSE
> - Easier to interpret directly as average error magnitude
> - Uses same units as the target variable
> - Preferred when outliers should not have disproportionate influence
> 
> **Mathematical Definition**: $$\text{MAE} = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$
> 
> This formula:
> 
> 1. Calculates the absolute difference between each predicted value ($\hat{y}_i$) and actual value ($y_i$)
> 2. Takes the average of these absolute differences
> 3. Provides a linear penalty for errors (unlike RMSE's quadratic penalty)

> [!idea] Median and Maximum Error
> 
> These metrics focus on specific aspects of the error distribution rather than averages.
> 
> **Median Absolute Error**
> 
> - Calculates the median of all absolute differences between predictions and actual values
> - Extremely robust to outliers
> - Useful when dataset contains anomalies or extreme values
> 
> **Maximum Error**
> 
> - Reports the largest absolute error in the dataset
> - Helps identify worst-case prediction scenarios
> - Important in applications where the worst prediction matters most
> 
> **Mathematical Definitions**: $$\text{Median Absolute Error} = \text{median}(|y_1 - \hat{y}_1|, |y_2 - \hat{y}_2|, ..., |y_n - \hat{y}_n|)$$
> 
> $$\text{Maximum Error} = \max(|y_1 - \hat{y}_1|, |y_2 - \hat{y}_2|, ..., |y_n - \hat{y}_n|)$$
> 
> These formulas select specific points from the distribution of absolute errors instead of calculating averages.

> [!idea] R² (Coefficient of Determination)
> 
> R² measures the proportion of variance in the dependent variable explained by the independent variables.
> 
> - Ranges from 0 to 1 for most models (can be negative for poorly fitted models)
> - Value of 1 indicates perfect predictions
> - Value of 0 indicates model performs no better than predicting the mean
> - Scale-free metric, making it useful for comparing across different datasets
> 
> **Mathematical Definition**: $$R^2 = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2}$$
> 
> This formula:
> 
> 1. Calculates the ratio of the model's squared error sum to the total variance in the data
> 2. Subtracts this ratio from 1
> 3. Effectively compares model performance to a baseline model that always predicts the mean
> 
> **Correlation Coefficient**
> 
> The correlation coefficient (Pearson's r) measures the linear relationship between predicted and actual values. $$r = \frac{\sum_{i=1}^{n}(y_i - \bar{y})(\hat{y}_i - \bar{\hat{y}})}{\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2 \sum_{i=1}^{n}(\hat{y}_i - \bar{\hat{y}})^2}}$$
> 
> For linear regression with a single variable, $r^2 = R^2$. The formula measures how closely predictions and actual values move together, ranging from -1 (perfect negative correlation) to 1 (perfect positive correlation).

> [!consider] Choosing the Right Metric
> 
> The choice of metric should align with the specific goals of your regression task:
> 
> |Metric|When to Use|
> |---|---|
> |RMSE|When larger errors are disproportionately undesirable|
> |MAE|When you need a metric that's robust to outliers|
> |Median Error|When your dataset contains significant anomalies|
> |Maximum Error|When worst-case performance is critical|
> |R²|When you need a scale-free metric for comparison across datasets|
> 
> - For business-critical applications, consider reporting multiple metrics to provide a comprehensive view
> - Domain knowledge should guide metric selection – some fields have standard metrics for comparability
> - For asymmetric costs (e.g., underprediction more costly than overprediction), consider custom metrics that weight errors differently