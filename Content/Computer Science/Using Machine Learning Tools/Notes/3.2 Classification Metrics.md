> [!motivation] Why Measure Classification Performance
> 
> When developing classification models, we need objective ways to:
> 
> - Evaluate how accurately our model predicts class labels
> - Compare different classification algorithms and approaches
> - Tune model parameters for optimal performance
> - Understand trade-offs between different error types (false positives vs. false negatives)
> 
> Different classification metrics highlight different aspects of model performance, allowing us to select metrics aligned with the specific requirements of our application.

> [!idea] Accuracy
> 
> Accuracy measures the proportion of correct predictions among all predictions made.
> 
> - Most intuitive and commonly used classification metric
> - Works well when classes are balanced (similar number of instances)
> - Simple to explain to non-technical stakeholders
> - Can be misleading for imbalanced datasets
> 
> **Mathematical Definition**: $$\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} = \frac{TP + TN}{TP + TN + FP + FN}$$
> 
> This formula:
> 
> 1. Counts true positives (TP) and true negatives (TN) - correct predictions
> 2. Divides by the total number of predictions (including false positives (FP) and false negatives (FN))
> 3. Yields a value between 0 and 1, often expressed as a percentage

> [!idea] Precision and Recall
> 
> These metrics focus on different aspects of positive class prediction.
> 
> **Precision**
> 
> - Measures the accuracy of positive predictions
> - Answers: "When the model predicts positive, how often is it correct?"
> - Critical when false positives are costly
> - Also called Positive Predictive Value
> 
> **Mathematical Definition**: $$\text{Precision} = \frac{TP}{TP + FP}$$
> 
> This formula divides true positives by all positive predictions, measuring how many positive predictions were actually correct.
> 
> **Recall**
> 
> - Measures the ability to find all positive instances
> - Answers: "What proportion of actual positives did the model identify?"
> - Critical when missing positive cases is costly
> - Also called Sensitivity or True Positive Rate
> 
> **Mathematical Definition**: $$\text{Recall} = \frac{TP}{TP + FN}$$
> 
> This formula divides true positives by all actual positive cases, measuring how many positive cases were correctly identified.

> [!idea] F1 Score
> 
> F1 Score combines precision and recall into a single metric.
> 
> - Harmonic mean of precision and recall
> - Balances both metrics when a single evaluation metric is needed
> - Particularly useful for imbalanced datasets
> - Ranges from 0 (worst) to 1 (best)
> 
> **Mathematical Definition**: $$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2TP}{2TP + FP + FN}$$
> 
> This formula:
> 
> 1. Multiplies precision and recall
> 2. Divides by their sum and multiplies by 2 (to scale from 0-1 instead of 0-0.5)
> 3. Ensures the model is penalized if either precision or recall is low

> [!idea] ROC Curve and AUC
> 
> ROC (Receiver Operating Characteristic) and AUC (Area Under Curve) evaluate classification models across different threshold settings.
> 
> **ROC Curve**
> 
> - Plots True Positive Rate (Recall) against False Positive Rate at various thresholds
> - Shows the trade-off between sensitivity and specificity
> - Useful for selecting optimal threshold
> - Independent of class distribution
> 
> **AUC**
> 
> - Measures the entire two-dimensional area under the ROC curve
> - Ranges from 0.5 (random classifier) to 1.0 (perfect classifier)
> - Represents probability that model ranks a random positive example higher than a random negative example
> - Threshold-independent metric
> 
> **Mathematical Definition**: $$\text{True Positive Rate (TPR)} = \frac{TP}{TP + FN}$$ $$\text{False Positive Rate (FPR)} = \frac{FP}{FP + TN}$$ $$\text{AUC} = \int_{0}^{1} \text{TPR}(\text{FPR}^{-1}(t)) dt$$
> 
> The AUC formula integrates the True Positive Rate over all possible False Positive Rates, effectively measuring the model's ability to distinguish between classes.

> [!idea] Confusion Matrix
> 
> A confusion matrix provides a tabular summary of classification results.
> 
> - Shows counts of true positives, false positives, true negatives, and false negatives
> - Basis for calculating many other metrics
> - Provides complete information about model's classification performance
> - Reveals which types of errors the model tends to make
> 
> **Structure**:
> 
> ||Predicted Positive|Predicted Negative|
> |---|---|---|
> |**Actual Positive**|True Positive (TP)|False Negative (FN)|
> |**Actual Negative**|False Positive (FP)|True Negative (TN)|
> 
> From this matrix, we can calculate:
> 
> - Accuracy: (TP + TN) / (TP + TN + FP + FN)
> - Precision: TP / (TP + FP)
> - Recall: TP / (TP + FN)
> - Specificity: TN / (TN + FP)

> [!consider] Choosing the Right Classification Metric
> 
> The choice of metric should align with the specific goals of your classification task:
> 
> |Metric|When to Use|Example|
> |---|---|---|
> |Accuracy|When classes are balanced and all error types are equally important|**Email Category Classification**: When sorting emails into work categories where all categories have similar frequencies and misclassifications have similar impact.|
> |Precision|When false positives are more costly than false negatives|**Spam Detection**: When marking legitimate emails as spam (false positives) is more problematic than missing some spam emails. High precision ensures users don't lose important messages.|
> |Recall|When false negatives are more costly than false positives|**Disease Diagnosis**: When missing a positive case (false negative) could mean a patient doesn't receive needed treatment, while false positives lead to additional testing.|
> |F1 Score|When balance between precision and recall is needed, especially with imbalanced classes|**Fraud Detection**: When dealing with rare fraudulent transactions where both catching fraud (recall) and avoiding false accusations (precision) are important.|
> |AUC|When evaluating model's discriminative ability independent of classification threshold|**Customer Churn Prediction**: When you want to evaluate how well your model separates customers who will churn from those who won't, before deciding on an operational threshold.|
> 
> **Practical Illustration**: Consider three fraud detection models:
> 
> - Model A has high precision but low recall - identifies few fraudulent transactions but with high confidence
> - Model B has high recall but lower precision - catches most fraud but generates more false alarms
> - Model C has balanced precision and recall - moderate performance on both metrics
> 
> For a bank with limited investigation resources, Model A might be preferred. For a high-value transaction processor where missing fraud is extremely costly, Model B might be better. For a comprehensive evaluation, examining the ROC curve allows you to select a threshold that balances the trade-offs for your specific business needs.