> [!motivation] Why Measure Classification Performance
> 
> When developing classification models, we need objective ways to:
> 
> - Evaluate how accurately our model predicts class labels
> - Compare different classification algorithms and approaches
> - Tune model parameters for optimal performance
> - Understand trade-offs between different error types (false positives vs. false negatives)
> 
> Different classification metrics highlight different aspects of model performance, allowing us to select metrics aligned with the specific requirements of our application.

> [!idea] Confusion Matrix
> 
> A confusion matrix provides a tabular summary of classification results.
> 
> - Shows counts of true positives, false positives, true negatives, and false negatives
> - Basis for calculating many other metrics
> - Provides complete information about model's classification performance
> - Reveals which types of errors the model tends to make
> 
> **Structure**:
> 
> ||Predicted Positive|Predicted Negative|
> |---|---|---|
> |**Actual Positive**|True Positive (TP)|False Negative (FN)|
> |**Actual Negative**|False Positive (FP)|True Negative (TN)|
> 
> 
> ![[Pasted image 20250324151634.png|600]]

> [!idea] Accuracy
> 
> Accuracy measures the proportion of correct predictions among all predictions made.
> 
> - Most intuitive and commonly used classification metric
> - Works well when classes are balanced (similar number of instances)
> - Simple to explain to non-technical stakeholders
> - Can be misleading for imbalanced datasets
> 
> **Mathematical Definition**: $$\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} = \frac{TP + TN}{TP + TN + FP + FN}$$
> 
> This formula:
> 
> 1. Counts true positives (TP) and true negatives (TN) - correct predictions
> 2. Divides by the total number of predictions (including false positives (FP) and false negatives (FN))
> 3. Yields a value between 0 and 1, often expressed as a percentage

> [!idea] Precision and Recall
> 
> These metrics focus on different aspects of positive class prediction.
> 
> **Precision**
> 
> - Measures the accuracy of positive predictions. **What fraction of positive predictions are correct?**
> - Answers: "When the model predicts positive, how often is it correct?"
> - Critical when false positives are costly
> - Also called Positive Predictive Value
> 
> **Mathematical Definition**: $$\text{Precision} = \frac{TP}{TP + FP}$$
> 
> This formula divides true positives by all positive predictions, measuring how many positive predictions were actually correct.
> 
> **Recall**
> 
> - Measures the ability to find all positive instances. **What fraction of the real positive class are detected?**
> - Answers: "What proportion of actual positives did the model identify?"
> - Critical when missing positive cases is costly
> - Also called Sensitivity or True Positive Rate
> 
> **Mathematical Definition**: $$\text{Recall} = \frac{TP}{TP + FN}$$
> 
> This formula divides true positives by all actual positive cases, measuring how many positive cases were correctly identified.

> [!idea] F1 Score
> 
> F1 Score combines precision and recall into a single metric.
> 
> - Harmonic mean of precision and recall
> - Balances both metrics when a single evaluation metric is needed
> - Particularly useful for imbalanced datasets
> - Ranges from 0 (worst) to 1 (best)
> 
> **Mathematical Definition**: $$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2TP}{2TP + FP + FN}$$
> 
> This formula:
> 
> 1. Multiplies precision and recall
> 2. Divides by their sum and multiplies by 2 (to scale from 0-1 instead of 0-0.5)
> 3. Ensures the model is penalized if either precision or recall is low


> [!consider] Classification Threshold and Precision-Recall Tradeoff
> 
> **Classification Threshold**
> 
> - Most classifiers don't directly output class labels (0/1); they output probability scores (0.0-1.0)
> - The threshold is the cutoff point above which we assign the positive class
> - Default threshold is typically 0.5, but can be adjusted based on needs
> - Example: For a spam classifier, if threshold = 0.7, emails with spam score > 0.7 are classified as spam
> 
> **Why Precision and Recall Trade Off**
> 
> - When we lower the threshold:
>     - More actual positives are correctly identified (↑ recall)
>     - But more negatives are incorrectly classified as positive (↓ precision)
>     - Example: Lower spam threshold catches more actual spam (higher recall) but also misclassifies more legitimate emails as spam (lower precision)
> - When we raise the threshold:
>     - Fewer negatives are incorrectly classified as positive (↑ precision)
>     - But more actual positives are missed (↓ recall)
>     - Example: Higher spam threshold ensures emails flagged as spam are nearly always spam (higher precision) but misses some actual spam (lower recall)
> 
> **When to Prioritize Recall**
> 
> - Medical diagnosis: Missing a disease (false negative) could be life-threatening
> - Fraud detection in banking: Missing fraudulent transactions could lead to significant financial losses
> - Predictive maintenance: Missing equipment failure signals could result in costly downtime
> 
> **When to Prioritize Precision**
> 
> - Content recommendation: Irrelevant recommendations (false positives) frustrate users
> - Criminal justice: Falsely identifying someone as guilty has severe consequences
> - Email spam filtering: Legitimate emails in spam folder (false positives) may never be seen
> 
> This inverse relationship means we rarely maximize both metrics simultaneously, requiring a strategic choice based on which error type is more costly in a given application.

> [!idea] ROC Curve and AUC
> 
> ROC curves and AUC help evaluate and compare classification models independent of any specific threshold.
> 
> **ROC Curve: What it Does**
> 
> - Visualizes classifier performance across all possible thresholds in a single graph
> - Shows the tradeoff between catching positive cases (TPR) and avoiding false alarms (FPR)
> - Helps identify optimal threshold based on your specific requirements
> - Enables comparing different models' performance regardless of threshold settings
> 
> **How it Works**
> 
> - Plots True Positive Rate (TPR) against False Positive Rate (FPR) at every possible threshold
> - TPR = TP/(TP+FN) = Proportion of actual positives correctly identified
> - FPR = FP/(FP+TN) = Proportion of actual negatives incorrectly classified as positive
> - Each point on the curve represents a different threshold setting
> - Diagonal line (y=x) represents a random classifier
> 
> **AUC (Area Under Curve): What it Does**
> 
> - Quantifies overall classifier performance in a single metric
> - Allows direct comparison between different models
> - Remains valid even when class distributions change
> 
> **How it Works**
> 
> - Calculated as the area under the ROC curve
> - Ranges from 0.5 (random classifier) to 1.0 (perfect classifier)
> - Equals the probability that the model ranks a random positive example higher than a random negative example
> 
> **Mathematical Definition**: $$\text{True Positive Rate (TPR)} = \frac{TP}{TP + FN}$$ $$\text{False Positive Rate (FPR)} = \frac{FP}{FP + TN}$$ $$\text{AUC} = \int_{0}^{1} \text{TPR}(\text{FPR}^{-1}(t)) dt$$
> 
> The AUC formula integrates the True Positive Rate over all possible False Positive Rates, effectively measuring the model's ability to distinguish between classes.
> 
> **When to Use**
> 
> - When you need to compare models independent of threshold choice
> - For imbalanced datasets where accuracy can be misleading
> - In applications where ranking quality matters (recommendations, search, etc.)
> - When costs of false positives vs. false negatives may change over time


> [!consider] Choosing the Right Classification Metric
> 
> The choice of metric should align with the specific goals of your classification task:
> 
> |Metric|When to Use|Example|
> |---|---|---|
> |Accuracy|When classes are balanced and all error types are equally important|**Email Category Classification**: When sorting emails into work categories where all categories have similar frequencies and misclassifications have similar impact.|
> |Precision|When false positives are more costly than false negatives|**Spam Detection**: When marking legitimate emails as spam (false positives) is more problematic than missing some spam emails. High precision ensures users don't lose important messages.|
> |Recall|When false negatives are more costly than false positives|**Disease Diagnosis**: When missing a positive case (false negative) could mean a patient doesn't receive needed treatment, while false positives lead to additional testing.|
> |F1 Score|When balance between precision and recall is needed, especially with imbalanced classes|**Fraud Detection**: When dealing with rare fraudulent transactions where both catching fraud (recall) and avoiding false accusations (precision) are important.|
> |AUC|When evaluating model's discriminative ability independent of classification threshold|**Customer Churn Prediction**: When you want to evaluate how well your model separates customers who will churn from those who won't, before deciding on an operational threshold.|
> 
> **Practical Illustration**: Consider three fraud detection models:
> 
> - Model A has high precision but low recall - identifies few fraudulent transactions but with high confidence
> - Model B has high recall but lower precision - catches most fraud but generates more false alarms
> - Model C has balanced precision and recall - moderate performance on both metrics
> 
> For a bank with limited investigation resources, Model A might be preferred. For a high-value transaction processor where missing fraud is extremely costly, Model B might be better. For a comprehensive evaluation, examining the ROC curve allows you to select a threshold that balances the trade-offs for your specific business needs.