> [!motivation] Why Measure Classification Performance
> 
> When developing classification models, we need objective ways to:
> 
> - Evaluate how accurately our model predicts class labels
> - Compare different classification algorithms and approaches
> - Tune model parameters for optimal performance
> - Understand trade-offs between different error types (false positives vs. false negatives)
> 
> Different classification metrics highlight different aspects of model performance, allowing us to select metrics aligned with the specific requirements of our application.

> [!idea] Confusion Matrix
> 
> A confusion matrix provides a tabular summary of classification results.
> 
> - Shows counts of true positives, false positives, true negatives, and false negatives
> - Basis for calculating many other metrics
> - Provides complete information about model's classification performance
> - Reveals which types of errors the model tends to make
> 
> **Structure**:
> 
> ||Predicted Positive|Predicted Negative|
> |---|---|---|
> |**Actual Positive**|True Positive (TP)|False Negative (FN)|
> |**Actual Negative**|False Positive (FP)|True Negative (TN)|
> 
> 
> ![[Pasted image 20250324151634.png|600]]

> [!idea] Accuracy
> 
> Accuracy measures the proportion of correct predictions among all predictions made.
> 
> - Most intuitive and commonly used classification metric
> - Works well when classes are balanced (similar number of instances)
> - Simple to explain to non-technical stakeholders
> - Can be misleading for imbalanced datasets
> 
> **Mathematical Definition**: $$\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} = \frac{TP + TN}{TP + TN + FP + FN}$$
> 
> This formula:
> 
> 1. Counts true positives (TP) and true negatives (TN) - correct predictions
> 2. Divides by the total number of predictions (including false positives (FP) and false negatives (FN))
> 3. Yields a value between 0 and 1, often expressed as a percentage

> [!idea] Precision and Recall
> 
> These metrics focus on different aspects of positive class prediction.
> 
> **Precision**
> 
> - Measures the accuracy of positive predictions. **What fraction of positive predictions are correct?**
> - Answers: "When the model predicts positive, how often is it correct?"
> - Critical when false positives are costly
> - Also called Positive Predictive Value
> 
> **Mathematical Definition**: $$\text{Precision} = \frac{TP}{TP + FP}$$
> 
> This formula divides true positives by all positive predictions, measuring how many positive predictions were actually correct.
> 
> **Recall**
> 
> - Measures the ability to find all positive instances. **What fraction of the real positive class are detected?**
> - Answers: "What proportion of actual positives did the model identify?"
> - Critical when missing positive cases is costly
> - Also called Sensitivity or True Positive Rate
> 
> **Mathematical Definition**: $$\text{Recall} = \frac{TP}{TP + FN}$$
> 
> This formula divides true positives by all actual positive cases, measuring how many positive cases were correctly identified.

> [!idea] F1 Score
> 
> F1 Score combines precision and recall into a single metric.
> 
> - Harmonic mean of precision and recall
> - Balances both metrics when a single evaluation metric is needed
> - Particularly useful for imbalanced datasets
> - Ranges from 0 (worst) to 1 (best)
> 
> **Mathematical Definition**: $$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2TP}{2TP + FP + FN}$$
> 
> This formula:
> 
> 1. Multiplies precision and recall
> 2. Divides by their sum and multiplies by 2 (to scale from 0-1 instead of 0-0.5)
> 3. Ensures the model is penalized if either precision or recall is low


> [!consider] Classification Threshold and Precision-Recall Tradeoff
> 
> **Classification Threshold**
> 
> - Most classifiers don't directly output class labels (0/1); they output probability scores (0.0-1.0)
> - The threshold is the cutoff point above which we assign the positive class
> - Default threshold is typically 0.5, but can be adjusted based on needs
> - Example: For a spam classifier, if threshold = 0.7, emails with spam score > 0.7 are classified as spam
> 
> **Why Precision and Recall Trade Off**
> 
> - When we lower the threshold:
>     - More actual positives are correctly identified (↑ recall)
>     - But more negatives are incorrectly classified as positive (↓ precision)
>     - Example: Lower spam threshold catches more actual spam (higher recall) but also misclassifies more legitimate emails as spam (lower precision)
> - When we raise the threshold:
>     - Fewer negatives are incorrectly classified as positive (↑ precision)
>     - But more actual positives are missed (↓ recall)
>     - Example: Higher spam threshold ensures emails flagged as spam are nearly always spam (higher precision) but misses some actual spam (lower recall)
> 
> **When to Prioritize Recall**
> 
> - Medical diagnosis: Missing a disease (false negative) could be life-threatening
> - Fraud detection in banking: Missing fraudulent transactions could lead to significant financial losses
> - Predictive maintenance: Missing equipment failure signals could result in costly downtime
> 
> **When to Prioritize Precision**
> 
> - Content recommendation: Irrelevant recommendations (false positives) frustrate users
> - Criminal justice: Falsely identifying someone as guilty has severe consequences
> - Email spam filtering: Legitimate emails in spam folder (false positives) may never be seen
> 
> This inverse relationship means we rarely maximize both metrics simultaneously, requiring a strategic choice based on which error type is more costly in a given application.

> [!idea] Receiver Operating Characteristic (ROC) Curve
> 
> The ROC curve is a graphical plot that evaluates the performance of a binary classification model at different classification thresholds.
> 
> - Created by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings
>     - TPR (Recall): Proportion of actual positive instances that are correctly identified
>     - FPR: Proportion of actual negative instances that are incorrectly classified as positive
> - Demonstrates the trade-off between sensitivity (TPR) and specificity (1-FPR) as the decision threshold is varied
>     - Sensitivity: Ability to correctly identify positive instances
>     - Specificity: Ability to correctly identify negative instances
> - Helps in selecting an optimal threshold based on the specific requirements of the problem
>     - Lower threshold increases sensitivity but may increase false positives
>     - Higher threshold increases specificity but may increase false negatives
> 
> ```
> To interpret:
> - A model with perfect discrimination (no overlap between classes) has an ROC curve that passes through the upper left corner (0,1)
> - A model with no discriminative ability (random guessing) has an ROC curve that is a diagonal line from the bottom left to the top right corner
> - Most models have an ROC curve that falls between these two extremes, with better models having curves that are closer to the upper left corner
> ```

> [!idea] Area Under the ROC Curve (AUC)
> 
> AUC measures the entire two-dimensional area underneath the entire ROC curve.
> 
> - Provides an aggregate measure of performance across all possible classification thresholds
> - Ranges in value from 0 to 1
>     - AUC of 0.5 represents a model no better than random guessing
>     - AUC of 1.0 represents a model with perfect discrimination
> - Useful as a single number summary of model skill
> - Allows comparing different models or algorithms
> 
> Mathematically, AUC is the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. It can be calculated via the integral:
> 
> $$ \text{AUC} = \int_0^1 \text{TPR}(T) ; d(\text{FPR}(T)) $$
> 
> Where $T$ is the threshold, $\text{TPR}(T)$ is the true positive rate at threshold $T$ , and $\text{FPR}(T)$ is the false positive rate at $T$.

> [!example] Using ROC and AUC in Practice
> 
> Consider a bank developing a credit risk model to predict probability of default for loan applicants.
> 
> The bank compares three models:
> 
> |Model|AUC|
> |---|---|
> |A|0.7|
> |B|0.8|
> |C|0.85|
> 
> Based on AUC, Model C has the best overall discriminative ability.
> 
> However, the bank also needs to consider business constraints:
> 
> - Rejecting good applicants (false positives) leads to lost revenue
> - Approving risky applicants (false negatives) leads to defaults and losses
> 
> By analyzing the ROC curves, the bank can find optimal thresholds for each model:
> 
> - If minimizing defaults is priority, choose a threshold with low FPR
> - If maximizing approvals is priority, choose a threshold with high TPR
> 
> Although Model C has highest AUC, at the bank's desired FPR of 0.2, Model B actually provides a higher TPR.
> 
> Therefore, the bank selects Model B with a strategically chosen threshold to meet their business needs, exemplifying how ROC and AUC enable nuanced model evaluation and selection.

I hope this helps clarify the content of the callouts. Let me know if you have any further questions or feedback.