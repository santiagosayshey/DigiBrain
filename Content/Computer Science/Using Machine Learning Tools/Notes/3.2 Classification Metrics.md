> [!motivation] Why Measure Classification Performance
> 
> When developing classification models, we need objective ways to:
> 
> - Evaluate how accurately our model predicts class labels
> - Compare different classification algorithms and approaches
> - Tune model parameters for optimal performance
> - Understand trade-offs between different error types (false positives vs. false negatives)
> 
> Different classification metrics highlight different aspects of model performance, allowing us to select metrics aligned with the specific requirements of our application.

> [!idea] Confusion Matrix
> 
> A confusion matrix provides a tabular summary of classification results.
> 
> - Shows counts of true positives, false positives, true negatives, and false negatives
> - Basis for calculating many other metrics
> - Provides complete information about model's classification performance
> - Reveals which types of errors the model tends to make
> 
> **Structure**:
> 
> ||Predicted Positive|Predicted Negative|
> |---|---|---|
> |**Actual Positive**|True Positive (TP)|False Negative (FN)|
> |**Actual Negative**|False Positive (FP)|True Negative (TN)|
> 
> 
> ![[Pasted image 20250324151634.png|600]]

> [!idea] Accuracy
> 
> Accuracy measures the proportion of correct predictions among all predictions made.
> 
> - Most intuitive and commonly used classification metric
> - Works well when classes are balanced (similar number of instances)
> - Simple to explain to non-technical stakeholders
> - Can be misleading for imbalanced datasets
> 
> **Mathematical Definition**: $$\text{Accuracy} = \frac{\text{Number of Correct Predictions}}{\text{Total Number of Predictions}} = \frac{TP + TN}{TP + TN + FP + FN}$$
> 
> This formula:
> 
> 1. Counts true positives (TP) and true negatives (TN) - correct predictions
> 2. Divides by the total number of predictions (including false positives (FP) and false negatives (FN))
> 3. Yields a value between 0 and 1, often expressed as a percentage

> [!idea] Precision and Recall
> 
> These metrics focus on different aspects of positive class prediction.
> 
> **Precision**
> 
> - Measures the accuracy of positive predictions. **What fraction of positive predictions are correct?**
> - Answers: "When the model predicts positive, how often is it correct?"
> - Critical when false positives are costly
> - Also called Positive Predictive Value
> 
> **Mathematical Definition**: $$\text{Precision} = \frac{TP}{TP + FP}$$
> 
> This formula divides true positives by all positive predictions, measuring how many positive predictions were actually correct.
> 
> **Recall**
> 
> - Measures the ability to find all positive instances. **What fraction of the real positive class are detected?**
> - Answers: "What proportion of actual positives did the model identify?"
> - Critical when missing positive cases is costly
> - Also called Sensitivity or True Positive Rate
> 
> **Mathematical Definition**: $$\text{Recall} = \frac{TP}{TP + FN}$$
> 
> This formula divides true positives by all actual positive cases, measuring how many positive cases were correctly identified.

> [!idea] F1 Score
> 
> F1 Score combines precision and recall into a single metric.
> 
> - Harmonic mean of precision and recall
> - Balances both metrics when a single evaluation metric is needed
> - Particularly useful for imbalanced datasets
> - Ranges from 0 (worst) to 1 (best)
> 
> **Mathematical Definition**: $$F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} = \frac{2TP}{2TP + FP + FN}$$
> 
> This formula:
> 
> 1. Multiplies precision and recall
> 2. Divides by their sum and multiplies by 2 (to scale from 0-1 instead of 0-0.5)
> 3. Ensures the model is penalized if either precision or recall is low


> [!consider] Classification Threshold and Precision-Recall Tradeoff
> 
> **Classification Threshold**
> 
> - Most classifiers don't directly output class labels (0/1); they output probability scores (0.0-1.0)
> - The threshold is the cutoff point above which we assign the positive class
> - Default threshold is typically 0.5, but can be adjusted based on needs
> - Example: For a spam classifier, if threshold = 0.7, emails with spam score > 0.7 are classified as spam
> 
> **Why Precision and Recall Trade Off**
> 
> - When we lower the threshold:
>     - More actual positives are correctly identified (↑ recall)
>     - But more negatives are incorrectly classified as positive (↓ precision)
>     - Example: Lower spam threshold catches more actual spam (higher recall) but also misclassifies more legitimate emails as spam (lower precision)
> - When we raise the threshold:
>     - Fewer negatives are incorrectly classified as positive (↑ precision)
>     - But more actual positives are missed (↓ recall)
>     - Example: Higher spam threshold ensures emails flagged as spam are nearly always spam (higher precision) but misses some actual spam (lower recall)
> 
> **When to Prioritize Recall**
> 
> - Medical diagnosis: Missing a disease (false negative) could be life-threatening
> - Fraud detection in banking: Missing fraudulent transactions could lead to significant financial losses
> - Predictive maintenance: Missing equipment failure signals could result in costly downtime
> 
> **When to Prioritize Precision**
> 
> - Content recommendation: Irrelevant recommendations (false positives) frustrate users
> - Criminal justice: Falsely identifying someone as guilty has severe consequences
> - Email spam filtering: Legitimate emails in spam folder (false positives) may never be seen
> 
> This inverse relationship means we rarely maximize both metrics simultaneously, requiring a strategic choice based on which error type is more costly in a given application.

> [!idea] ROC Curve (RUC)
> 
> The ROC (Receiver Operating Characteristic) curve visualizes the performance of a binary classifier across different classification thresholds.
> 
> - Plots True Positive Rate (Recall) against False Positive Rate for varying thresholds
> - Highlights trade-offs between sensitivity and fall-out
> - Independent of any specific threshold
> - Useful for evaluating overall model ranking ability
> 
> **Mathematics**:
> 
> - True Positive Rate (TPR): $\frac{TP}{TP + FN}$
> - False Positive Rate (FPR): $\frac{FP}{FP + TN}$
> - A perfect classifier has a point in the top-left (TPR=1, FPR=0)
> 
> ![[Pasted image 20250324181029.png|500]]


> [!idea] AUC
> 
> AUC (Area Under the Curve) quantifies the area under the ROC curve.
> 
> - Represents the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative one
> - Scalar metric summarizing ROC performance across all thresholds
> - Ranges from 0 to 1
> - AUC = 0.5 → random guess; AUC = 1 → perfect separation
> 
> **Mathematics**:
> 
> - AUC=∫01TPR(FPR) dFPR\text{AUC} = \int_0^1 \text{TPR}(FPR) \, dFPR
> - Typically approximated using trapezoidal rule over discrete points
> 
> **Interpretation**:
> 
> - Does not rely on a specific classification threshold
> - Useful for comparing models' ranking capability, especially under class imbalance

> [!example] ROC and AUC in Practice
> 
> A model for predicting loan defaults assigns a probability score to each applicant.
> 
> - Model A: AUC = 0.92
> - Model B: AUC = 0.65
> 
> **Scenario**:
> 
> - Bank wants to prioritize applicants most likely to default for further review
> - Threshold has not been decided yet
> 
> **How ROC/AUC helps**:
> 
> - ROC curves show how each model performs across all thresholds
> - AUC allows comparison without needing to fix a threshold
> - Model A is preferred as it better separates defaulters from non-defaulters at all threshold levels
> 
> image_goes_here  
> _ROC curves for Model A (higher curve) and Model B (closer to diagonal), both compared to the baseline random line_