> [!motivation] From Classification to Regression
> 
> While classification creates boundaries between different classes, many real-world problems require predicting continuous values rather than class labels.
> 
> **Regression challenges:**
> 
> - Predicting precise numerical values instead of class labels
> - Maintaining the benefits of large margin approaches in a regression context
> - Handling both linear and nonlinear relationships between features and targets
> - Managing sensitivity to outliers in continuous data
> 
> Support Vector Regression (SVR) adapts the core SVM principles to regression tasks by fundamentally reinterpreting the concept of the "margin." Rather than separating classes, SVR focuses on fitting data within a specified error range while maintaining maximal flatness in the prediction function.
> 
> This approach offers unique advantages over traditional regression models, particularly when dealing with complex nonlinear patterns or when robust performance against outliers is required.

> [!idea] Linear SVM Regression
> 
> Support Vector Regression (SVR) flips the SVM classification concept: instead of maximizing the margin between classes, it tries to fit as many instances as possible within a margin (or "tube") around the regression line.
> 
> **Key concepts:**
> 
> - The **epsilon (ε) parameter** controls the width of this tube
> - Points inside the tube contribute zero error to the objective function
> - Points outside the tube become support vectors and contribute to the model
> - The model aims to be as flat as possible while keeping most points within the tube
> 
> **Image 1 goes here**
> 
> The image shows two linear SVR models with different ε values (0.5 and 1.2). The left model with smaller ε creates a narrower tube and has more support vectors (gray circles). The right model with larger ε has a wider tube and fewer support vectors.
> 
> **Impact of epsilon:**
> 
> - Smaller ε: Narrower tube, more support vectors, more complex model
> - Larger ε: Wider tube, fewer support vectors, simpler model
> 
> **Implementation in scikit-learn:**
> 
> ```python
> from sklearn.svm import LinearSVR
> from sklearn.pipeline import make_pipeline
> from sklearn.preprocessing import StandardScaler
> 
> # Create a linear SVR model
> svm_reg = make_pipeline(
>     StandardScaler(),
>     LinearSVR(epsilon=0.5, random_state=42)
> )
> 
> # Train the model
> svm_reg.fit(X, y)
> ```
> 
> The ε-insensitive loss function makes SVR robust to small errors and outliers, as predictions within ε distance of the true value contribute no penalty to the optimization.

> [!idea] Nonlinear SVM Regression
> 
> Just as with classification, kernels allow SVMs to model nonlinear relationships in regression tasks without explicitly transforming the feature space.
> 
> **Key advantages:**
> 
> - Captures complex, nonlinear patterns in data
> - Maintains the robustness and ε-insensitivity of linear SVR
> - Leverages the kernel trick for computational efficiency
> 
> **Image 2 goes here**
> 
> The image shows two polynomial SVR models (degree=2) with different C values. Both have the same ε=0.1 but different regularization strengths. The left model (C=0.01) has a smoother curve due to stronger regularization, while the right model (C=100) fits the training data more closely.
> 
> **Key parameters:**
> 
> - **kernel**: Defines the transformation (polynomial, RBF, etc.)
> - **degree**: For polynomial kernels, controls complexity of the curve
> - **C**: Controls regularization strength (smaller C = more regularization)
> - **epsilon**: Controls the width of the insensitive tube
> 
> **Implementation in scikit-learn:**
> 
> ```python
> from sklearn.svm import SVR
> from sklearn.pipeline import make_pipeline
> from sklearn.preprocessing import StandardScaler
> 
> # Create a nonlinear SVR model with polynomial kernel
> svm_poly_reg = make_pipeline(
>     StandardScaler(),
>     SVR(kernel="poly", degree=2, C=0.01, epsilon=0.1)
> )
> 
> # Train the model
> svm_poly_reg.fit(X, y)
> ```
> 
> **Implementation considerations:**
> 
> - `LinearSVR` scales well with dataset size but only handles linear relationships
> - `SVR` with kernels can model complex patterns but becomes slow with large datasets
> - Proper feature scaling is crucial for kernel-based methods