> [!motivation] Beyond Spheres: Modeling Complex Clusters
>
> We saw that K-Means struggles when clusters aren't roughly spherical or when they have varying sizes and densities. This limitation arises because K-Means assigns points based solely on distance to a central point.
>
> Gaussian Mixture Models (GMMs) offer a more flexible, probabilistic approach. They assume data points originate from a combination of several Gaussian distributions, each potentially representing an ellipsoid-shaped cluster with its own size, shape, orientation, and density. This makes GMMs better suited for modeling more complex cluster structures.

> [!idea] GMM vs K-Means: The Key Difference - Soft vs. Hard Clustering
>
> Both K-Means and Gaussian Mixture Models (GMMs) are used for clustering data, but they approach it differently, leading to distinct outcomes.
>
> * **K-Means Review**: Assigns each data point strictly to the *one* cluster whose center (centroid) is closest. It assumes clusters are generally round (spherical). This is called **hard clustering**.
>
> * **GMM Approach**: Also groups data into a set number ($k$) of clusters, but GMM is designed to handle clusters that might be **elliptical (oval-shaped, stretched, or rotated)** and potentially **overlap**.
>
> * **The Fundamental Difference**: Instead of forcing each point into a single cluster, GMM calculates the **probability** that a point belongs to *each* cluster. For example, it might determine a point has a 70% chance of belonging to cluster 1 and a 30% chance of belonging to cluster 2. This is called **soft clustering**.
>
> **Why is this difference important?**
> The probabilistic ("soft") assignment makes GMM more flexible:
> 1.  It can better model clusters that aren't perfectly round.
> 2.  It provides a measure of certainty or uncertainty about cluster membership, especially for points near boundaries or in overlapping regions.
>
> So, while K-Means gives a definite answer ("Point X is in Cluster A"), GMM gives a probabilistic answer ("Point X is *most likely* in Cluster A, but also has some probability of being in Cluster B").

> [!consider] How GMMs Model Shapes and Probabilities via EM
>
> We know GMMs handle non-spherical (ellipsoidal) clusters and assign probabilities ("soft" assignments), unlike K-Means. How does it accomplish this? The key is the **Expectation-Maximization (EM)** algorithm, which learns not just the cluster center, but also its shape and relative importance.
>
> For each of the $k$ clusters (Gaussian components) it tries to find, GMM estimates three main things:
> * **Mean ($\mu$)**: The central location of the cluster's ellipsoid.
> * **Covariance ($\Sigma$)**: A matrix describing the ellipsoid's shape â€“ how spread out it is, whether it's stretched or compressed along different axes, and its overall orientation or tilt.
> * **Weight ($\phi$)**: The relative size or importance of this cluster compared to the others (how many points likely belong to it).
>
> The EM algorithm is an iterative process to find the best values for these parameters ($\mu$'s, $\Sigma$'s, $\phi$'s) for all $k$ components:
>
> 1.  **Start with Guesses:** The algorithm begins with initial estimates for the center ($\mu$), shape ($\Sigma$), and weight ($\phi$) of each of the $k$ ellipsoids.
>
> 2.  **E-step (Expectation) - Calculate Probabilities:** Using the *current estimates* for each ellipsoid's center ($\mu$) and shape ($\Sigma$), this step calculates, for *every* data point, the probability that it belongs to *each* ellipsoid. This probability is higher if the point fits well within the ellipsoid's shape and is closer to its center. These probabilities are the "soft assignments" or responsibilities.
>
> 3.  **M-step (Maximization) - Update Parameters:** This step refines the estimates for each ellipsoid's parameters ($\mu, \Sigma, \phi$) based on the probabilities calculated in the E-step. It uses a *weighted average* of all data points, where points with higher probability (better fit) for a given ellipsoid have more influence on that ellipsoid's update.
>     * This weighted update recalculates the ellipsoid's **center** (the new $\mu$), its **shape/orientation** (the new $\Sigma$), and its relative **weight/size** (the new $\phi$). Because the shape ($\Sigma$) itself is being updated based on how well points fit the *current* shape, the model learns to adapt to non-spherical distributions.
>
> 4.  **Iteration:** The algorithm repeats the E-step (re-calculating probabilities based on the *updated* ellipsoids) and M-step (further *updating* the parameters $\mu, \Sigma, \phi$) until the parameters stabilize.
>
> In essence, by iteratively estimating probabilities based on both center *and shape*, and then updating *both* center and shape parameters using those probabilities, EM allows GMMs to discover and model flexible, ellipsoidal clusters.

> [!example] GMM Implementation with Scikit-Learn
>
> Scikit-Learn's `GaussianMixture` class allows us to apply the concepts discussed previously.
>
> 1.  **Initialization and Fitting (Running EM)**: Import `GaussianMixture`. When instantiating, specify the number of components $k$ (`n_components`) and typically set `n_init > 1` because the EM algorithm, like K-Means, can converge to suboptimal solutions depending on the start. Calling `fit(X)` executes the Expectation-Maximization (EM) algorithm iteratively to find the parameters ($\mu, \Sigma, \phi$) of the $k$ Gaussian components that best describe the data `X`.
>     ```python
>     from sklearn.mixture import GaussianMixture
>     import numpy as np
>
>     # Assuming X is your data
>     k = 3 # Number of Gaussian components (clusters)
>     gm = GaussianMixture(n_components=k, n_init=10, random_state=42)
>     gm.fit(X) # Runs the EM algorithm
>
>     # Check convergence status
>     # print(f"Converged: {gm.converged_}, Iterations: {gm.n_iter_}")
>     ```
>
> 2.  **Inspecting Learned Parameters**: After `fit`, the `gm` object holds the estimated parameters found by EM:
>     ```python
>     # Estimated cluster weights (phi) - relative size/importance
>     weights = gm.weights_
>     # Example: array([0.39, 0.40, 0.21])
>
>     # Estimated cluster means (mu) - ellipsoid centers
>     means = gm.means_
>     # Example: array([[ 0.05, 0.08], [-1.41, 1.43], [ 3.40, 1.06]])
>
>     # Estimated cluster covariances (Sigma) - ellipsoid shape/orientation/size
>     covariances = gm.covariances_
>     # Example: array([[[ 0.69, 0.80], [ 0.80, 1.21]], ... ]) # k matrices
>     ```
>     These represent the learned properties of the underlying Gaussian distributions that GMM assumes generated the data.
>
> 3.  **Cluster Assignments (Using Learned Parameters)**:
>     * `predict()`: Assigns each data point to the Gaussian component (cluster) it most likely belongs to, based on the final probabilities calculated using the learned $\mu$'s and $\Sigma$'s during the last E-step of EM. This gives a "hard" assignment.
>     ```python
>     # Hard clustering: Assign each point to the most likely component
>     hard_labels = gm.predict(X)
>     # Example: array([0, 0, 1, ..., 2, 2, 2])
>     ```
>     * `predict_proba()`: Directly returns the probabilities (responsibilities) calculated in the final E-step, indicating the likelihood of each point belonging to each of the $k$ learned Gaussian components. This is the "soft" assignment.
>     ```python
>     # Soft clustering: Get probabilities for each point belonging to each component
>     soft_probabilities = gm.predict_proba(X)
>     # Example: array([[0.977, 0.   , 0.023], [0.983, 0.001, 0.016], ...])
>     ```
>
> 4.  **Generating New Samples (`sample`)**: Because GMM learns the parameters ($\mu, \Sigma, \phi$) of the underlying distributions, it's a generative model and can create new data points that follow those learned distributions.
>     ```python
>     # X_new, y_new = gm.sample(6) # Generate 6 new samples
>     ```
>
> 5.  **Density Estimation (`score_samples`)**: Evaluate the learned model's probability density function (PDF) at given points. Higher scores indicate regions where the learned mixture model assigns higher probability density.
>     ```python
>     # log_pdf_values = gm.score_samples(X) # Get log PDF value
>     # Example: array([-2.61, -3.57, -3.33, ...])
>     ```
> 
> ![[Pasted image 20250428163828.png|500]]
>
> * **Controlling Shape (`covariance_type`)**: This hyperparameter constrains the structure of the covariance matrices ($\Sigma$'s) that the EM algorithm is allowed to learn (e.g., "spherical", "diag", "tied", "full"), influencing the flexibility of the learned ellipsoid shapes.
> 
> ![[Pasted image 20250428163848.png|500]]

> [!consider] Anomaly Detection with GMMs
>
> Gaussian Mixture Models can be effectively used for **anomaly detection** (or outlier detection), which is the task of identifying instances that deviate significantly from the norm. Instances flagged are called anomalies or outliers, while normal instances are inliers. This is useful for applications like fraud detection or identifying defective products.
>
> **Core Idea:**
> The GMM, having learned the underlying probability distribution(s) of the data, can estimate the probability density at any given point. Instances located in **low-density regions** (where the model assigns a low probability density) are considered potential anomalies.
>
> **Mechanism & Threshold:**
> 1.  Use the `score_samples(X)` method of a trained GMM (`gm`) to get the log probability density for each instance. Lower scores mean lower density.
> 2.  Define a **density threshold**. Instances whose density score falls below this threshold are flagged as anomalies.
> 3.  This threshold is often determined based on the expected percentage of anomalies. For example, if you expect 2% of products to be defective, you can set the threshold at the 2nd percentile of the density scores. Adjusting this threshold allows trading off precision (fewer false positives) and recall (fewer false negatives).
>
> **Implementation Example:** Identifying anomalies based on the lowest 4% density scores:
> ```python
> # Assuming 'gm' is a fitted GaussianMixture model and 'X' is the data
> densities = gm.score_samples(X) # Get log probability densities
> density_threshold = np.percentile(densities, 4) # Find the 4th percentile threshold
> anomalies = X[densities < density_threshold] # Select instances below threshold
> ```
>
> ![[Pasted image 20250428164123.png|500]]
>
> **Anomaly vs. Novelty Detection**: Anomaly detection assumes outliers might be present in the training data (and often aims to find them). Novelty detection assumes the training data is "clean" and aims to identify *new* instances that differ from the training distribution.
>
> **Potential Issue (Outlier Bias)**: Since GMM tries to fit *all* data, a large number of outliers can skew the model's perception of "normal" density. If this occurs, consider fitting the model once, removing the most extreme outliers identified, and then refitting the GMM on the cleaned data. Alternatively, robust covariance estimation techniques (e.g., `sklearn.covariance.EllipticEnvelope`) can be explored.

> [!consider] Selecting the Number of Clusters (k) for GMMs
>
> Unlike K-Means, using metrics like inertia or silhouette score to find the optimal number of clusters ($k$) for GMMs is unreliable. This is because GMMs can model non-spherical and varying-sized clusters well, making these distance-based metrics less informative about the true underlying structure.
>
> **Information Criteria (BIC & AIC):**
> Instead, theoretical information criteria are preferred for selecting $k$ with GMMs. The two most common are:
> * **Bayesian Information Criterion (BIC)**
> * **Akaike Information Criterion (AIC)**
>
> Both criteria aim to find a balance between how well the model fits the data and how complex the model is (how many parameters it needs to learn).
> * **Goal**: Find the model (and thus the number of clusters $k$) that **minimizes** the BIC or AIC score.
> * **How they work (Conceptually)**:
>     * They reward models that achieve a high **maximized likelihood value** ($\hat{L}$). $\hat{L}$ represents how well the best version of the GMM (with its learned parameters $\mu, \Sigma, \phi$) explains the observed data $X$. A higher $\hat{L}$ means a better fit. (See Figure 9-19 for concept illustration).
>     * They penalize models with more parameters ($p$). GMMs with more clusters ($k$) inherently have more parameters ($\mu$'s, $\Sigma$'s, $\phi$'s) to estimate.
>     * The formulas (simplified) are:
>         * $BIC \approx (\text{complexity penalty}) \times p - 2 \log(\hat{L})$
>         * $AIC = 2p - 2 \log(\hat{L})$
>         (where $p$ increases with $k$, and the BIC penalty term $\log(m)p$ grows faster with more data $m$ than AIC's $2p$).
> * **BIC vs. AIC**: They often select the same $k$. When they differ, BIC tends to favor simpler models (smaller $k$) than AIC, especially on large datasets.
>
> ![[Pasted image 20250428164715.png|500]]
>
> **Implementation in Scikit-Learn:**
> After fitting a `GaussianMixture` model (`gm`) to data `X`, you can calculate these criteria using built-in methods:
> ```python
> # Assuming 'gm' is a fitted GaussianMixture object for a specific k
> bic_score = gm.bic(X)
> aic_score = gm.aic(X)
> # print(f"BIC: {bic_score}, AIC: {aic_score}")
> ```
>
> **Choosing *k***: The typical approach is to train GMMs with different values of $k$ (e.g., from 1 to 10), calculate the BIC and/or AIC for each $k$, and select the value of $k$ that results in the lowest score.
>
> ![[Pasted image 20250428164733.png|500]]

> [!idea] Bayesian GMM: Automatic Cluster Number Selection
>
> Instead of iterating through different values of $k$ and using criteria like BIC or AIC, **Bayesian Gaussian Mixture Models** offer an alternative approach that can automatically determine the relevant number of clusters.
>
> **Core Idea:**
> The `BayesianGaussianMixture` class in Scikit-Learn implements this. It works by assigning weights ($\phi$) to each potential cluster component during the fitting process. Crucially, it is capable of assigning weights that are zero or very close to zero to components that are not needed to explain the data.
>
> **How to Use:**
> 1.  Initialize `BayesianGaussianMixture` by setting `n_components` to a number that you believe is *larger* than the actual optimal number of clusters. (Requires some rough estimate).
> 2.  Fit the model (`bgm.fit(X)`) as usual.
> 3.  Inspect the `weights_` attribute of the fitted model. Components with near-zero weights are effectively considered unnecessary by the algorithm.
>
> *Example from Text:* If you set `n_components=10` but the data truly only has 3 clusters, the fitted `bgm.weights_` might show ~3 components with significant weights and the rest with weights near zero, indicating the model automatically identified that only 3 clusters were needed.
>
> **Important Limitation:**
> While Bayesian GMMs help with selecting $k$, they **still fundamentally assume that the underlying clusters are ellipsoidal** (Gaussian). They do not perform well on datasets with complex, non-ellipsoidal shapes, such as the moons dataset. In such cases, the algorithm might try to approximate the non-ellipsoidal shape using multiple small ellipsoids, failing to identify the true cluster structure.
>
> ![[Pasted image 20250428164956.png|500]]
> 
> Therefore, while useful for automating $k$-selection for ellipsoid-like data, BGMMs are not a solution for clustering arbitrarily shaped data.


> [!idea] Bayesian GMM: Automatic Cluster Number Selection
>
> A significant challenge with standard Gaussian Mixture Models (GMMs) is selecting the optimal number of clusters ($k$). Often, this requires fitting multiple models with different $k$ values and comparing them using criteria like BIC or AIC, which can be computationally intensive and sometimes ambiguous.
>
> **Bayesian Gaussian Mixture Models (BGMMs)** offer an alternative approach designed to mitigate this issue. Instead of requiring you to find the exact number of clusters beforehand, BGMMs allow the model itself to determine the *effective* number of components needed during the learning process.
>
> **How it Works:** The core idea is that the Bayesian approach allows the model to assign **weights** ($\phi$) to each potential cluster component it's considering. If a component is deemed unnecessary to explain the data, the model can automatically assign it a weight that is effectively zero (or very close to it).
> In practice, you initialize the `BayesianGaussianMixture` class with a number of components (`n_components`) that you estimate is *larger* than the true number required. When you fit the model, it automatically adjusts the component weights. By inspecting the `weights_` attribute after fitting, you can see which components the model kept active (non-zero weights) and which it discarded (near-zero weights), revealing the number of clusters it found necessary. For instance, setting `n_components=10` might result in only 3 components having significant weights, indicating the model determined $k=3$ was optimal for the data.
>
> This method effectively automates the process of finding a suitable number of clusters, avoiding the manual search required by standard GMMs combined with BIC/AIC.
>
> **Important Limitation:** Keep in mind that BGMMs **still assume the underlying clusters are fundamentally ellipsoidal** (Gaussian). They share the same limitation as standard GMMs when dealing with complex, non-ellipsoidal shapes like the moons dataset, where they might try to approximate the shape with multiple inappropriate ellipsoids.
>
> ```image_goes_here
> Caption: Figure 9-21. Bayesian GMM applied to non-ellipsoidal clusters.
> Description: A plot showing the moons dataset. Superimposed are the ellipsoids found by a Bayesian GMM (e.g., initialized with k=10). Instead of finding the two crescent shapes, the algorithm fits multiple (e.g., 8) smaller ellipsoids in an attempt to cover the data points, failing to capture the true underlying structure due to the Gaussian assumption.
> ```