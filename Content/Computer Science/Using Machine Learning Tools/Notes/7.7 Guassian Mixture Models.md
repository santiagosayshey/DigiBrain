> [!motivation] Beyond Spheres: Modeling Complex Clusters
>
> We saw that K-Means struggles when clusters aren't roughly spherical or when they have varying sizes and densities. This limitation arises because K-Means assigns points based solely on distance to a central point.
>
> Gaussian Mixture Models (GMMs) offer a more flexible, probabilistic approach. They assume data points originate from a combination of several Gaussian distributions, each potentially representing an ellipsoid-shaped cluster with its own size, shape, orientation, and density. This makes GMMs better suited for modeling more complex cluster structures.

> [!idea] GMM vs K-Means: The Key Difference - Soft vs. Hard Clustering
>
> Both K-Means and Gaussian Mixture Models (GMMs) are used for clustering data, but they approach it differently, leading to distinct outcomes.
>
> * **K-Means Review**: Assigns each data point strictly to the *one* cluster whose center (centroid) is closest. It assumes clusters are generally round (spherical). This is called **hard clustering**.
>
> * **GMM Approach**: Also groups data into a set number ($k$) of clusters, but GMM is designed to handle clusters that might be **elliptical (oval-shaped, stretched, or rotated)** and potentially **overlap**.
>
> * **The Fundamental Difference**: Instead of forcing each point into a single cluster, GMM calculates the **probability** that a point belongs to *each* cluster. For example, it might determine a point has a 70% chance of belonging to cluster 1 and a 30% chance of belonging to cluster 2. This is called **soft clustering**.
>
> **Why is this difference important?**
> The probabilistic ("soft") assignment makes GMM more flexible:
> 1.  It can better model clusters that aren't perfectly round.
> 2.  It provides a measure of certainty or uncertainty about cluster membership, especially for points near boundaries or in overlapping regions.
>
> So, while K-Means gives a definite answer ("Point X is in Cluster A"), GMM gives a probabilistic answer ("Point X is *most likely* in Cluster A, but also has some probability of being in Cluster B").

> [!consider] How GMMs Model Shapes and Probabilities via EM
>
> We know GMMs handle non-spherical (ellipsoidal) clusters and assign probabilities ("soft" assignments), unlike K-Means. How does it accomplish this? The key is the **Expectation-Maximization (EM)** algorithm, which learns not just the cluster center, but also its shape and relative importance.
>
> For each of the $k$ clusters (Gaussian components) it tries to find, GMM estimates three main things:
> * **Mean ($\mu$)**: The central location of the cluster's ellipsoid.
> * **Covariance ($\Sigma$)**: A matrix describing the ellipsoid's shape â€“ how spread out it is, whether it's stretched or compressed along different axes, and its overall orientation or tilt.
> * **Weight ($\phi$)**: The relative size or importance of this cluster compared to the others (how many points likely belong to it).
>
> The EM algorithm is an iterative process to find the best values for these parameters ($\mu$'s, $\Sigma$'s, $\phi$'s) for all $k$ components:
>
> 1.  **Start with Guesses:** The algorithm begins with initial estimates for the center ($\mu$), shape ($\Sigma$), and weight ($\phi$) of each of the $k$ ellipsoids.
>
> 2.  **E-step (Expectation) - Calculate Probabilities:** Using the *current estimates* for each ellipsoid's center ($\mu$) and shape ($\Sigma$), this step calculates, for *every* data point, the probability that it belongs to *each* ellipsoid. This probability is higher if the point fits well within the ellipsoid's shape and is closer to its center. These probabilities are the "soft assignments" or responsibilities.
>
> 3.  **M-step (Maximization) - Update Parameters:** This step refines the estimates for each ellipsoid's parameters ($\mu, \Sigma, \phi$) based on the probabilities calculated in the E-step. It uses a *weighted average* of all data points, where points with higher probability (better fit) for a given ellipsoid have more influence on that ellipsoid's update.
>     * This weighted update recalculates the ellipsoid's **center** (the new $\mu$), its **shape/orientation** (the new $\Sigma$), and its relative **weight/size** (the new $\phi$). Because the shape ($\Sigma$) itself is being updated based on how well points fit the *current* shape, the model learns to adapt to non-spherical distributions.
>
> 4.  **Iteration:** The algorithm repeats the E-step (re-calculating probabilities based on the *updated* ellipsoids) and M-step (further *updating* the parameters $\mu, \Sigma, \phi$) until the parameters stabilize.
>
> In essence, by iteratively estimating probabilities based on both center *and shape*, and then updating *both* center and shape parameters using those probabilities, EM allows GMMs to discover and model flexible, ellipsoidal clusters.

> [!example] GMM Implementation with Scikit-Learn
>
> Scikit-Learn's `GaussianMixture` class allows us to apply the concepts discussed previously.
>
> 1.  **Initialization and Fitting (Running EM)**: Import `GaussianMixture`. When instantiating, specify the number of components $k$ (`n_components`) and typically set `n_init > 1` because the EM algorithm, like K-Means, can converge to suboptimal solutions depending on the start. Calling `fit(X)` executes the Expectation-Maximization (EM) algorithm iteratively to find the parameters ($\mu, \Sigma, \phi$) of the $k$ Gaussian components that best describe the data `X`.
>     ```python
>     from sklearn.mixture import GaussianMixture
>     import numpy as np
>
>     # Assuming X is your data
>     k = 3 # Number of Gaussian components (clusters)
>     gm = GaussianMixture(n_components=k, n_init=10, random_state=42)
>     gm.fit(X) # Runs the EM algorithm
>
>     # Check convergence status
>     # print(f"Converged: {gm.converged_}, Iterations: {gm.n_iter_}")
>     ```
>
> 2.  **Inspecting Learned Parameters**: After `fit`, the `gm` object holds the estimated parameters found by EM:
>     ```python
>     # Estimated cluster weights (phi) - relative size/importance
>     weights = gm.weights_
>     # Example: array([0.39, 0.40, 0.21])
>
>     # Estimated cluster means (mu) - ellipsoid centers
>     means = gm.means_
>     # Example: array([[ 0.05, 0.08], [-1.41, 1.43], [ 3.40, 1.06]])
>
>     # Estimated cluster covariances (Sigma) - ellipsoid shape/orientation/size
>     covariances = gm.covariances_
>     # Example: array([[[ 0.69, 0.80], [ 0.80, 1.21]], ... ]) # k matrices
>     ```
>     These represent the learned properties of the underlying Gaussian distributions that GMM assumes generated the data.
>
> 3.  **Cluster Assignments (Using Learned Parameters)**:
>     * `predict()`: Assigns each data point to the Gaussian component (cluster) it most likely belongs to, based on the final probabilities calculated using the learned $\mu$'s and $\Sigma$'s during the last E-step of EM. This gives a "hard" assignment.
>     ```python
>     # Hard clustering: Assign each point to the most likely component
>     hard_labels = gm.predict(X)
>     # Example: array([0, 0, 1, ..., 2, 2, 2])
>     ```
>     * `predict_proba()`: Directly returns the probabilities (responsibilities) calculated in the final E-step, indicating the likelihood of each point belonging to each of the $k$ learned Gaussian components. This is the "soft" assignment.
>     ```python
>     # Soft clustering: Get probabilities for each point belonging to each component
>     soft_probabilities = gm.predict_proba(X)
>     # Example: array([[0.977, 0.   , 0.023], [0.983, 0.001, 0.016], ...])
>     ```
>
> 4.  **Generating New Samples (`sample`)**: Because GMM learns the parameters ($\mu, \Sigma, \phi$) of the underlying distributions, it's a generative model and can create new data points that follow those learned distributions.
>     ```python
>     # X_new, y_new = gm.sample(6) # Generate 6 new samples
>     ```
>
> 5.  **Density Estimation (`score_samples`)**: Evaluate the learned model's probability density function (PDF) at given points. Higher scores indicate regions where the learned mixture model assigns higher probability density.
>     ```python
>     # log_pdf_values = gm.score_samples(X) # Get log PDF value
>     # Example: array([-2.61, -3.57, -3.33, ...])
>     ```
> 
> ![[Pasted image 20250428163828.png|500]]
>
> * **Controlling Shape (`covariance_type`)**: This hyperparameter constrains the structure of the covariance matrices ($\Sigma$'s) that the EM algorithm is allowed to learn (e.g., "spherical", "diag", "tied", "full"), influencing the flexibility of the learned ellipsoid shapes.
> 
> ![[Pasted image 20250428163848.png|500]]

> [!consider] Anomaly Detection with GMMs
>
> Gaussian Mixture Models can be effectively used for **anomaly detection** (or outlier detection), which is the task of identifying instances that deviate significantly from the norm. Instances flagged are called anomalies or outliers, while normal instances are inliers. This is useful for applications like fraud detection or identifying defective products.
>
> **Core Idea:**
> The GMM, having learned the underlying probability distribution(s) of the data, can estimate the probability density at any given point. Instances located in **low-density regions** (where the model assigns a low probability density) are considered potential anomalies.
>
> **Mechanism & Threshold:**
> 1.  Use the `score_samples(X)` method of a trained GMM (`gm`) to get the log probability density for each instance. Lower scores mean lower density.
> 2.  Define a **density threshold**. Instances whose density score falls below this threshold are flagged as anomalies.
> 3.  This threshold is often determined based on the expected percentage of anomalies. For example, if you expect 2% of products to be defective, you can set the threshold at the 2nd percentile of the density scores. Adjusting this threshold allows trading off precision (fewer false positives) and recall (fewer false negatives).
>
> **Implementation Example:** Identifying anomalies based on the lowest 4% density scores:
> ```python
> # Assuming 'gm' is a fitted GaussianMixture model and 'X' is the data
> densities = gm.score_samples(X) # Get log probability densities
> density_threshold = np.percentile(densities, 4) # Find the 4th percentile threshold
> anomalies = X[densities < density_threshold] # Select instances below threshold
> ```
>
> ![[Pasted image 20250428164123.png|500]]
>
> **Anomaly vs. Novelty Detection**: Anomaly detection assumes outliers might be present in the training data (and often aims to find them). Novelty detection assumes the training data is "clean" and aims to identify *new* instances that differ from the training distribution.
>
> **Potential Issue (Outlier Bias)**: Since GMM tries to fit *all* data, a large number of outliers can skew the model's perception of "normal" density. If this occurs, consider fitting the model once, removing the most extreme outliers identified, and then refitting the GMM on the cleaned data. Alternatively, robust covariance estimation techniques (e.g., `sklearn.covariance.EllipticEnvelope`) can be explored.


