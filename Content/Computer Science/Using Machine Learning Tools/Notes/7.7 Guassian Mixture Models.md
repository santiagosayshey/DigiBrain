> [!motivation] Beyond Spheres: Modeling Complex Clusters
>
> We saw that K-Means struggles when clusters aren't roughly spherical or when they have varying sizes and densities. This limitation arises because K-Means assigns points based solely on distance to a central point.
>
> Gaussian Mixture Models (GMMs) offer a more flexible, probabilistic approach. They assume data points originate from a combination of several Gaussian distributions, each potentially representing an ellipsoid-shaped cluster with its own size, shape, orientation, and density. This makes GMMs better suited for modeling more complex cluster structures.

> [!idea] Gaussian Mixture Models (GMMs): A Probabilistic Approach
>
> To understand Gaussian Mixture Models (GMMs), let's first look at a single **Gaussian Distribution** (also known as the Normal Distribution or "bell curve").
> * **Concept**: It describes data that clusters around a central value (the mean, $\mu$) with points spreading out symmetrically. The spread (how wide the bell is) is determined by the variance ($\sigma^2$) or standard deviation ($\sigma$).
> * **Multi-Dimensional Shape**: In multiple dimensions (i.e., with multiple features), a Gaussian distribution naturally forms shapes that look like **ellipsoids** (or spheres as a special case). The center of the ellipsoid is the mean vector ($\mu$). The size, "stretch", and orientation (rotation) of the ellipsoid are determined by the **covariance matrix** ($\Sigma$). Points are densest near the center ($\mu$) and become less dense further away, following the ellipsoidal shape.
>
> **Gaussian Mixture Models (GMMs)** assume that your *entire dataset* wasn't generated by just one of these ellipsoidal Gaussian sources, but rather by a *mixture* of several ($k$) different Gaussian distributions operating together.
> * **Analogy**: Imagine $k$ different machines, each producing points according to its own Gaussian distribution (its own ellipsoidal shape, size, orientation $\Sigma$, centered at $\mu$). Some machines might produce more points than others (their weight $\phi$). GMM assumes your observed data is the combined output from all these machines, and its goal is to figure out the settings (parameters $\mu, \Sigma, \phi$) of each "machine" or component based on the final mix of points.
>
> **Learning the Mixture (EM Algorithm)**:
> * The algorithm used to estimate these unknown parameters ($\mu$'s, $\Sigma$'s, $\phi$'s) from the data is typically **Expectation-Maximization (EM)**.
> * **Comparison to K-Means**: Like K-Means, EM is iterative and alternates between assigning points (Expectation) and updating parameters (Maximization).
> * **Soft Assignments**: The key difference is that during the Expectation step, EM calculates the *probability* (or responsibility) that each data point belongs to *each* of the $k$ Gaussian components. During Maximization, each component's parameters are updated using all data points, but weighted by these probabilities. This is a "soft" assignment, unlike K-Means' "hard" assignment to only one cluster.
>
> Because GMMs model these underlying ellipsoidal probability distributions, they are more flexible than K-Means for clusters with varying shapes and orientations, and they are also generative models (can create new data points).

> [!example] GMM Implementation with Scikit-Learn
>
> Scikit-Learn's `GaussianMixture` class implements GMMs.
>
> 1.  **Initialization and Fitting**: Import `GaussianMixture`, specify the number of components ($k$, via `n_components`), and set `n_init` (similar to K-Means, running multiple initializations helps avoid poor local optima, default is 1, so set higher e.g., 10). Then, `fit` the model to the data `X`.
>     ```python
>     from sklearn.mixture import GaussianMixture
>     import numpy as np
>
>     # Assuming X is your data
>     k = 3 # Number of Gaussian components (clusters)
>     gm = GaussianMixture(n_components=k, n_init=10, random_state=42)
>     gm.fit(X)
>
>     # Check convergence status
>     # print(f"Converged: {gm.converged_}, Iterations: {gm.n_iter_}")
>     ```
>
> 2.  **Inspecting Parameters**: Access the estimated parameters of the Gaussian components.
>     ```python
>     # Cluster weights (phi)
>     weights = gm.weights_
>     # Example: array([0.39, 0.40, 0.21])
>
>     # Cluster means (mu)
>     means = gm.means_
>     # Example: array([[ 0.05, 0.08], [-1.41, 1.43], [ 3.40, 1.06]])
>
>     # Cluster covariances (Sigma)
>     covariances = gm.covariances_
>     # Example: array([[[ 0.69, 0.80], [ 0.80, 1.21]], ... ]) # k matrices
>     ```
>
> 3.  **Cluster Assignments**: Use `predict()` for hard assignments (which Gaussian component is most likely for each instance) or `predict_proba()` for soft assignments (the probability of belonging to each component).
>     ```python
>     # Hard clustering: Assign each point to the most likely component
>     hard_labels = gm.predict(X)
>     # Example: array([0, 0, 1, ..., 2, 2, 2])
>
>     # Soft clustering: Get probabilities for each point belonging to each component
>     soft_probabilities = gm.predict_proba(X)
>     # Example: array([[0.977, 0.   , 0.023], [0.983, 0.001, 0.016], ...])
>     ```
>
> 4.  **Generating New Samples**: Use `sample()` to draw new data points from the learned mixture model.
>     ```python
>     # X_new, y_new = gm.sample(6) # Generate 6 new samples
>     ```
>
> 5.  **Density Estimation**: Use `score_samples()` to get the log probability density of the model at given points (higher score means higher density region).
>     ```python
>     # log_pdf_values = gm.score_samples(X)
>     # Example: array([-2.61, -3.57, -3.33, ...])
>     ```
> ```image_goes_here
> Caption: Figure 9-16. Visualization of a trained GMM.
> Description: A plot showing data points, the estimated means (centers) of the Gaussian components (clusters), decision boundaries (dashed lines) separating the regions where each component is most likely, and density contour lines for each component, illustrating their ellipsoidal shapes and orientations.
> ```
>
> * **Controlling Shape (`covariance_type`)**: The flexibility of the ellipsoid shapes can be controlled with the `covariance_type` hyperparameter ("full", "tied", "spherical", "diag"), which can help if EM struggles to converge or if you have prior knowledge about cluster shapes.
> ```image_goes_here
> Caption: Figure 9-17. GMM results with constrained covariance types.
> Description: Two plots showing GMM results on the same data but with constraints. Left plot uses `covariance_type="tied"` (all clusters forced to have the same shape/size/orientation). Right plot uses `covariance_type="spherical"` (clusters forced to be spherical but can have different sizes). Shows how constraints affect the model's ability to fit the data.
> ```