> [!motivation] Beyond Spheres: Modeling Complex Clusters
>
> We saw that K-Means struggles when clusters aren't roughly spherical or when they have varying sizes and densities. This limitation arises because K-Means assigns points based solely on distance to a central point.
>
> Gaussian Mixture Models (GMMs) offer a more flexible, probabilistic approach. They assume data points originate from a combination of several Gaussian distributions, each potentially representing an ellipsoid-shaped cluster with its own size, shape, orientation, and density. This makes GMMs better suited for modeling more complex cluster structures.

> [!idea] Gaussian Mixture Models (GMMs): Finding Flexible Clusters
>
> We know K-Means is good at finding roundish (spherical) clusters, but sometimes data groups are stretched, tilted ovals (ellipsoids), or they overlap. **Gaussian Mixture Models (GMMs)** are designed for these situations.
>
> **The Goal:** GMM tries to find the best collection of a specific number ($k$) of **ellipsoid shapes** that collectively fit the pattern of your data points. For each ellipsoid, it figures out:
> * Where its center is.
> * How stretched or compressed it is in different directions.
> * What its orientation (tilt angle) is.
> * How relatively large or important it is compared to the others.
>
> **How it Works (Simplified):**
> GMM uses an iterative process to find these best-fitting ellipsoids:
> 1.  **Initial Guess:** It starts by making some initial guesses for the position, shape, and size of the $k$ ellipsoids.
> 2.  **"Soft" Assignment Check:** It looks at each data point and evaluates how well it fits within each of the *current* guessed ellipsoids. Unlike K-Means which forces a point into only the single closest cluster, GMM acknowledges that a point might plausibly fit into multiple ellipsoids, especially where they overlap. It calculates a score or likelihood for each point belonging to each ellipsoid.
> 3.  **Adjusting the Ellipsoids:** Based on how well the points fit, the algorithm then *readjusts* the properties (center, shape, size, orientation) of each ellipsoid. Ellipsoids are essentially "pulled" towards the data points that fit them best according to the scores calculated in the previous step.
> 4.  **Repeat:** Steps 2 and 3 are repeated â€“ check how well points fit the *new* ellipsoids, then adjust the ellipsoids again based on that fit. This continues until the ellipsoids stop changing significantly.
>
> **What You Get:**
> * The final set of $k$ fitted ellipsoids describing the data's structure.
> * For any data point, GMM can tell you which ellipsoid it *most likely* belongs to (a "hard" assignment like K-Means).
> * More usefully, it can give you the *probability* or score indicating how likely the point belongs to *each* of the ellipsoids (a "soft" assignment), which is helpful for understanding uncertainty or overlap.
>
> This iterative adjustment process, considering how well points fit shaped ellipsoids rather than just distance to a center point, is what allows GMMs to successfully model more complex cluster shapes than K-Means.

> [!example] GMM Implementation with Scikit-Learn
>
> Scikit-Learn's `GaussianMixture` class implements GMMs.
>
> 1.  **Initialization and Fitting**: Import `GaussianMixture`, specify the number of components ($k$, via `n_components`), and set `n_init` (similar to K-Means, running multiple initializations helps avoid poor local optima, default is 1, so set higher e.g., 10). Then, `fit` the model to the data `X`.
>     ```python
>     from sklearn.mixture import GaussianMixture
>     import numpy as np
>
>     # Assuming X is your data
>     k = 3 # Number of Gaussian components (clusters)
>     gm = GaussianMixture(n_components=k, n_init=10, random_state=42)
>     gm.fit(X)
>
>     # Check convergence status
>     # print(f"Converged: {gm.converged_}, Iterations: {gm.n_iter_}")
>     ```
>
> 2.  **Inspecting Parameters**: Access the estimated parameters of the Gaussian components.
>     ```python
>     # Cluster weights (phi)
>     weights = gm.weights_
>     # Example: array([0.39, 0.40, 0.21])
>
>     # Cluster means (mu)
>     means = gm.means_
>     # Example: array([[ 0.05, 0.08], [-1.41, 1.43], [ 3.40, 1.06]])
>
>     # Cluster covariances (Sigma)
>     covariances = gm.covariances_
>     # Example: array([[[ 0.69, 0.80], [ 0.80, 1.21]], ... ]) # k matrices
>     ```
>
> 3.  **Cluster Assignments**: Use `predict()` for hard assignments (which Gaussian component is most likely for each instance) or `predict_proba()` for soft assignments (the probability of belonging to each component).
>     ```python
>     # Hard clustering: Assign each point to the most likely component
>     hard_labels = gm.predict(X)
>     # Example: array([0, 0, 1, ..., 2, 2, 2])
>
>     # Soft clustering: Get probabilities for each point belonging to each component
>     soft_probabilities = gm.predict_proba(X)
>     # Example: array([[0.977, 0.   , 0.023], [0.983, 0.001, 0.016], ...])
>     ```
>
> 4.  **Generating New Samples**: Use `sample()` to draw new data points from the learned mixture model.
>     ```python
>     # X_new, y_new = gm.sample(6) # Generate 6 new samples
>     ```
>
> 5.  **Density Estimation**: Use `score_samples()` to get the log probability density of the model at given points (higher score means higher density region).
>     ```python
>     # log_pdf_values = gm.score_samples(X)
>     # Example: array([-2.61, -3.57, -3.33, ...])
>     ```
> ```image_goes_here
> Caption: Figure 9-16. Visualization of a trained GMM.
> Description: A plot showing data points, the estimated means (centers) of the Gaussian components (clusters), decision boundaries (dashed lines) separating the regions where each component is most likely, and density contour lines for each component, illustrating their ellipsoidal shapes and orientations.
> ```
>
> * **Controlling Shape (`covariance_type`)**: The flexibility of the ellipsoid shapes can be controlled with the `covariance_type` hyperparameter ("full", "tied", "spherical", "diag"), which can help if EM struggles to converge or if you have prior knowledge about cluster shapes.
> ```image_goes_here
> Caption: Figure 9-17. GMM results with constrained covariance types.
> Description: Two plots showing GMM results on the same data but with constraints. Left plot uses `covariance_type="tied"` (all clusters forced to have the same shape/size/orientation). Right plot uses `covariance_type="spherical"` (clusters forced to be spherical but can have different sizes). Shows how constraints affect the model's ability to fit the data.
> ```