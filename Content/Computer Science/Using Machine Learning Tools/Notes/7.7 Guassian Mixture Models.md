> [!motivation] Beyond Spheres: Modeling Complex Clusters
>
> We saw that K-Means struggles when clusters aren't roughly spherical or when they have varying sizes and densities (recall the difficulty with ellipsoidal blobs shown conceptually in Figure 9-11). This limitation arises because K-Means assigns points based solely on distance to a central point.
>
> Gaussian Mixture Models (GMMs) offer a more flexible, probabilistic approach. They assume data points originate from a combination of several Gaussian distributions, each potentially representing an ellipsoid-shaped cluster with its own size, shape, orientation, and density. This makes GMMs better suited for modeling more complex cluster structures.

> [!idea] Gaussian Mixture Models (GMMs): A Probabilistic Approach
>
> Instead of assigning each point to a single cluster definitively, GMMs take a probabilistic view. Imagine the data was created by several different "source" processes, each following a Gaussian (bell-curve like) distribution that produces typically ellipsoidal clouds of points. GMM assumes your observed data is a *mixture* of points drawn from these underlying, hidden Gaussian sources.
>
> The goal of the GMM algorithm is to figure out the properties of these source Gaussians based on the observed data:
> * **Where is each source centered?** (Its mean, $\mu$)
> * **What is its shape, size, and orientation?** (Its covariance matrix, $\Sigma$)
> * **What proportion of data came from each source?** (Its weight, $\phi$)
>
> To estimate these parameters ($\mu$'s, $\Sigma$'s, $\phi$'s) for a predefined number ($k$) of Gaussian components, GMMs typically use the **Expectation-Maximization (EM)** algorithm.
> * **Similarities to K-Means**: EM is iterative, like K-Means. It starts with random guesses for the parameters and alternates between two steps until convergence: an "Expectation" step (like assigning points) and a "Maximization" step (like updating parameters).
> * **Key Difference (Soft Assignments)**: Unlike K-Means' hard assignments, the EM **Expectation** step calculates the *probability* (called responsibility) that each data point belongs to each of the $k$ Gaussian components. The **Maximization** step then updates each Gaussian's parameters ($\mu, \Sigma, \phi$) using *all* data points, weighted by their calculated responsibilities for that Gaussian. This means points can partially "belong" to multiple clusters during learning.
> * **Generative Nature**: Because GMMs model the underlying probability distributions, they are generative models â€“ you can ask the fitted model to generate *new* data points that resemble the original data.

> [!example] GMM Implementation with Scikit-Learn
>
> Scikit-Learn's `GaussianMixture` class implements GMMs.
>
> 1.  **Initialization and Fitting**: Import `GaussianMixture`, specify the number of components ($k$, via `n_components`), and set `n_init` (similar to K-Means, running multiple initializations helps avoid poor local optima, default is 1, so set higher e.g., 10). Then, `fit` the model to the data `X`.
>     ```python
>     from sklearn.mixture import GaussianMixture
>     import numpy as np
>
>     # Assuming X is your data
>     k = 3 # Number of Gaussian components (clusters)
>     gm = GaussianMixture(n_components=k, n_init=10, random_state=42)
>     gm.fit(X)
>
>     # Check convergence status
>     # print(f"Converged: {gm.converged_}, Iterations: {gm.n_iter_}")
>     ```
>
> 2.  **Inspecting Parameters**: Access the estimated parameters of the Gaussian components.
>     ```python
>     # Cluster weights (phi)
>     weights = gm.weights_
>     # Example: array([0.39, 0.40, 0.21])
>
>     # Cluster means (mu)
>     means = gm.means_
>     # Example: array([[ 0.05, 0.08], [-1.41, 1.43], [ 3.40, 1.06]])
>
>     # Cluster covariances (Sigma)
>     covariances = gm.covariances_
>     # Example: array([[[ 0.69, 0.80], [ 0.80, 1.21]], ... ]) # k matrices
>     ```
>
> 3.  **Cluster Assignments**: Use `predict()` for hard assignments (which Gaussian component is most likely for each instance) or `predict_proba()` for soft assignments (the probability of belonging to each component).
>     ```python
>     # Hard clustering: Assign each point to the most likely component
>     hard_labels = gm.predict(X)
>     # Example: array([0, 0, 1, ..., 2, 2, 2])
>
>     # Soft clustering: Get probabilities for each point belonging to each component
>     soft_probabilities = gm.predict_proba(X)
>     # Example: array([[0.977, 0.   , 0.023], [0.983, 0.001, 0.016], ...])
>     ```
>
> 4.  **Generating New Samples**: Use `sample()` to draw new data points from the learned mixture model.
>     ```python
>     # X_new, y_new = gm.sample(6) # Generate 6 new samples
>     ```
>
> 5.  **Density Estimation**: Use `score_samples()` to get the log probability density of the model at given points (higher score means higher density region).
>     ```python
>     # log_pdf_values = gm.score_samples(X)
>     # Example: array([-2.61, -3.57, -3.33, ...])
>     ```
> ```image_goes_here
> Caption: Figure 9-16. Visualization of a trained GMM.
> Description: A plot showing data points, the estimated means (centers) of the Gaussian components (clusters), decision boundaries (dashed lines) separating the regions where each component is most likely, and density contour lines for each component, illustrating their ellipsoidal shapes and orientations.
> ```
>
> * **Controlling Shape (`covariance_type`)**: The flexibility of the ellipsoid shapes can be controlled with the `covariance_type` hyperparameter ("full", "tied", "spherical", "diag"), which can help if EM struggles to converge or if you have prior knowledge about cluster shapes.
> ```image_goes_here
> Caption: Figure 9-17. GMM results with constrained covariance types.
> Description: Two plots showing GMM results on the same data but with constraints. Left plot uses `covariance_type="tied"` (all clusters forced to have the same shape/size/orientation). Right plot uses `covariance_type="spherical"` (clusters forced to be spherical but can have different sizes). Shows how constraints affect the model's ability to fit the data.
> ```