> [!motivation] Beyond Spheres: Modeling Complex Clusters
>
> We saw that K-Means struggles when clusters aren't roughly spherical or when they have varying sizes and densities. This limitation arises because K-Means assigns points based solely on distance to a central point.
>
> Gaussian Mixture Models (GMMs) offer a more flexible, probabilistic approach. They assume data points originate from a combination of several Gaussian distributions, each potentially representing an ellipsoid-shaped cluster with its own size, shape, orientation, and density. This makes GMMs better suited for modeling more complex cluster structures.

> [!idea] GMM vs K-Means: The Key Difference - Soft vs. Hard Clustering
>
> Both K-Means and Gaussian Mixture Models (GMMs) are used for clustering data, but they approach it differently, leading to distinct outcomes.
>
> * **K-Means Review**: Assigns each data point strictly to the *one* cluster whose center (centroid) is closest. It assumes clusters are generally round (spherical). This is called **hard clustering**.
>
> * **GMM Approach**: Also groups data into a set number ($k$) of clusters, but GMM is designed to handle clusters that might be **elliptical (oval-shaped, stretched, or rotated)** and potentially **overlap**.
>
> * **The Fundamental Difference**: Instead of forcing each point into a single cluster, GMM calculates the **probability** that a point belongs to *each* cluster. For example, it might determine a point has a 70% chance of belonging to cluster 1 and a 30% chance of belonging to cluster 2. This is called **soft clustering**.
>
> **Why is this difference important?**
> The probabilistic ("soft") assignment makes GMM more flexible:
> 1.  It can better model clusters that aren't perfectly round.
> 2.  It provides a measure of certainty or uncertainty about cluster membership, especially for points near boundaries or in overlapping regions.
>
> So, while K-Means gives a definite answer ("Point X is in Cluster A"), GMM gives a probabilistic answer ("Point X is *most likely* in Cluster A, but also has some probability of being in Cluster B").

> [!consider] How GMMs Model Shapes and Probabilities
>
> We know GMMs handle non-spherical (ellipsoidal) clusters and assign probabilities, unlike K-Means. How does it manage this? The key lies in how it defines and learns the clusters.
>
> 1.  **Modeling Cluster Shape (Beyond Just Centers)**:
>     * K-Means only keeps track of each cluster's center point (the mean, $\mu$). The cluster is implicitly assumed to be spherical around that center.
>     * GMM tracks *more* information for each cluster (component). Besides the center ($\mu$), it also learns a **covariance matrix** ($\Sigma$). Think of $\Sigma$ as a "blueprint" describing the **shape, size, and orientation** of the ellipsoid associated with that cluster. It captures whether the cluster is stretched, compressed, or tilted relative to the feature axes.
>
> 2.  **Calculating Probabilities (Beyond Just Distance)**:
>     * K-Means assigns points based *only* on which center ($\mu$) is closest (Euclidean distance).
>     * GMM calculates the probability of a point belonging to a cluster based on how well that point fits the *entire Gaussian distribution* defined by both its center ($\mu$) *and* its shape/orientation ($\Sigma$). A point might be slightly farther from the center of ellipsoid A than ellipsoid B, but if it fits better *along the shape* of ellipsoid A, GMM might assign it a higher probability for A. This calculation uses the Gaussian probability density function (PDF), which mathematically describes the likelihood based on the ellipsoid's parameters.
>
> 3.  **Learning with Expectation-Maximization (EM)**:
>     * GMM uses the iterative **EM algorithm** to find the best parameters ($\mu$'s, $\Sigma$'s, and weights $\phi$'s).
>     * **E-step (Expectation)**: Using the current estimates for the ellipsoids, calculate the probability ("responsibility") of each data point belonging to each ellipsoid.
>     * **M-step (Maximization)**: Update each ellipsoid's parameters ($\mu, \Sigma, \phi$). This update uses a *weighted average* of all data points, where the weights are the probabilities calculated in the E-step. This weighting mechanism allows the ellipsoids to stretch, rotate, and resize to better fit the points they are most likely responsible for, adapting to non-spherical shapes.
>
> By explicitly modeling the shape ($\Sigma$) and using this probabilistic, weighted update process (EM), GMM can naturally learn non-spherical cluster structures and provide meaningful soft assignments (probabilities) where K-Means cannot.

> [!example] GMM Implementation with Scikit-Learn
>
> Scikit-Learn's `GaussianMixture` class implements GMMs.
>
> 1.  **Initialization and Fitting**: Import `GaussianMixture`, specify the number of components ($k$, via `n_components`), and set `n_init` (similar to K-Means, running multiple initializations helps avoid poor local optima, default is 1, so set higher e.g., 10). Then, `fit` the model to the data `X`.
>     ```python
>     from sklearn.mixture import GaussianMixture
>     import numpy as np
>
>     # Assuming X is your data
>     k = 3 # Number of Gaussian components (clusters)
>     gm = GaussianMixture(n_components=k, n_init=10, random_state=42)
>     gm.fit(X)
>
>     # Check convergence status
>     # print(f"Converged: {gm.converged_}, Iterations: {gm.n_iter_}")
>     ```
>
> 2.  **Inspecting Parameters**: Access the estimated parameters of the Gaussian components.
>     ```python
>     # Cluster weights (phi)
>     weights = gm.weights_
>     # Example: array([0.39, 0.40, 0.21])
>
>     # Cluster means (mu)
>     means = gm.means_
>     # Example: array([[ 0.05, 0.08], [-1.41, 1.43], [ 3.40, 1.06]])
>
>     # Cluster covariances (Sigma) - define shape/orientation
>     covariances = gm.covariances_
>     # Example: array([[[ 0.69, 0.80], [ 0.80, 1.21]], ... ]) # k matrices
>     ```
>
> 3.  **Cluster Assignments**: Use `predict()` for hard assignments (which Gaussian component is most likely for each instance) or `predict_proba()` for soft assignments (the probability of belonging to each component).
>     ```python
>     # Hard clustering: Assign each point to the most likely component
>     hard_labels = gm.predict(X)
>     # Example: array([0, 0, 1, ..., 2, 2, 2])
>
>     # Soft clustering: Get probabilities for each point belonging to each component
>     soft_probabilities = gm.predict_proba(X)
>     # Example: array([[0.977, 0.   , 0.023], [0.983, 0.001, 0.016], ...])
>     ```
>
> 4.  **Generating New Samples**: Use `sample()` to draw new data points from the learned mixture model.
>     ```python
>     # X_new, y_new = gm.sample(6) # Generate 6 new samples
>     ```
>
> 5.  **Density Estimation**: Use `score_samples()` to get the log probability density of the model at given points (higher score means higher density region).
>     ```python
>     # log_pdf_values = gm.score_samples(X)
>     # Example: array([-2.61, -3.57, -3.33, ...])
>     ```
> ```image_goes_here
> Caption: Figure 9-16. Visualization of a trained GMM.
> Description: A plot showing data points, the estimated means (centers) of the Gaussian components (clusters), decision boundaries (dashed lines) separating the regions where each component is most likely, and density contour lines for each component, illustrating their ellipsoidal shapes and orientations.
> ```
>
> * **Controlling Shape (`covariance_type`)**: The flexibility of the ellipsoid shapes can be constrained using the `covariance_type` hyperparameter ("full", "tied", "spherical", "diag"), useful if EM struggles or based on prior knowledge.
> ```image_goes_here
> Caption: Figure 9-17. GMM results with constrained covariance types.
> Description: Two plots showing GMM results on the same data but with constraints. Left plot uses `covariance_type="tied"` (all clusters forced to have the same shape/size/orientation). Right plot uses `covariance_type="spherical"` (clusters forced to be spherical but can have different sizes). Shows how constraints affect the model's ability to fit the data.
> ```