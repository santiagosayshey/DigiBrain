> [!motivation] Beyond Spheres: Modeling Complex Clusters
>
> We saw that K-Means struggles when clusters aren't roughly spherical or when they have varying sizes and densities. This limitation arises because K-Means assigns points based solely on distance to a central point.
>
> Gaussian Mixture Models (GMMs) offer a more flexible, probabilistic approach. They assume data points originate from a combination of several Gaussian distributions, each potentially representing an ellipsoid-shaped cluster with its own size, shape, orientation, and density. This makes GMMs better suited for modeling more complex cluster structures.

> [!idea] Gaussian Mixture Models (GMMs): Modeling Flexible Clusters
>
> Often, real-world data clusters aren't perfectly spherical or easily separated. K-Means, which assumes spherical clusters, can struggle with elongated, rotated, or overlapping groups. **Gaussian Mixture Models (GMMs)** offer a more flexible approach by assuming data originates from a *mixture* of several underlying **Gaussian distributions**.
>
> Think of a Gaussian distribution (or Normal distribution) as a way to describe points clustering around a central point (the mean, $\mu$). In multiple dimensions (multiple features), this cluster naturally forms an **ellipsoid** shape. The exact size, stretch, and orientation of this ellipsoid are defined by the distribution's **covariance matrix** ($\Sigma$).
>
> The core idea of GMM is that your observed dataset is a blend of points generated by a specific number ($k$) of these different Gaussian distributions, each with its own unique center ($\mu$), shape/orientation ($\Sigma$), and relative contribution or size (its weight $\phi$). The **goal of fitting a GMM** is to look at your mixed data and estimate the parameters ($\mu$'s, $\Sigma$'s, $\phi$'s) of these $k$ underlying Gaussian components that best explain the data you see.
>
> GMMs typically use the **Expectation-Maximization (EM)** algorithm to find these parameters. While iterative like K-Means, EM works differently:
> 1.  It starts with initial guesses for the $k$ Gaussian components (their $\mu, \Sigma, \phi$).
> 2.  **E-step (Expectation):** Instead of making a hard assignment, it calculates the *probability* for each data point that it belongs to each of the $k$ components based on the current guesses. A point lying where two ellipsoids overlap might have, say, a 60% chance of belonging to component 1 and a 40% chance of belonging to component 2. These probabilities are called 'responsibilities'.
> 3.  **M-step (Maximization):** It then updates the parameters ($\mu, \Sigma, \phi$) for each component. This update considers *all* data points, but weights the influence of each point based on the responsibilities calculated in the E-step. Components are essentially pulled towards the points they are most responsible for.
> 4.  Steps 2 and 3 repeat until the component parameters stabilize.
>
> This process allows GMMs to model complex, overlapping, ellipsoidal clusters more effectively than K-Means. As a result, they provide **soft clustering** (probabilities of membership) and, because they model the underlying data generation process, can also be used to generate new data points similar to the original data (they are **generative models**).

> [!example] GMM Implementation with Scikit-Learn
>
> Scikit-Learn's `GaussianMixture` class implements GMMs.
>
> 1.  **Initialization and Fitting**: Import `GaussianMixture`, specify the number of components ($k$, via `n_components`), and set `n_init` (similar to K-Means, running multiple initializations helps avoid poor local optima, default is 1, so set higher e.g., 10). Then, `fit` the model to the data `X`.
>     ```python
>     from sklearn.mixture import GaussianMixture
>     import numpy as np
>
>     # Assuming X is your data
>     k = 3 # Number of Gaussian components (clusters)
>     gm = GaussianMixture(n_components=k, n_init=10, random_state=42)
>     gm.fit(X)
>
>     # Check convergence status
>     # print(f"Converged: {gm.converged_}, Iterations: {gm.n_iter_}")
>     ```
>
> 2.  **Inspecting Parameters**: Access the estimated parameters of the Gaussian components.
>     ```python
>     # Cluster weights (phi)
>     weights = gm.weights_
>     # Example: array([0.39, 0.40, 0.21])
>
>     # Cluster means (mu)
>     means = gm.means_
>     # Example: array([[ 0.05, 0.08], [-1.41, 1.43], [ 3.40, 1.06]])
>
>     # Cluster covariances (Sigma)
>     covariances = gm.covariances_
>     # Example: array([[[ 0.69, 0.80], [ 0.80, 1.21]], ... ]) # k matrices
>     ```
>
> 3.  **Cluster Assignments**: Use `predict()` for hard assignments (which Gaussian component is most likely for each instance) or `predict_proba()` for soft assignments (the probability of belonging to each component).
>     ```python
>     # Hard clustering: Assign each point to the most likely component
>     hard_labels = gm.predict(X)
>     # Example: array([0, 0, 1, ..., 2, 2, 2])
>
>     # Soft clustering: Get probabilities for each point belonging to each component
>     soft_probabilities = gm.predict_proba(X)
>     # Example: array([[0.977, 0.   , 0.023], [0.983, 0.001, 0.016], ...])
>     ```
>
> 4.  **Generating New Samples**: Use `sample()` to draw new data points from the learned mixture model.
>     ```python
>     # X_new, y_new = gm.sample(6) # Generate 6 new samples
>     ```
>
> 5.  **Density Estimation**: Use `score_samples()` to get the log probability density of the model at given points (higher score means higher density region).
>     ```python
>     # log_pdf_values = gm.score_samples(X)
>     # Example: array([-2.61, -3.57, -3.33, ...])
>     ```
> ```image_goes_here
> Caption: Figure 9-16. Visualization of a trained GMM.
> Description: A plot showing data points, the estimated means (centers) of the Gaussian components (clusters), decision boundaries (dashed lines) separating the regions where each component is most likely, and density contour lines for each component, illustrating their ellipsoidal shapes and orientations.
> ```
>
> * **Controlling Shape (`covariance_type`)**: The flexibility of the ellipsoid shapes can be controlled with the `covariance_type` hyperparameter ("full", "tied", "spherical", "diag"), which can help if EM struggles to converge or if you have prior knowledge about cluster shapes.
> ```image_goes_here
> Caption: Figure 9-17. GMM results with constrained covariance types.
> Description: Two plots showing GMM results on the same data but with constraints. Left plot uses `covariance_type="tied"` (all clusters forced to have the same shape/size/orientation). Right plot uses `covariance_type="spherical"` (clusters forced to be spherical but can have different sizes). Shows how constraints affect the model's ability to fit the data.
> ```