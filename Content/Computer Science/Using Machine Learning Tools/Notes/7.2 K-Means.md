> [!motivation] The Need for Grouping Unlabeled Data
> 
> Humans intuitively group similar objects, like recognizing plants of the same species, even without formal botanical knowledge. This natural ability highlights a common data analysis need.
> 
> - Consider datasets like the Iris dataset shown below. When labels (species) are present (left panel), classification algorithms can assign instances to predefined categories.
> - However, much data lacks these labels (right panel). In such cases, classification methods cannot be applied.
> - The challenge arises: how can we automatically discover inherent groupings within unlabeled data?
> 
> ![[Pasted image 20250428125444.png|500]]
> 
> This necessitates methods that can identify structure without supervision.

> [!idea] Clustering
> 
> Clustering is an unsupervised learning task focused on **identifying similar instances within a dataset and assigning them to groups, known as clusters**. Unlike classification, it does not use predefined labels.
> 
> - **Objective**: To partition data such that instances within a cluster are similar to each other, and instances in different clusters are dissimilar.
> - **Algorithm Variability**: Different clustering algorithms define "clusters" differently. Some seek dense regions (DBSCAN), others group around central points (K-Means), and some build hierarchies of clusters.
> - **Feature Utilization**: Algorithms can leverage multiple features, potentially revealing clusters not obvious in lower-dimensional views (like the Iris dataset example).
> 
> Clustering has diverse applications:
> 
> - Customer segmentation for targeted marketing.
> - Exploratory data analysis to understand dataset structure.
> - Dimensionality reduction via cluster affinity vectors.
> - Feature engineering by adding cluster affinity as features.
> - Anomaly detection (outliers often have low affinity to all clusters).
> - Semi-supervised learning by propagating labels within clusters.
> - Search engines for finding similar items (e.g., images).
> - Image segmentation by grouping pixels of similar color.

> [!idea] K-Means Algorithm
> 
> K-Means is an iterative algorithm designed to **partition a dataset into a predefined number (k) of distinct, non-overlapping clusters.** It is particularly effective for identifying "blob-like" groups of instances.
> 
> ![[Pasted image 20250428132156.png|300]]
> 
> - **Objective**: Minimize the variance within each cluster. It achieves this by finding k central points (centroids) and assigning each data point to the cluster associated with the nearest centroid.
> - **Requirement**: The number of clusters, k, must be specified by the user before running the algorithm.
> - **Outcome**: The process results in assigning a cluster index (0 to kâˆ’1) to each instance. The decision boundaries between clusters form a Voronoi tessellation around the final centroids/
> 
> 
> 
> ![[Pasted image 20250428132258.png|300]]

> [!example] K-Means Implementation (Scikit-Learn)
>
> K-Means in Scikit-Learn helps group unlabeled data. You tell it how many groups ($k$) to find, and it assigns each data point to one group.
>
> ```python
> from sklearn.cluster import KMeans
> from sklearn.datasets import make_blobs
> import numpy as np
>
> # Example data (X has features, e.g., petal length/width for flowers)
> # X, y = make_blobs([...]) # y (true groups) is not used by KMeans
>
> k = 3 # Let's say we want to find 3 groups (clusters)
> kmeans = KMeans(n_clusters=k, random_state=42)
>
> # Run K-Means: finds centers and assigns each point in X to a cluster
> kmeans.fit(X)
> ```
>
> After fitting, you can inspect the results stored in the `kmeans` object:
>
> * The `labels_` attribute shows the cluster assignment for each data point used during training:
> ```python
> # Get the cluster assignments (a NumPy array):
> assigned_clusters = kmeans.labels_
> # Example: array([0, 0, 0, ..., 1, 1, 1, ..., 2, 2, 2], dtype=int32)
> ```
> This array tells you which cluster ID (e.g., 0, 1, or 2 if $k=3$) was assigned to the first data point in `X`, the second, the third, and so on. It reveals the actual grouping discovered by the algorithm in your training data. Conceptually, if you plotted the data, all points with label '0' would form one visual group, label '1' another, etc.
>
> * The `cluster_centers_` attribute stores the coordinates calculated for the center point of each cluster:
> ```python
> # Get the coordinates of the cluster centers (NumPy array):
> group_centers = kmeans.cluster_centers_
> # Example: array([ [center0_feat1, center0_feat2],
> #                  [center1_feat1, center1_feat2],
> #                  [center2_feat1, center2_feat2] ])
> ```
> These coordinates (one row per cluster) define the final calculated center for each group (cluster 0, cluster 1, cluster 2, etc.) based on the features of the data points assigned to them.
>
> * The `predict` method assigns new, unseen data points to the closest learned cluster centroid:
> ```python
> # Example new data points
> X_new = np.array([[petal_len1, petal_wid1], [petal_len2, petal_wid2]])
> # Find the cluster ID for each new point:
> predicted_groups = kmeans.predict(X_new)
> # Example: array([0, 2], dtype=int32)
> ```
> This returns an array showing the predicted cluster ID (e.g., 0, 1, or 2) for each of the new data points you provided, based on which existing cluster center they are nearest to. It allows you to classify new data using the discovered groups.

> [!consider] Hard vs. Soft Clustering and Feature Transformation
>
> K-Means, using the `predict` method, performs **hard clustering**: each instance is assigned definitively to a single cluster (the one with the closest centroid).
>
> An alternative approach is **soft clustering**, where instead of a single assignment, each instance receives a score indicating its relationship (e.g., distance or similarity/affinity) to *each* cluster. This can be useful when instances might be close to multiple cluster boundaries.
>
> * The `transform()` method in Scikit-Learn's `KMeans` provides a form of soft scoring by calculating the distance from each input instance to every cluster centroid found during training.
>
> ```python
> # Assuming 'kmeans' is a fitted KMeans object and X_new are new data points
> # Calculate distance from each point in X_new to each of the k centroids:
> distances_to_centroids = kmeans.transform(X_new)
> # Example Output (rounded):
> # array([[2.81, 0.33, 2.9 , 1.49, 2.89],
> #        [5.81, 2.8 , 5.85, 4.48, 5.84],
> #        [1.21, 3.29, 0.29, 1.69, 1.71],
> #        [0.73, 3.22, 0.36, 1.55, 1.22]])
> ```
> In the example output above, the first row corresponds to the first instance in `X_new`. The values `[2.81, 0.33, 2.9 , 1.49, 2.89]` are its distances to centroid 0, centroid 1, centroid 2, centroid 3, and centroid 4 respectively. The second row shows the distances for the second instance in `X_new`, and so on.
>
> * **Use Cases for Transformation**:
>     * **Non-linear Dimensionality Reduction**: Transforming an $N$-dimensional dataset using `transform()` results in a $k$-dimensional dataset (where $k$ is the number of clusters), with each dimension representing the distance to a centroid. This can capture non-linear structures.
>     * **Feature Engineering**: These distances can serve as informative new features for subsequent supervised learning models.

> [!idea] How K-Means Works: An Iterative Approach
>
> K-Means tackles a "chicken-and-egg" problem: to find the center of a cluster (centroid), you need to know which points belong to it; but to assign points to a cluster, you need to know where its center is. Since we start with neither, K-Means uses an iterative process:
>
> 1.  **Initialize Centroids**: Start by guessing the initial positions for the $k$ cluster centroids. A common way is to pick $k$ data points randomly from the dataset and use their locations as the initial centroids. (See image below, top-left).
>
> 2.  **Assign Instances (Labeling)**: Assign each data point in the dataset to its *nearest* currently positioned centroid. This forms temporary clusters. (See image below, top-right).
>
> 3.  **Update Centroids**: Recalculate the position of each of the $k$ centroids. The new position for a centroid is the mean (average location) of all the data points that were assigned to it in the previous step. (See image below, center-left).
>
> 4.  **Repeat**: Keep repeating Steps 2 (re-assign points to the *new* nearest centroid) and 3 (update centroid positions based on the *new* assignments) iteratively. (See subsequent panels in image below).
>
> ![[Pasted image 20250428135207.png|600]]
>
> The algorithm stops when the centroids essentially stop moving between iterations (or after a maximum number of iterations), meaning the assignments have stabilized. This process is guaranteed to converge, typically quite quickly.


> [!consider] K-Means Initialization Sensitivity and Improvements
>
> While K-Means convergence is guaranteed, it settles in a *local* optimum, heavily influenced by the initial random centroid placement. This means a single run might yield a suboptimal clustering if the start was "unlucky."
>
> ![[Pasted image 20250428135900.png|500]]
>
> Several strategies and algorithm variants address this sensitivity and improve performance:
>
> * **Multiple Initializations (`n_init`)**: Run the K-Means algorithm multiple times (Scikit-Learn default `n_init=10`) with different random starting centroids. The run resulting in the lowest **inertia** is chosen as the final result.
>     * **Inertia**: The sum of squared distances between each instance and its closest centroid. Lower inertia generally indicates a better clustering (denser, more compact clusters). It's accessible via the `inertia_` attribute. (The `score()` method returns negative inertia).
>
> * **K-Means++ Initialization**: This is the default initialization method in Scikit-Learn's `KMeans`. It's a "smarter" way to choose initial centroids:
>     1.  The first centroid is chosen randomly.
>     2.  Subsequent centroids are chosen from the remaining data points with a probability proportional to their squared distance from the *nearest* already-chosen centroid.
>     *This strategy tends to place initial centroids far apart, significantly reducing the chance of converging to a poor local optimum and often requiring fewer `n_init` runs.*
>
> * **Manual Initialization (`init`)**: If you have prior knowledge of good approximate centroid locations, you can provide them as a NumPy array to the `init` parameter and set `n_init=1`.
>     ```python
>     # Example: Providing specific starting points if known
>     # good_init = np.array([[-3, 3], [-3, 2], ...])
>     # kmeans = KMeans(n_clusters=k, init=good_init, n_init=1)
>     ```
>
> * **Elkan's Algorithm (Acceleration)**: A variant (`algorithm="elkan"`) that can sometimes speed up K-Means on datasets with clear cluster structures by using the triangle inequality to avoid redundant distance calculations. Its effectiveness varies depending on the dataset.
>
> * **Mini-Batch K-Means**: Uses small, random subsets (mini-batches) of the data at each iteration instead of the full dataset.
>     * **Pros**: Significantly faster (often 3-4x), capable of handling datasets too large to fit in memory (using `partial_fit` for incremental learning). Implemented in `MiniBatchKMeans` class.
>     * **Cons**: Generally results in slightly higher inertia (slightly less optimal clustering) compared to standard K-Means.
>
> ![[Pasted image 20250428140537.png|300]]
> Choosing the right approach depends on dataset size, desired speed, and tolerance for potentially slightly suboptimal results versus computational cost.
