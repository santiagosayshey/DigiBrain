> [!motivation] Need for Faster Optimizers
>
> **Training very large deep neural networks can be a time-consuming process**. While strategies like careful weight initialization, appropriate activation functions, Batch Normalization, and transfer learning significantly speed up training and improve outcomes, further enhancements are often needed.
> - Another major avenue for accelerating training and potentially finding better solutions lies in using optimization algorithms that are more advanced than the standard Gradient Descent optimizer.

> [!idea] Momentum Optimizer
>
> Momentum optimization accelerates training by drawing an analogy to a physical system, like a bowling ball rolling down a hill: it accumulates momentum, allowing it to move faster and overcome obstacles.
> - **Core Concept:** Unlike standard Gradient Descent which only considers the current gradient to determine its step, momentum optimization incorporates information from past gradients.
>   - It maintains a **momentum vector (`m`)** which accumulates a fraction of the past gradients.
>   - At each iteration, the current gradient (scaled by the learning rate) contributes to this momentum vector, effectively acting as an **acceleration**.
>   - The weights are then updated by adding this momentum vector.
>   - A **momentum hyperparameter (`β`)** (typically around 0.9) controls the "friction" in the system. A value of 0 means high friction (no momentum), while 1 means no friction.
>
> - **Benefits:**
>   - **Faster Convergence:** Generally reaches the minimum much faster than standard Gradient Descent, especially on surfaces with gentle slopes or plateaus where it can "roll" through.
>   - **Navigates Elongated Valleys:** More effective at finding the optimum in cost functions shaped like elongated bowls (common when input features have very different scales and Batch Normalization isn't used).
>   - **Helps Overcome Local Optima:** The accumulated momentum can help the optimizer roll past small local optima.
>
> - **Implementation in Keras:**
>   - Easily used by setting the `momentum` hyperparameter in the `SGD` optimizer:
>     `optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)`
>
> - **Consideration:** While very effective, the momentum can cause the optimizer to overshoot the minimum and oscillate. The `β` hyperparameter helps dampen these oscillations. It also adds one more hyperparameter to tune, though `β=0.9` often works well.

> [!idea] Nesterov Accelerated Gradient
>
> Nesterov Accelerated Gradient (NAG), also known as Nesterov Momentum, is a modification of momentum optimization that often yields faster convergence and better performance.
> - **Core Concept:**
>   - Standard momentum calculates the gradient at the current position (`θ`) and then takes a step in the direction of the updated momentum vector.
>   - NAG introduces a "look-ahead" aspect: it first makes a step in the direction of the current accumulated momentum (`θ + βm`, where `m` is the momentum vector and `β` is the momentum hyperparameter). It then calculates the gradient at this new, anticipated position and uses that gradient to correct the momentum vector.
>   - The intuition is that since the momentum vector is likely pointing towards the optimum, evaluating the gradient slightly ahead in that direction provides a more accurate adjustment.
>
> - **Benefits:**
>   - **Faster Convergence:** NAG typically converges faster than standard momentum optimization.
>   - **Reduced Oscillations:** By calculating the gradient "ahead," NAG can anticipate overshooting across a valley and apply a correction sooner, leading to reduced oscillations and more direct convergence.
>
> - **Implementation in Keras:**
>   - Used by setting the `nesterov=True` argument in the `SGD` optimizer, along with `momentum`:
>     `optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)`