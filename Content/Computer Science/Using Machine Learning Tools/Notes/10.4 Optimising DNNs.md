> [!motivation] Need for Faster Optimizers
>
> **Training very large deep neural networks can be a time-consuming process**. While strategies like careful weight initialization, appropriate activation functions, Batch Normalization, and transfer learning significantly speed up training and improve outcomes, further enhancements are often needed.
> - Another major avenue for accelerating training and potentially finding better solutions lies in using optimization algorithms that are more advanced than the standard Gradient Descent optimizer.

> [!idea] Momentum Optimizer
>
> Momentum optimization accelerates training by drawing an analogy to a physical system, like a bowling ball rolling down a hill: it accumulates momentum, allowing it to move faster and overcome obstacles.
> - **Core Concept:** Unlike standard Gradient Descent which only considers the current gradient to determine its step, momentum optimization incorporates information from past gradients.
>   - It maintains a **momentum vector (`m`)** which accumulates a fraction of the past gradients.
>   - At each iteration, the current gradient (scaled by the learning rate) contributes to this momentum vector, effectively acting as an **acceleration**.
>   - The weights are then updated by adding this momentum vector.
>   - A **momentum hyperparameter (`β`)** (typically around 0.9) controls the "friction" in the system. A value of 0 means high friction (no momentum), while 1 means no friction.
>
> - **Benefits:**
>   - **Faster Convergence:** Generally reaches the minimum much faster than standard Gradient Descent, especially on surfaces with gentle slopes or plateaus where it can "roll" through.
>   - **Navigates Elongated Valleys:** More effective at finding the optimum in cost functions shaped like elongated bowls (common when input features have very different scales and Batch Normalization isn't used).
>   - **Helps Overcome Local Optima:** The accumulated momentum can help the optimizer roll past small local optima.
>
> - **Implementation in Keras:**
>   - Easily used by setting the `momentum` hyperparameter in the `SGD` optimizer:
>     `optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)`
>
> - **Consideration:** While very effective, the momentum can cause the optimizer to overshoot the minimum and oscillate. The `β` hyperparameter helps dampen these oscillations. It also adds one more hyperparameter to tune, though `β=0.9` often works well.

> [!idea] Nesterov Accelerated Gradient
>
> Nesterov Accelerated Gradient (NAG), also known as Nesterov Momentum, is a modification of momentum optimization that often yields faster convergence and better performance.
> - **Core Concept:**
>   - Standard momentum calculates the gradient at the current position (`θ`) and then takes a step in the direction of the updated momentum vector.
>   - NAG introduces a "look-ahead" aspect: it first makes a step in the direction of the current accumulated momentum (`θ + βm`, where `m` is the momentum vector and `β` is the momentum hyperparameter). It then calculates the gradient at this new, anticipated position and uses that gradient to correct the momentum vector.
>   - The intuition is that since the momentum vector is likely pointing towards the optimum, evaluating the gradient slightly ahead in that direction provides a more accurate adjustment.
>
> - **Benefits:**
>   - **Faster Convergence:** NAG typically converges faster than standard momentum optimization.
>   - **Reduced Oscillations:** By calculating the gradient "ahead," NAG can anticipate overshooting across a valley and apply a correction sooner, leading to reduced oscillations and more direct convergence.
>
> - **Implementation in Keras:**
>   - Used by setting the `nesterov=True` argument in the `SGD` optimizer, along with `momentum`:
>     `optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)`

> [!idea] AdaGrad
>
> AdaGrad (Adaptive Gradient Algorithm) is an optimizer that adapts the learning rate for each parameter individually, performing larger updates for parameters associated with infrequent features and smaller updates for parameters associated with frequent features.
> - **Core Concept & Mechanism:**
>   - AdaGrad adjusts the learning rate on a per-parameter basis by accumulating the sum of squares of past gradients for each parameter.
>   - Let `s` be a vector storing this sum of squared gradients for all parameters `θ`. At each iteration, `s` is updated by adding the square of the current gradient: `s ← s + ∇J(θ) ⊗ ∇J(θ)`.
>   - The learning rate for each parameter `θᵢ` is then effectively scaled down by the square root of the corresponding accumulated sum `sᵢ` (plus a small smoothing term `ε` to prevent division by zero). The update rule is approximately: `θ ← θ − η ∇J(θ) ⊘ (√s + ε)`.
>   - This means the learning rate decays more quickly for parameters with consistently large gradients (steep dimensions) and more slowly for parameters with small gradients (gentler dimensions).
>
> - **Benefits:**
>   - **Adaptive Learning Rate:** It automatically adapts the learning rate, which is particularly helpful for problems with sparse features or when features have very different frequencies or scales.
>   - **Less Learning Rate Tuning:** Often requires less manual tuning of the initial global learning rate `η`.
>   - **Improved Direction:** Can help direct the optimization path more accurately towards the optimum in scenarios like elongated cost function valleys.
>
> - **Drawback:**
>   - **Aggressive Learning Rate Decay:** Its primary limitation, especially in deep learning, is that the accumulated sum of squared gradients in the denominator keeps growing throughout training. This can cause the learning rate to become infinitesimally small prematurely, leading to the algorithm effectively stopping learning too early.
>
> Keras implements this optimizer as `tf.keras.optimizers.Adagrad`.

> [!idea] RMSProp
>
> RMSProp (Root Mean Square Propagation) is an adaptive learning rate optimization algorithm designed to address the primary drawback of AdaGrad—its tendency for the learning rate to decay too aggressively and prematurely stop learning.
> - **Core Concept & Mechanism:**
>   - Like AdaGrad, RMSProp adapts the learning rate for each parameter individually.
>   - The key difference is how it accumulates information about past gradients: instead of summing all past squared gradients, RMSProp calculates an **exponentially decaying average** of the squared gradients.
>   - It maintains a moving average `s` of squared gradients, typically updated as: `s ← ρs + (1 − ρ)∇J(θ) ⊗ ∇J(θ)`, where `ρ` (rho) is a decay rate hyperparameter (e.g., 0.9).
>   - The parameter updates are then scaled by dividing the gradient by the square root of this moving average `s` (plus a smoothing term `ε`): `θ ← θ − η ∇J(θ) ⊘ (√s + ε)`.
>
> - **Benefits:**
>   - **Resolves AdaGrad's Issue:** By using an exponentially decaying average, the learning rate is prevented from monotonically decreasing to near zero too quickly. The influence of very old gradients diminishes, allowing the optimizer to continue making progress.
>   - **Adaptive Learning Rate:** It still provides per-parameter learning rate adaptation, beneficial for diverse feature scales.
>   - **Effective Performance:** RMSProp is often a very effective optimizer for a wide range of deep learning tasks.
>
> - **Implementation in Keras:**
>   - Available as `tf.keras.optimizers.RMSprop`.
>     `optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)`

> [!idea] Adam Optimizer & Variants
>
> Adam (Adaptive Moment Estimation) is a highly popular and effective optimization algorithm that combines the key ideas of Momentum optimization and RMSProp.
>
> - **Adam Core Concept:**
>   - **Momentum Component:** It keeps an exponentially decaying average of past gradients (the first moment, `m`), similar to momentum optimization. This helps accelerate progress in relevant directions.
>   - **Adaptive Learning Rate Component:** It also keeps an exponentially decaying average of past *squared* gradients (the second moment, `s`), similar to RMSProp. This adapts the learning rate for each parameter.
>   - **Bias Correction:** Adam includes bias-correction steps for both `m` and `s` to counteract their initialization at zero, especially improving performance in the early stages of training.
>   - **Update Rule:** The final weight update uses these corrected estimates of `m` (mean of gradients) and `s` (uncentered variance of gradients), effectively scaling the learning rate per parameter.
>   - **Benefits:** Often works very well with default hyperparameter values (`β1` typically 0.9 for `m`, `β2` typically 0.999 for `s`), requires less manual tuning of the initial learning rate, and is generally a robust choice for many problems.
>   - **Keras Implementation:** `tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)`
>
> - **Common Adam Variants:**
>   - **AdaMax:** A variant of Adam that uses the ℓ∞ norm (maximum value) instead of the ℓ2 norm (square root of sum of squares) for normalizing the parameter updates based on the second moment `s`. This can sometimes offer improved stability.
>     - **Keras Implementation:** `tf.keras.optimizers.Adamax`
>   - **Nadam:** Combines Adam with the Nesterov Accelerated Gradient (NAG) technique. By incorporating NAG's "look-ahead" gradient calculation, Nadam often converges slightly faster than Adam.
>     - **Keras Implementation:** `tf.keras.optimizers.Nadam`
>   - **AdamW:** Addresses how weight decay is applied with adaptive optimizers like Adam.
>     - **L2 Regularization (Context):** L2 regularization is a common technique to reduce overfitting by adding a penalty to the loss function proportional to the sum of the squares of the model's weights, encouraging smaller weights.
>     - **Weight Decay (Context):** This is another regularization technique where weights are directly reduced by a small factor at each training step. For standard SGD, L2 regularization and weight decay are effectively equivalent.
>     - **AdamW's Contribution:** With Adam, standard L2 regularization doesn't always interact as intended to achieve the effect of weight decay. AdamW modifies Adam to correctly incorporate weight decay, which can lead to better generalization performance.
>     - **Keras Implementation:** Typically found in TensorFlow Addons: `tfa.optimizers.AdamW(weight_decay=..., learning_rate=...)`

> [!consider] Adaptive Optimizer Caveats
>
> While adaptive optimization methods like RMSProp and the Adam family (Adam, AdaMax, Nadam, AdamW) are popular for their fast convergence and ease of use, it's important to be aware of some potential considerations:
> - **Generalization Concerns:** Some research, notably a 2017 paper by Ashia C. Wilson et al., has suggested that adaptive optimizers can sometimes converge to solutions that generalize less effectively to unseen data compared to optimizers like SGD with Nesterov momentum (NAG), despite potentially achieving good performance on the training set.
> - **Dataset Sensitivity:** The performance and generalization capabilities of adaptive optimizers can be dataset-dependent. Some datasets or model architectures might be, as the textbook suggests, "allergic to adaptive gradients."
> - **Alternative to Consider:** If you observe that a model trained with an adaptive optimizer is not generalizing well (e.g., high variance between training and validation performance), it might be beneficial to experiment with Nesterov Accelerated Gradient (NAG) instead.
> - **Evolving Research:** The field of optimization in deep learning is continually evolving. New research papers frequently emerge, offering further insights, comparisons, and novel algorithms. Staying updated with recent findings can be advantageous.

> [!motivation] Value of Dynamic Learning Rates
>
> While selecting an appropriate initial learning rate is crucial, relying on a single, constant learning rate throughout the entire training process is often suboptimal for deep neural networks.
> - **Challenges with a Constant Learning Rate:**
>   - **Too High:** A learning rate that's too large can cause the training process to diverge, with the loss increasing instead of decreasing, or it may oscillate erratically around the optimal solution without converging.
>   - **Too Low:** A learning rate that's too small will lead to very slow training, requiring many epochs to reach a good solution, and potentially getting stuck in suboptimal local minima or plateaus.
>   - **"Just Right" is Still Limited:** Even a well-chosen constant learning rate might make rapid progress initially but then struggle to settle into a fine-grained optimum, or it might be too slow in the early phases if it's set low enough for stable final convergence.
>
> - **The Advantage of Dynamic Adjustment:**
>   - **Learning Rate Schedules** (strategies for changing the learning rate during training) offer a solution. By starting with a relatively larger learning rate, training can progress quickly. Then, by gradually decreasing the learning rate, the optimizer can settle into a more precise minimum, potentially leading to both faster convergence and a better final model.

> [!idea] Common Learning Rate Schedules
>
> Several learning rate scheduling strategies exist to dynamically adjust the learning rate during training, often leading to faster convergence and better model performance. Here are some common approaches:
> - **Power Scheduling:**
>   - The learning rate `η` decreases over iterations `t` according to a formula like `η(t) = η₀ / (1 + t/s)ᶜ`, where `η₀` is the initial rate, `s` controls the decay speed, and `c` is a power (often 1).
>   - This schedule causes the learning rate to drop relatively quickly at the beginning of training and then more slowly as training progresses.
> - **Exponential Scheduling:**
>   - The learning rate decreases by a multiplicative factor (e.g., by a factor of 10) over a set number of steps or epochs. A common form is `η(t) = η₀ * r^(t/s)`, where `r` is the decay rate (e.g., 0.1) and `s` is the number of steps for one decay period.
>   - This results in a learning rate that reduces by a consistent proportion at regular intervals.
> - **Piecewise Constant Scheduling:**
>   - This approach uses a specific constant learning rate for a certain number of epochs, then switches to a different (usually smaller) constant learning rate for another block of epochs, and so on.
>   - The sequence of learning rates and the duration for each are defined manually.
> - **Performance Scheduling:**
>   - The learning rate is reduced automatically when a monitored metric (typically the validation loss or error) stops improving for a specified number of epochs (the "patience" parameter).
>   - For example, the learning rate might be halved if the validation loss doesn't decrease for 5 consecutive epochs.
> - **1cycle Scheduling:**
>   - Introduced by Leslie Smith, this schedule involves two main phases over the course of training:
>     1.  The learning rate starts low (`η₀`), linearly increases to a pre-determined maximum value (`η₁`) about halfway through training.
>     2.  It then linearly decreases back to `η₀` during the second half.
>   - Often, the last few epochs involve a further rapid drop in learning rate.
>   - Momentum is also typically varied inversely: starting high, decreasing as LR increases, then increasing again as LR decreases. This schedule can significantly accelerate training and improve performance ("super-convergence").

> [!summary] Optimizers & LR Recap
>
> Efficiently training deep neural networks often involves selecting appropriate optimization algorithms beyond standard Gradient Descent and employing dynamic learning rate strategies. These tools can significantly speed up convergence and lead to better model performance:
> - **Momentum & Nesterov Accelerated Gradient (NAG)**: These methods accelerate SGD by accumulating a velocity term (momentum) based on past gradients, helping to navigate plateaus and speed through gentle slopes. NAG refines this by calculating the gradient "ahead" in the momentum direction for smarter updates.
> - **AdaGrad**: This optimizer adapts the learning rate for each parameter individually, using smaller rates for frequently updated parameters and larger rates for infrequently updated ones. While innovative, it can sometimes cause the learning rate to decrease too quickly.
> - **RMSProp**: It addresses AdaGrad's rapid decay by using an exponentially decaying average of past squared gradients to normalize current gradients, resulting in a more adaptive and sustained learning process.
> - **Adam Family (Adam, AdaMax, Nadam, AdamW)**: These widely used optimizers combine the ideas of momentum (first-moment estimation) and RMSProp-like per-parameter scaling (second-moment estimation), often providing robust and fast convergence with variants offering further enhancements like Nesterov momentum or improved weight decay.
> - **Learning Rate Scheduling**: This involves dynamically adjusting the learning rate during training using predefined rules (e.g., power decay, exponential decay, piecewise constant) or based on performance (e.g., reducing LR on plateau). This typically allows for faster initial progress and finer-tuning in later stages.

