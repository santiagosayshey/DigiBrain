> [!motivation] Need for Faster Optimizers
>
> **Training very large deep neural networks can be a time-consuming process**. While strategies like careful weight initialization, appropriate activation functions, Batch Normalization, and transfer learning significantly speed up training and improve outcomes, further enhancements are often needed.
> - Another major avenue for accelerating training and potentially finding better solutions lies in using optimization algorithms that are more advanced than the standard Gradient Descent optimizer.

