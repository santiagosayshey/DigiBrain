> [!motivation] The Challenge of Unstable Gradients
>
> Training deep neural networks, while powerful, often encounters a significant hurdle known as **unstable gradients**. This encompasses two primary issues: vanishing gradients and exploding gradients, both of which can severely undermine the learning process.
> - **Vanishing Gradients:** In very deep networks, gradients can become exceedingly small as they are propagated backward from the output layer to the earlier layers. This results in negligible weight updates for the initial layers, causing them to learn very slowly or not at all.
> - **Exploding Gradients:** Conversely, gradients can grow exponentially large during backpropagation. This leads to excessively large weight updates, causing the optimization process to become unstable and diverge, often resulting in `NaN` (Not a Number) values for weights or loss.
>
> These phenomena historically posed a major barrier, slowing down the progress and adoption of deep learning architectures until effective solutions were developed.

> [!idea] Understanding Gradient Instability: Causes
>
> The instability of gradients in deep networks arises primarily from the mechanics of backpropagation and the properties of the network's components. Several factors contribute to gradients either diminishing to near zero (vanishing) or growing uncontrollably (exploding):
> - **Repeated Multiplications:** During backpropagation, gradients are calculated using the chain rule, involving multiplication of derivatives from layer to layer. If these derivatives are consistently smaller than 1, their product rapidly shrinks towards zero (vanishing). If they are consistently larger than 1, their product grows exponentially (exploding).
> - **Activation Function Saturation:** Traditional activation functions like the sigmoid or hyperbolic tangent (tanh) saturate for large positive or negative input values. In these saturated regions, their derivatives are extremely close to zero. This means that during backpropagation, very little gradient signal passes through, especially in the deeper layers (closer to the input), contributing significantly to vanishing gradients.
> - **Weight Initialization Issues:** Early approaches to weight initialization (e.g., drawing from a normal distribution with a mean of 0 and a standard deviation of 1) could exacerbate the problem. As highlighted by Glorot and Bengio in their 2010 paper, such schemes, when combined with sigmoid-like activations, often led to the variance of each layer's outputs being significantly greater than the variance of its inputs. This progressive increase in variance would push activation functions towards their saturation points more quickly.
> - **Activation Function Mean:** The sigmoid function, with a mean output of 0.5 (not 0), also contributed to issues, as non-zero-centered outputs can affect the dynamics of gradient updates in subsequent layers.