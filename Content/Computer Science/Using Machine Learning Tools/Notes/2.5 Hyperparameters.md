> [!motivation] Parameters and Hyperparameters
> 
> Machine learning models need a way to learn patterns from data and make adjustments to improve their performance. Different models analyze various types of data—from simple tabular information to complex images and text. Each model needs a mechanism to capture patterns at different levels of abstraction.
> 
> The challenge lies in determining:
> 
> - How models should represent knowledge internally
> - What values should be automatically learned from data
> - What values should be manually set by humans
> - How to find the optimal configuration for a given task

> [!idea] Parameters
> 
> Parameters are values the **model learns automatically during training through optimization algorithms.**
> 
> Parameters define the internal structure of a model and determine how inputs are transformed into outputs. They include:
> 
> - Weights and biases in neural networks
> - Coefficients in linear and logistic regression
> - Split points in decision trees
> - Means and variances in Gaussian mixture models
> 
> **Key characteristics:**
> 
> - Learned directly from training data
> - Updated during training through optimization algorithms
> - Their optimal values are not known beforehand
> - Their quantity affects model capacity and complexity
> 
> **Examples by model type:**
> 
> |Model Type|Parameter Examples|
> |---|---|
> |Linear Regression|Slope and intercept coefficients|
> |Neural Network|Connection weights and bias terms|
> |Random Forest|Decision boundaries at each node|
> |SVM|Support vectors and their weights|

> [!idea] Hyperparameters
> 
> Hyperparameters are **configuration values set before training begins and control the learning process itself.**
> 
> Unlike parameters, hyperparameters are not learned from data but must be specified by the practitioner. They include:
> 
> - Learning rate in gradient descent
> - Number of hidden layers/neurons in neural networks
> - Number of trees in random forests
> - Regularization strength in regularized models
> 
> **Key characteristics:**
> 
> - Set manually before training
> - Cannot be learned directly from the data
> - Often require tuning through validation performance
> - Control the balance between underfitting and overfitting
> 
> **Common tuning methods:**
> 
> - Grid search: Exhaustive search through specified values
> - Random search: Testing random combinations from specified ranges
> - Bayesian optimization: Sequential search using previous results
> - Cross-validation: Testing performance across multiple data subsets
> 
> ```image_goes_here
> ![A visual showing the relationship between hyperparameters and parameters. The image should show hyperparameters (outside/external to the model) controlling model structure and training process, while parameters (inside the model) are being updated during training.]
> ```

> [!example] Polynomial Regression and Hyperparameters
> 
> In polynomial regression, which is an extension of linear regression, the degree of the polynomial is a hyperparameter.
> 
> **How it works:**
> 
> - The polynomial degree determines the highest power of the independent variable
> - For example, in $y = w_0 + w_1x + w_2x^2 + w_3x^3$, the degree is 3 (cubic)
> - This hyperparameter controls model complexity:
>     - Degree 1: Linear model (straight line)
>     - Degree 2: Quadratic model (parabola)
>     - Degree 3: Cubic model (more flexible curve)
>     - Higher degrees: Increasingly flexible curves
> 
> **Key insight:**
> 
> The degree isn't learned from data—it's a choice made before training. Once set, the model learns the coefficients ($w_0, w_1, w_2, ...$) as parameters based on the training data.
> 
> This illustrates the fundamental difference:
> 
> - Hyperparameters (like polynomial degree): Set by humans, control model structure
> - Parameters (like coefficients): Learned from data through optimization

> [!idea] Hyperparameter Tuning
> 
> Hyperparameter tuning refers to the process of finding optimal hyperparameter values for a machine learning model.
> 
> **Core approaches:**
> 
> - **Grid Search:** Tests all combinations from predefined sets of hyperparameter values
>     - Systematic but computationally expensive for large hyperparameter spaces
>     - Works well for small search spaces with few hyperparameters
> - **Random Search:** Samples random combinations from specified distributions
>     - Often more efficient than grid search for high-dimensional spaces
>     - Can find good solutions with fewer iterations
> - **Bayesian Optimization:** Uses previous evaluations to inform next combinations to try
>     - Builds a probabilistic model of the objective function
>     - Balances exploration and exploitation of the search space
> 
> **Evaluation framework:**
> 
> - Uses validation datasets or cross-validation to assess performance
> - Requires defining an appropriate performance metric (accuracy, F1-score, RMSE, etc.)
> - Guards against overfitting through proper dataset splitting
> 
> **Connection to model selection:**
> 
> Hyperparameter tuning resembles model selection in that both:
> 
> - Involve testing different configurations to find the best performer
> - Require validation data to assess generalization ability
> - Balance model complexity against performance
> 
> In practice, hyperparameter tuning can be viewed as fine-tuning a specific model architecture, while model selection compares different algorithmic approaches altogether.