> [!motivation] The Illusion of Abstraction
> When writing programs, we often operate under a simplified, idealized model of how the underlying hardware works. We **assume dedicated resources, uniform memory access, and serial execution. However, the reality is far more complex.** 
> 
> Consider a basic program that calculates the hypotenuse of a right triangle:
> 
> ```python
> basesquared = base*base
> sidesquare = side*side
> hypotenuse = sqrt(basesquared+sidesquare)
> ```
>
> At first glance, it seems straightforward: calculate the squares of the base and side, then find the square root of their sum. But the actual execution depends on the system's architecture, which can vary greatly from our assumptions.
>
> | Model     | Reality      |
> | --------- | ------------ |
> | Dedicated Hardware | Shared Hardware |
> | Uniform Memory     | Non-Uniform Memory |
> | Serial Execution   | Superscalar Execution |
> 
> This raises a crucial question: **what happens when our virtualization breaks down and our programs are exposed to the mercy of the hardware?** Micro-architectural attacks exploit these discrepancies, undermining the security assumptions we rely on.

> [!idea] Cache Reload Attack: Exploiting the Gap Between Abstraction and Reality
> A cache reload attack exploits the difference between the abstract view of memory and the reality of how caches work, allowing an attacker to infer sensitive information about other processes.
>
> **Abstraction**: Programs assume a uniform memory model, where all memory appears to be a single, fast storage area. The cache virtualizes fast memory access by storing frequently used data closer to the CPU. Programs operate as if they have only one type of memory, unaware that data comes from multiple levels of cache and main memory.
>
> **Reality**: In actual hardware, the cache sits between the CPU and main memory as a separate, intermediate layer. Data moves between these layers based on usage patterns and access times. The cache is physically distinct from the CPU and main memory.
>
> This distinction can be exploited through a cache reload attack:
>
> 1. **Shared Cache**: The cache is a limited, shared resource used by all processes running on a CPU core. When one process accesses data, it is loaded into the cache, making it faster to access for all processes.
>
> 2. **Measuring Access Times**: An attacker can measure the time it takes to access specific memory locations. Fast access (a cache hit) indicates the data was recently used, likely by another process. Slow access (a cache miss) suggests the data was not recently used.
>
> 3. **Inferring Sensitive Information**: By carefully selecting memory locations to monitor and analyzing access times, an attacker can infer which data other processes are using. This can leak sensitive information, such as encryption keys or password hashes, without directly accessing the memory of the target process.
>
> The cache reload attack exploits the gap between the abstract, uniform memory model assumed by programs and the reality of a separate, shared cache layer. It allows an attacker to break the expected isolation between processes and infer sensitive data by indirectly observing cache usage patterns.
>
> This highlights the importance of considering the hardware-level details and potential side-channel attacks when designing secure systems, as the abstractions we rely on may not always hold in the face of shared hardware resources and clever exploitation techniques.


> [!example] Cache Reload Attack in Action
> Suppose we have a victim process that performs encryption using a secret key. The key is stored in memory, and during the encryption process, parts of the key are loaded into the cache.
>
> The attacker wants to determine which parts of the key are being used, hoping to reconstruct the full key over time. Here's how the attack could unfold:
>
> 1. **Establish baseline**: The attacker measures the access time for each part of the key, noting that all accesses are slow (cache misses), as the data is not yet in the cache.
>
> 2. **Wait for encryption**: The attacker waits for the victim process to perform encryption. As the victim uses parts of the key, those parts are loaded into the shared cache.
>
> 3. **Probe the cache**: The attacker measures the access time for each part of the key again.
>    - Parts used by the victim will now be fast to access (cache hits).
>    - Parts not used will remain slow (cache misses).
>
> 4. **Analyze timings**: By comparing the new timings to the baseline, the attacker can infer which parts of the key were used in the encryption.
>
> | Key Part | Baseline Access | Post-Encryption Access | Used by Victim? |
> |----------|-----------------|------------------------|-----------------|
> | Part 1   | Slow (Miss)     | Slow (Miss)            | No              |
> | Part 2   | Slow (Miss)     | Fast (Hit)             | Yes             |
> | Part 3   | Slow (Miss)     | Slow (Miss)            | No              |
> | Part 4   | Slow (Miss)     | Fast (Hit)             | Yes             |
>
> By repeating this process over multiple encryptions, the attacker can gradually learn more about the key, potentially enabling them to reconstruct it entirely.
>
> This example demonstrates how the cache reload attack can break the abstraction of uniform memory access and isolated processes. By exploiting the shared nature and measurable timings of the cache, an attacker can infer sensitive information about other processes running on the same hardware.

