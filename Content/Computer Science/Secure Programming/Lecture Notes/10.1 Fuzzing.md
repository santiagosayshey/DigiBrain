> [!motivation] Why Fuzzing Matters
> - Traditional software testing often **misses edge cases** and vulnerabilities
> - Programs that parse files or process complex input data are particularly vulnerable
> - Any crash indicates a potential bug, though not all bugs are exploitable
> - Need for systematic approaches to find unintended program states


> [!idea] Understanding Fuzzing and Its Strategies
> Fuzzing **finds program bugs by feeding unexpected data into programs**. Think of fuzzing like testing a car:
> 
> **Mutation-based Fuzzing - CHANGING Existing Inputs**:
> - Takes working input and randomly changes it
> - Example: Starting with a valid PDF that opens fine, then changing random bytes. Like taking a working PDF of a tax form and randomly changing numbers and text. The PDF reader might crash when trying to display corrupted data
> 
> **Generation-based Fuzzing - CREATING New Inputs**:
> - Creates brand new inputs following format rules
> - Example: Creating HTTP requests from scratch knowing HTTP rules. Like crafting web requests that are technically valid HTTP but unusual - sending header fields that are maximum allowed length or using rare but valid HTTP methods
> 
> **Coverage-guided Fuzzing - SMART Testing**:
> - Watches what code runs and adjusts testing accordingly
> - Example: Testing an image viewer and noticing certain image dimensions trigger new code paths. Like discovering that very large PNG files make the program use a different decompression routine, then focusing tests on that area
> 
> **Common Tool Examples**:
> | Strategy | Notable Tools |
> |----------|--------------|
> | Mutation | AFL, radamsa |
> | Generation | Peach, Syzkaller |
> | Coverage | AFL++, libFuzzer |
> 
> All strategies aim to crash programs and find bugs, but:
> - Mutation is simple but effective: "break what works"
> - Generation is thorough but complex: "create tricky inputs"
> - Coverage is smart but resource-heavy: "test what's untested"

How's this? One clear callout explaining what fuzzing is, what each strategy means with clear examples, and how they differ?

> [!example] Basic Fuzzing Implementation
> Simple file fuzzing example:
> ```bash
> cat /dev/random | head -c 512 >> rand.jpeg; open rand.jpeg
> ```
> **How it works**:
> 1. Generates random data stream
> 2. Takes first 512 bytes
> 3. Appends to JPEG file
> 4. Attempts to open resulting file
> 
> **Better Approaches**:
> - Randomly corrupt real JPEG files
> - Reference JPEG spec for "JPEG-looking" data
> - Measure parser coverage during testing

> [!idea] Mutation Based Fuzzing
> **Process**:
> 1. Collect corpus of valid input files
> 2. Apply random mutations:
>    - Bit flips
>    - Integer increments/decrements
>    - Boundary value substitution
>    - Special character insertion
> 
> **Characteristics**:
> - Simple setup and execution
> - Uses off-the-shelf tools
> - Success depends on initial corpus quality
> - May have shallow coverage for formats with checksums
> 
> **Real-World Success**:
> Charlie Miller's 2010 PDF fuzzer found 64 exploitable crashes using simple mutations:
> ```python
> numwrites = random.randrange(math.ceil((float(len(buf)) / FuzzFactor))) + 1
> for j in range(numwrites):
>     rbyte = random.randrange(256)
>     rn = random.randrange(len(buf))
>     buf[rn] = "%c"%(rbyte)
> ```

> [!idea] Generation Based Fuzzing
> **Core Process**:
> - Convert input format specifications into generation procedures
> - Generate test cases with random perturbations
> - Run program and check for crashes
> - Iterate and refine
> 
> **Characteristics**:
> - Deeper coverage through format knowledge
> - Domain-specific implementations
> - Requires significant setup effort
> - Not limited by input format complexity

> [!example] Syzkaller Implementation
> **Key Features**:
> - Kernel system call fuzzer
> - Generates syscall sequences from descriptions
> - Runs tests in VM environment
> - Detects potential LPE vulnerabilities
> 
> ```go
> # Example syscall description
> open$proc(file ptr[in, string[proc_file]], flags flags[open_flags], mode const[0]) fd
> ```

> [!idea] Coverage Based Fuzzing
> **Core Concept**: Use code coverage as feedback for fuzzing
> 
> **Coverage Types**:
> - Basic block coverage
> - Edge coverage
> - Path coverage
> 
> **Benefits**:
> - Finds new program states efficiently
> - Combines well with other strategies
> - Proven track record
> 
> **Limitations**:
> - Can't bypass strong checksums
> - Misses certain bug types (e.g., race conditions)

> [!example] American Fuzzy Lop (AFL)
> **Workflow**:
> 1. Compile with coverage instrumentation
> 2. Trim test cases for efficiency
> 3. Mutate files in queue
> 4. Add cases that find new coverage
> 5. Iterate process
> 
> **Key Innovation**: Uses program behavior feedback to guide fuzzing process

> [!example] Fuzzilli
> **Characteristics**:
> - JavaScript engine fuzzer
> - Uses intermediate language (IL)
> - Fuzzes IL instead of raw JavaScript
> 
> ```javascript
> // Example generated test case
> function v0(v1) { 
>   try { 
>     new ArrayBuffer(v1); 
>   } catch(v2) {} 
> }
> v0(0x1000000000000);
> ```

> [!consider] Software Testing Integration
> Three key testing types that complement fuzzing:
> 
> **Unit Tests**:
> - Test individual components
> - Cover error handling
> - Verify expected behavior
> 
> **Regression Tests**:
> - Prevent bug reintroduction
> - "If you don't run them, attackers will"
> 
> **Integration Tests**:
> - Verify component interactions
> - Test system-level behavior
> 
> **General Tips**:
> - Use memory-safe languages where possible
> - Document threat models early
> - Design APIs for safe usage by default
> - Treat all external input as adversarial
> - Maintain consistent coding style