> [!idea] RAID 1: Mirroring
> RAID 1 implements **data redundancy through mirroring**:
> - **Core Concept**: Create exact copies (mirrors) of data across multiple disks
> - **Purpose**: Enhance data reliability and read performance
> - **Implementation**: Typically uses two disks, each containing an identical copy of all data
> 
> ```mermaid
> graph TD
>     L["Logical Disk"] --> D1["Disk 1"]
>     L --> D2["Disk 2"]
>     D1 <--> |"Mirrored Data"| D2
> ```
> 
> **Analysis**:
> - **Capacity**: 50% of total physical capacity (e.g., two 1TB disks yield 1TB usable space)
> - **Write Performance**: Slightly decreased due to writing to multiple disks
> - **Read Performance**: Potentially improved through load balancing across mirrored disks

> [!example] RAID 1 in Action
> Consider a RAID 1 setup with two 1TB disks:
> 
> **Scenario**: Writing a 100MB file
> 1. File is written to Disk 1
> 2. Identical copy is simultaneously written to Disk 2
> 
> **Reading the 100MB file**:
> - System can read 50MB from Disk 1 and 50MB from Disk 2 concurrently
> - Potential for improved read performance through load balancing
> 

> [!idea] RAID 1 Analysis 
> **Capacity**: 
> - Total usable capacity: $C = (n/2) * c$
>   - $n$ = number of disks (must be even)
>   - $c$ = capacity of one disk
> 
> **Fault Tolerance**:
> - Disks that can fail without data loss: $n/2$
> - Probability of data loss: $(p^2)^{n/2}$
>   - $p$ = probability of single disk failure
> 
> **Latency**:
> - Read: $L_r \approx L_{disk}$
> - Write: $L_w \approx 2 * L_{disk}$
>   - $L_{disk}$ = latency of a single disk
> 
> **Throughput**:
> - Sequential read: $T_{sr} \approx n * t_{disk}$
> - Sequential write: $T_{sw} \approx (n/2) * t_{disk}$
> - Random read: $T_{rr} \approx n * t_{disk}$
> - Random write: $T_{rw} \approx (n/2) * t_{disk}$
>   - $t_{disk}$ = throughput of a single disk
> 
> **Key Points**:
> - Capacity utilization is 50%
> - High fault tolerance (can lose half of the disks)
> - Improved read throughput
> - Write throughput limited by mirroring operation
> - Increased write latency due to mirroring


> [!consider] Fault Tolerance in RAID 1
> RAID 1 provides robust fault tolerance:
> - **Disk Failures**: System can tolerate failure of all but one disk
>   - Assuming disks are fail-stop (complete failure rather than partial or corrupted data)
> - **Recovery**: 
>   - Failed disk can be replaced without data loss
>   - System remains operational during disk replacement
> - **Limitations**: 
>   - Does not protect against data corruption or accidental deletion
>   - Higher cost per usable gigabyte compared to other RAID levels



> [!example] RAID 1 Disk Replacement Process
> Scenario: A RAID 1 array with two 2TB disks, where Disk 1 fails
> 
> 1. **Failure Detection**:
>    - RAID controller detects Disk 1 failure
>    - System continues operating using Disk 2
> 
> 2. **Replacement Preparation**:
>    - Administrator is notified of disk failure
>    - New 2TB disk is obtained for replacement
> 
> 3. **Hot-Swap Process** (assuming hot-swap capability):
>    - Failed Disk 1 is physically removed
>    - New disk is inserted into the empty slot
> 
> 4. **Rebuild Process**:
>    - RAID controller automatically begins rebuild
>    - Data is copied from Disk 2 to the new disk
>    - **Rebuild time**: Approximately 2-3 hours for a 2TB disk
>      (varies based on disk speed and system load)
> 
> 5. **Verification**:
>    - RAID controller verifies data integrity
>    - Array returns to fully redundant state
> 
> **Key Points**:
> - System remains operational throughout the process
> - No data loss occurs if Disk 2 remains functional
> - **Performance Impact**: Read/write speeds may decrease during rebuild
> - **Vulnerability**: Array is vulnerable to data loss if Disk 2 fails during rebuild
> 

> [!consider] RAID 1 Consistent Update Problem
> **Problem**: Ensuring consistency when a disk crashes during a write operation
> 
> **Scenario**:
> 1. Write operation begins on both disks
> 2. Disk 1 completes the write
> 3. Disk 1 crashes before Disk 2 completes its write
> 4. Disk 1 comes back online
> 
> **Challenge**: Determining which disk has the correct, up-to-date data
> 
> **Solution**: Use of Non-Volatile RAM (NVRAM) in RAID controller
> 
> **How NVRAM Solves the Problem**:
> 1. **Write Caching**: 
>    - Write operations are first cached in NVRAM
>    - NVRAM retains data even during power loss
> 
> 2. **Atomic Updates**:
>    - Controller ensures all disks are updated before marking write as complete
>    - If a disk fails during write, operation is retried upon disk recovery
> 
> 3. **Recovery Process**:
>    - On disk recovery or system restart, controller checks NVRAM
>    - Incomplete writes are replayed to ensure consistency across all disks
> 
> **Benefits**:
> - Guarantees consistency across mirrored disks
> - Improves write performance by acknowledging writes once cached in NVRAM
> - Protects against data loss during power failures
> 
> **Considerations**:
> - NVRAM adds to the cost of RAID controllers
> - Proper battery backup or capacitor-based power is crucial for NVRAM
