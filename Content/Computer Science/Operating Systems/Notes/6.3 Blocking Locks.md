> [!motivation] The Need for Fair Locks
> Simple spin locks, while efficient, can lead to **unfairness and potential starvation**:
> - Threads might acquire the lock in an arbitrary order
> - Some threads could repeatedly acquire the lock, while others wait indefinitely
> - This unfairness can lead to unpredictable performance and responsiveness issues
> 
> Ticket locks provide a mechanism to ensure **fair ordering** in lock acquisition, addressing these concerns.

> [!example] Thread Starvation with Simple Spin Locks
> Consider a scenario with three threads (A, B, C) competing for a simple spin lock:
> ```
> Time  Thread A              Thread B              Thread C              lock->flag
> ────  ───────────────────   ───────────────────   ───────────────────   ───────────
>  t1   acquire() -> success                                              1
>  t2   [Critical Section]
>  t3                         acquire() -> fail                           1
>  t4                         [Spin]
>  t5                                               acquire() -> fail     1
>  t6                                               [Spin]
>  t7   release()                                                         0
>  t8   acquire() -> success                                              1
>  t9   [Critical Section]
> t10   release()                                                         0
> t11   acquire() -> success                                              1
> t12   [Critical Section]
> t13                         acquire() -> fail                           1
> t14                         [Spin]
> t15                                               acquire() -> fail     1
> t16                                               [Spin]
> t17   release()                                                         0
> t18   acquire() -> success                                              1
> ```
> 
> In this scenario:
> - Thread A repeatedly acquires the lock due to its proximity or scheduling luck
> - Threads B and C are stuck spinning, unable to acquire the lock
> - This pattern could theoretically continue indefinitely, starving B and C
> 
> **Key observations:**
> - No guaranteed order of acquisition
> - Fast threads or those with favorable scheduling can dominate lock acquisition
> - Some threads may face indefinite waiting (starvation)

> [!idea] Ticket Locks: Ensuring FIFO Ordering
> Ticket locks implement a first-in-first-out (FIFO) ordering for lock acquisition, similar to a deli counter system.
> 
> **Key components:**
> - `next_ticket`: Counter for assigning new ticket numbers
> - `now_serving`: Counter indicating the current ticket being served
> 
> **How it works:**
> 1. To acquire the lock, a thread atomically increments `next_ticket` and gets its ticket
> 2. The thread then waits until its ticket number equals `now_serving`
> 3. To release the lock, a thread increments `now_serving`
> 
> **Basic implementation:**
> ```c
> typedef struct {
>     atomic_int next_ticket;
>     atomic_int now_serving;
> } ticket_lock_t;
> 
> void ticket_lock_init(ticket_lock_t *lock) {
>     atomic_store(&lock->next_ticket, 0);
>     atomic_store(&lock->now_serving, 0);
> }
> 
> void ticket_lock_acquire(ticket_lock_t *lock) {
>     int my_ticket = atomic_fetch_add(&lock->next_ticket, 1);
>     while (atomic_load(&lock->now_serving) != my_ticket) {
>         // Busy-wait (could be optimized with pause or yield)
>     }
> }
> 
> void ticket_lock_release(ticket_lock_t *lock) {
>     atomic_fetch_add(&lock->now_serving, 1);
> }
> ```

> [!example] Ticket Lock in Action
> Consider three threads (A, B, C) trying to acquire a ticket lock:
> 
> ```
> Time  Thread A              Thread B              Thread C              next_ticket  now_serving
> ────  ───────────────────   ───────────────────   ───────────────────   ───────────  ───────────
>  t1   get_ticket() -> 0                                                 1            0
>  t2   [Enter critical]
>  t3                         get_ticket() -> 1                           2            0
>  t4                         [Wait]
>  t5                                               get_ticket() -> 2     3            0
>  t6                                               [Wait]
>  t7   [Exit]
>  t8   release()                                                         3            1
>  t9                         [Enter critical]                            3            1
> t10                         [Exit]
> t11                         release()                                   3            2
> t12                                               [Enter critical]      3            2
> ```
> 
> This example shows how ticket locks ensure FIFO ordering:
> - Threads acquire tickets in the order they arrive (A:0, B:1, C:2)
> - They enter the critical section in the same order, regardless of when they finish waiting

> [!consider] Advantages and Limitations of Ticket Locks
> **Advantages:**
> - **Fairness:** Guarantees FIFO ordering, preventing starvation
> - **Simplicity:** Relatively simple to implement and understand
> - **Scalability:** Works well for moderate contention scenarios
> 
> **Limitations:**
> - **Memory usage:** Requires two counters, which can wrap around for long-running systems
> - **Busy-waiting:** Still relies on spinning, which can waste CPU cycles
> - **Lack of adaptivity:** Doesn't adapt to different contention scenarios
> 
> **Considerations:**
> - Ticket locks are often used as a building block for more complex locking schemes
> - They can be combined with other techniques (e.g., exponential backoff) to reduce CPU usage during waiting
> - In practice, many systems use more sophisticated locks (e.g., queue-based locks) for high-contention scenarios

> [!motivation] CPU Scheduler Ignorance in Lock Contention
> The CPU scheduler, designed for general-purpose multitasking, is **unaware of the specific needs and dependencies** of processes or threads competing for locks:
> 
> - It doesn't consider which threads are waiting for a lock
> - It may schedule threads that can't make progress due to lock contention
> - This can lead to inefficient use of CPU time and increased power consumption
> 
> **Example scenario:**
> Suppose we have processes A, B, C, and D, where B, C, and D are waiting for A to release a lock:
> 
> ```
> Time  CPU 1   CPU 2   CPU 3   CPU 4   Lock Holder
> ────  ──────  ──────  ──────  ──────  ────────────
>  t1   A       B       C       D       A
>  t2   A       B       C       D       A
>  t3   B       C       D       A       A
>  t4   B       C       D       A       A
>  t5   C       D       A       B       A
> ```
> 
> In this scenario:
> - B, C, and D waste CPU cycles spinning while waiting for A
> - The scheduler continues to allocate time to B, C, and D, unaware they can't progress
> - This leads to inefficient CPU utilization and increased power consumption
> 
> A more efficient approach would allow waiting threads to yield the CPU, improving overall system performance.

> [!idea] Ticket Locks with Yield: Improving Efficiency
> Ticket locks with yield combine the fairness of ticket locks with the efficiency of yielding the CPU when waiting.
> 
> **Key components:**
> - `next_ticket`: Counter for assigning new ticket numbers
> - `now_serving`: Counter indicating the current ticket being served
> - `yield()`: System call to voluntarily give up CPU time
> 
> **How it works:**
> 1. Thread acquires a ticket number
> 2. If it's not the thread's turn, it yields the CPU
> 3. Upon waking, it checks again, repeating the yield if necessary
> 4. When it's the thread's turn, it enters the critical section
> 
> **Implementation:**
> ```c
> typedef struct {
>     atomic_int next_ticket;
>     atomic_int now_serving;
> } yield_ticket_lock_t;
> 
> void yield_ticket_lock_acquire(yield_ticket_lock_t *lock) {
>     int my_ticket = atomic_fetch_add(&lock->next_ticket, 1);
>     while (atomic_load(&lock->now_serving) != my_ticket) {
>         sched_yield();  // Yield the CPU
>     }
> }
> 
> void yield_ticket_lock_release(yield_ticket_lock_t *lock) {
>     atomic_fetch_add(&lock->now_serving, 1);
> }
> ```
> 
> **Improved scenario:**
> ```
> Time  CPU 1   CPU 2   CPU 3   CPU 4   Lock Holder
> ────  ──────  ──────  ──────  ──────  ────────────
>  t1   A       idle    idle    idle    A
>  t2   A       idle    idle    idle    A
>  t3   A       idle    idle    idle    A
>  t4   B       idle    idle    idle    B
>  t5   C       idle    idle    idle    C
> ```
> 
> In this improved scenario:
> - B, C, and D yield the CPU while waiting for their turn
> - This allows A to use the CPU more efficiently, potentially releasing the lock faster
> - Overall system efficiency is improved, and power consumption is reduced

> [!consider] Benefits and Tradeoffs of Ticket Locks with Yield
> **Benefits:**
> - Maintains fairness of ticket locks
> - Reduces CPU waste from busy-waiting
> - Improves overall system efficiency
> - Potentially reduces power consumption
> 
> **Tradeoffs:**
> - Slightly higher overhead due to context switches
> - Potential for increased latency in low-contention scenarios
> - Relies on efficient implementation of `yield()` by the operating system
> 
> **Considerations:**
> - Well-suited for scenarios with moderate to high lock contention
> - Can be combined with adaptive strategies for optimal performance across different contention levels
> - May need tuning (e.g., introducing short spin periods before yielding) for best performance in specific use cases

> [!motivation] The Need for Blocking Locks
> While spin locks can be efficient for short waits, they have drawbacks:
> - **CPU waste:** Continuously spinning consumes CPU cycles
> - **Energy inefficiency:** Constant activity increases power consumption
> - **Priority inversion:** Low-priority tasks holding a lock can prevent high-priority tasks from running
> 
> For longer waits or in systems where energy efficiency is crucial, a blocking approach can be more beneficial.

> [!idea] Blocking Locks: Sleep When Waiting
> Blocking locks put waiting threads to sleep instead of spinning, freeing up CPU resources for other tasks.
> 
> **Key concepts:**
> - **Sleep queue:** A queue where waiting threads are placed when they can't acquire the lock
> - **Wake-up mechanism:** A way to wake up sleeping threads when the lock becomes available
> 
> **How it works:**
> 1. Thread attempts to acquire the lock
> 2. If the lock is unavailable, the thread is added to a sleep queue and blocks (sleeps)
> 3. When the lock is released, one or more waiting threads are woken up
> 4. Awakened threads compete to acquire the lock
> 
> **Basic implementation (pseudocode):**
> ```c
> typedef struct {
>     int locked;  // 0: unlocked, 1: locked
>     queue_t wait_queue;  // Queue of waiting threads
> } blocking_lock_t;
> 
> void blocking_lock_acquire(blocking_lock_t *lock) {
>     disable_interrupts();  // Enter critical section
>     if (lock->locked) {
>         add_to_wait_queue(&lock->wait_queue, current_thread);
>         sleep();  // Releases interrupts and switches context
>     } else {
>         lock->locked = 1;
>         enable_interrupts();
>     }
> }
> 
> void blocking_lock_release(blocking_lock_t *lock) {
>     disable_interrupts();
>     if (queue_empty(&lock->wait_queue)) {
>         lock->locked = 0;
>     } else {
>         thread_t *thread = remove_from_wait_queue(&lock->wait_queue);
>         wake_up(thread);  // This thread will acquire the lock
>     }
>     enable_interrupts();
> }
> ```

> [!example] Blocking Lock in Action
> Consider three threads (A, B, C) competing for a blocking lock:
> 
> ```
> Time  Thread A              Thread B              Thread C              Lock State
> ────  ───────────────────   ───────────────────   ───────────────────   ───────────
>  t1   acquire() -> success                                              Locked (A)
>  t2   [Critical Section]
>  t3                         acquire() -> block                          Locked (A), B sleeping
>  t4                                               acquire() -> block    Locked (A), B&C sleeping
>  t5   release()                                                         Unlocked, B woken
>  t6                         acquire() -> success                        Locked (B), C sleeping
>  t7                         [Critical Section]
>  t8                         release()                                   Unlocked, C woken
>  t9                                               acquire() -> success  Locked (C)
> ```
> 
> In this scenario:
> - Waiting threads (B and C) sleep instead of spinning
> - CPU can be used for other tasks while threads are blocked
> - Threads are woken in the order they were blocked (FIFO fairness)

> [!consider] Advantages and Limitations of Blocking Locks
> **Advantages:**
> - **CPU efficiency:** Frees up CPU for other tasks when threads are waiting
> - **Energy savings:** Reduced CPU activity can lead to lower power consumption
> - **Fairness:** Can easily implement FIFO ordering of lock acquisition
> - **Priority handling:** Can be integrated with priority scheduling mechanisms
> 
> **Limitations:**
> - **Context switch overhead:** Blocking and waking involve context switches, which have a cost
> - **Latency:** May introduce more latency compared to spinning for very short waits
> - **Complexity:** More complex to implement correctly, especially in user space
> 
> **Considerations:**
> - Often implemented by the operating system (e.g., mutexes, semaphores)
> - Can be combined with spinning in hybrid approaches (spin-then-block)
> - Well-suited for scenarios where lock hold times are unpredictable or potentially long
> - May require kernel support for efficient implementation
