> [!idea] RAID 1: Mirroring
> RAID 1 implements **data redundancy through mirroring**:
> - **Core Concept**: Create exact copies (mirrors) of data across multiple disks
> - **Purpose**: Enhance data reliability and read performance
> - **Implementation**: Typically uses two disks, each containing an identical copy of all data
> 
> ```mermaid
> graph TD
>     L["Logical Disk"] --> D1["Disk 1"]
>     L --> D2["Disk 2"]
>     D1 <--> |"Mirrored Data"| D2
> ```
> 
> **Analysis**:
> - **Capacity**: 50% of total physical capacity (e.g., two 1TB disks yield 1TB usable space)
> - **Write Performance**: Slightly decreased due to writing to multiple disks
> - **Read Performance**: Potentially improved through load balancing across mirrored disks

> [!idea] RAID 1 Analysis
> **Capacity**: 
> - Total capacity: $C = c$
>   - $c$ = capacity of one disk
> 
> **Fault Tolerance**:
> - Disks that can fail without data loss: $n - 1$
> - Probability of data loss: $p^n$
>   - $p$ = probability of single disk failure
>   - $n$ = number of disks (typically 2)
> 
> **Latency**:
> - Read: $L_r \approx L_{disk}$
> - Write: $L_w \approx 2 * L_{disk}$
>   - $L_{disk}$ = latency of a single disk
> 
> **Throughput**:
> - Sequential read: $T_{sr} \approx n * t_{disk}$
> - Sequential write: $T_{sw} \approx t_{disk}$
> - Random read: $T_{rr} \approx n * t_{disk}$
> - Random write: $T_{rw} \approx t_{disk}$
>   - $t_{disk}$ = throughput of a single disk
>   - $n$ = number of disks (typically 2)
> 
> **Key Points**:
> - Reduced capacity utilization (50% for two disks)
> - High fault tolerance (can lose all but one disk)
> - Improved read throughput
> - Write operations limited by slowest disk
> - Increased write latency due to mirroring

> [!consider] Fault Tolerance in RAID 1
> RAID 1 provides robust fault tolerance:
> - **Disk Failures**: System can tolerate failure of all but one disk
>   - Assuming disks are fail-stop (complete failure rather than partial or corrupted data)
> - **Recovery**: 
>   - Failed disk can be replaced without data loss
>   - System remains operational during disk replacement
> - **Limitations**: 
>   - Does not protect against data corruption or accidental deletion
>   - Higher cost per usable gigabyte compared to other RAID levels

> [!example] RAID 1 in Action
> Consider a RAID 1 setup with two 1TB disks:
> 
> **Scenario**: Writing a 100MB file
> 1. File is written to Disk 1
> 2. Identical copy is simultaneously written to Disk 2
> 
> **Reading the 100MB file**:
> - System can read 50MB from Disk 1 and 50MB from Disk 2 concurrently
> - Potential for improved read performance through load balancing
> 

> [!example] RAID 1 Disk Replacement Process
> Scenario: A RAID 1 array with two 2TB disks, where Disk 1 fails
> 
> 1. **Failure Detection**:
>    - RAID controller detects Disk 1 failure
>    - System continues operating using Disk 2
> 
> 2. **Replacement Preparation**:
>    - Administrator is notified of disk failure
>    - New 2TB disk is obtained for replacement
> 
> 3. **Hot-Swap Process** (assuming hot-swap capability):
>    - Failed Disk 1 is physically removed
>    - New disk is inserted into the empty slot
> 
> 4. **Rebuild Process**:
>    - RAID controller automatically begins rebuild
>    - Data is copied from Disk 2 to the new disk
>    - **Rebuild time**: Approximately 2-3 hours for a 2TB disk
>      (varies based on disk speed and system load)
> 
> 5. **Verification**:
>    - RAID controller verifies data integrity
>    - Array returns to fully redundant state
> 
> **Key Points**:
> - System remains operational throughout the process
> - No data loss occurs if Disk 2 remains functional
> - **Performance Impact**: Read/write speeds may decrease during rebuild
> - **Vulnerability**: Array is vulnerable to data loss if Disk 2 fails during rebuild
> 
> ```mermaid
> graph TD
>     A[Disk 1 Fails] --> B[Continue with Disk 2]
>     B --> C[Replace Disk 1]
>     C --> D[Rebuild Array]
>     D --> E[Verify Data]
>     E --> F[Full Redundancy Restored]
> ```



