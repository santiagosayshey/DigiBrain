> [!motivation] CPU Scheduler Ignorance in Lock Contention
> The CPU scheduler, designed for general-purpose multitasking, is **unaware of the specific needs and dependencies** of processes or threads competing for locks:
> 
> - It doesn't consider which threads are waiting for a lock
> - It may schedule threads that can't make progress due to lock contention
> - This can lead to inefficient use of CPU time and increased power consumption
> 
> **Example scenario:**
> Suppose we have processes A, B, C, and D, where B, C, and D are waiting for A to release a lock:
> 
> ```
> Time  CPU 1   CPU 2   CPU 3   CPU 4   Lock Holder
> ────  ──────  ──────  ──────  ──────  ────────────
>  t1   A       B       C       D       A
>  t2   A       B       C       D       A
>  t3   B       C       D       A       A
>  t4   B       C       D       A       A
>  t5   C       D       A       B       A
> ```
> 
> In this scenario:
> - B, C, and D waste CPU cycles spinning while waiting for A
> - The scheduler continues to allocate time to B, C, and D, unaware they can't progress
> - This leads to inefficient CPU utilization and increased power consumption
> 
> A more efficient approach would allow waiting threads to yield the CPU, improving overall system performance.

> [!idea] Ticket Locks with Yield: Improving Efficiency
> Ticket locks with yield combine the fairness of ticket locks with the efficiency of yielding the CPU when waiting.
> 
> **Key components:**
> - `next_ticket`: Counter for assigning new ticket numbers
> - `now_serving`: Counter indicating the current ticket being served
> - `yield()`: System call to voluntarily give up CPU time
> 
> **How it works:**
> 1. Thread acquires a ticket number
> 2. If it's not the thread's turn, it yields the CPU
> 3. Upon waking, it checks again, repeating the yield if necessary
> 4. When it's the thread's turn, it enters the critical section
> 
> **Implementation:**
> ```c
> typedef struct {
>     atomic_int next_ticket;
>     atomic_int now_serving;
> } yield_ticket_lock_t;
> 
> void yield_ticket_lock_acquire(yield_ticket_lock_t *lock) {
>     int my_ticket = atomic_fetch_add(&lock->next_ticket, 1);
>     while (atomic_load(&lock->now_serving) != my_ticket) {
>         sched_yield();  // Yield the CPU
>     }
> }
> 
> void yield_ticket_lock_release(yield_ticket_lock_t *lock) {
>     atomic_fetch_add(&lock->now_serving, 1);
> }
> ```
> 
> **Improved scenario:**
> ```
> Time  CPU 1   CPU 2   CPU 3   CPU 4   Lock Holder
> ────  ──────  ──────  ──────  ──────  ────────────
>  t1   A       idle    idle    idle    A
>  t2   A       idle    idle    idle    A
>  t3   A       idle    idle    idle    A
>  t4   B       idle    idle    idle    B
>  t5   C       idle    idle    idle    C
> ```
> 
> In this improved scenario:
> - B, C, and D yield the CPU while waiting for their turn
> - This allows A to use the CPU more efficiently, potentially releasing the lock faster
> - Overall system efficiency is improved, and power consumption is reduced

> [!consider] Benefits and Tradeoffs of Ticket Locks with Yield
> **Benefits:**
> - Maintains fairness of ticket locks
> - Reduces CPU waste from busy-waiting
> - Improves overall system efficiency
> - Potentially reduces power consumption
> 
> **Tradeoffs:**
> - Slightly higher overhead due to context switches
> - Potential for increased latency in low-contention scenarios
> - Relies on efficient implementation of `yield()` by the operating system
> 
> **Considerations:**
> - Well-suited for scenarios with moderate to high lock contention
> - Can be combined with adaptive strategies for optimal performance across different contention levels
> - May need tuning (e.g., introducing short spin periods before yielding) for best performance in specific use cases
