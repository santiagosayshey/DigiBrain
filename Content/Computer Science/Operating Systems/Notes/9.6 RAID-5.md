> [!idea] RAID 5: Distributed Parity
> RAID 5 uses **block-level striping with distributed parity**:
> 
> **Core Concept**: Distribute both data and parity information across all disks
> 
> **Key Differences from RAID 4**:
> - Parity is distributed across all disks, not confined to a single disk
> - Eliminates the write bottleneck associated with a dedicated parity disk
> 
> **Implementation**: 
> - Minimum of 3 disks
> - Parity and data are striped across all disks
> 
> ```mermaid
> graph TD
>    L["Logical Disk"] --> D1["Disk 1: Data + Parity"]
>    L --> D2["Disk 2: Data + Parity"]
>    L --> D3["Disk 3: Data + Parity"]
>    L --> D4["Disk 4: Data + Parity"]
> ```
> 
> **Key Characteristics**:
> - Improved write performance compared to RAID 4
> - Better load balancing across all disks
> - Maintains ability to reconstruct data if a single disk fails
> - More efficient use of disk space compared to RAID 1 (mirroring)

> [!idea] RAID 5 Analysis
> **Capacity**: 
> - Total usable capacity: $C = (n - 1) * c$
>   - $n$ = total number of disks
>   - $c$ = capacity of one disk
> 
> **Fault Tolerance**:
> - Disks that can fail without data loss: 1
> - Probability of data loss: Same as RAID 4
> 
> **Latency**:
> - Read: $L_r \approx L_{disk}$
> - Write: $L_w \approx 4 * L_{disk}$ (read-modify-write cycle)
> 
> **Throughput**:
> - Sequential read: $T_{sr} \approx n * t_{disk}$
> - Sequential write: $T_{sw} \approx (n-1) * t_{disk}$ / 4 (due to read-modify-write)
> - Random read: $T_{rr} \approx n * t_{disk}$
> - Random write: $T_{rw} \approx (n-1) * t_{disk}$ / 4
>   - $t_{disk}$ = throughput of a single disk

> [!example] RAID 5 in Action
> Consider a RAID 5 setup with four 1TB disks:
> 
> **Scenario**: Writing a 128KB file
> 
> 1. **Data and Parity Distribution**:
>    - 32KB data + 32KB parity written to Disk 1
>    - 32KB data + 32KB parity written to Disk 2
>    - 32KB data + 32KB parity written to Disk 3
>    - 32KB data + 32KB parity written to Disk 4
> 
> 2. **Parity Calculation**: Similar to RAID 4, but distributed
> 
> 3. **Write Process** (for one stripe):
>    - Read old data and old parity
>    - Calculate new parity
>    - Write new data and new parity
> 
> **Disk Failure Scenario**:
> 1. Any single disk fails
> 2. System continues to operate using remaining disks
> 3. Data is reconstructed on-the-fly using parity from other disks
> 4. Upon replacement, failed disk is rebuilt using data and parity from others
> 
> **Key Points**:
> - Improved write performance over RAID 4 due to distributed parity
> - Better load balancing and elimination of parity disk bottleneck
> - Slightly more complex write operations (read-modify-write)
> - Maintains fault tolerance against single disk failure

> [!consider] RAID 5 in Practice
> **Advantages**:
> 1. Better write performance than RAID 4
> 2. Efficient use of storage capacity
> 3. Good read performance due to striping
> 
> **Limitations**:
> 1. Write performance still impacted by parity calculations
> 2. Rebuild times can be long for large arrays
> 3. Vulnerable to data loss during rebuild if another disk fails
> 
> **Use Cases**:
> - General-purpose storage for environments with more reads than writes
> - Balancing performance, capacity, and redundancy in smaller arrays
> 
> **Industry Adoption**:
> - Widely used in various storage systems
> - Often preferred over RAID 4 in general-purpose applications
> - For larger arrays or more write-intensive workloads, RAID 6 or RAID 10 might be preferred
