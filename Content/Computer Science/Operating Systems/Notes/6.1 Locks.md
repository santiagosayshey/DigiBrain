> [!motivation] The Need for Concurrency
> So far, we have primarily considered machines that **execute single processes from start to finish.** However, this sequential execution model has limitations:
> 
> - **Underutilized Resources**: When a process is blocked or waiting for I/O operations, the CPU remains idle, leading to underutilized system resources.
> - **Limited Performance**: Sequential execution can limit the overall performance, especially when multiple tasks could be executed independently.
> - **Lack of Responsiveness**: Long-running processes can make the system unresponsive, affecting user experience and the execution of other processes.
> 
> To address these limitations, we want to introduce **concurrency, where multiple CPU threads or cores interleave their execution to form processes.** Concurrency allows for more efficient resource utilization, improved performance, and enhanced responsiveness.

> [!consider] Ensuring Correctness in Concurrent Programs
> Introducing concurrency raises concerns about ensuring the correctness of program execution, particularly when multiple threads access shared resources. Consider the following example:
> 
> ```
> balance = balance + 1
> ```
> 
> In a concurrent environment, **multiple threads might attempt to execute this statement simultaneously**. If not properly managed, this **can lead to race conditions** and incorrect results.
> 
> To ensure correctness, **instructions accessing shared memory need to be atomic**, meaning they must execute as an uninterruptible unit. Atomicity guarantees that the effects of an instruction are visible to other threads only after it has completed.
> 
> Achieving atomicity requires the concept of mutual exclusion. **If process A is executing a critical section (a block of code that accesses shared resources), then no other process, such as process C, should be allowed to enter that same critical section simultaneously.**

> [!exercise]+ Critical Section
> ```c
> // Critical Section
> balance = balance + depositAmount;
> totalDeposits = totalDeposits + 1;
> // End Critical Section
> ```
> 
> In this case, the code block updating the `balance` and `totalDeposits` variables should be treated as a critical section. Only one thread should execute this block at a time to avoid inconsistencies.
> 
> To enforce mutual exclusion and ensure correctness, programmers can use synchronization mechanisms such as **locks, semaphores, or mutexes**. These mechanisms coordinate access to shared resources and prevent multiple threads from concurrently executing critical sections.
> 
> Ensuring correctness in concurrent programs is crucial to maintain data integrity and avoid unexpected behavior. Programmers must carefully design and implement synchronization mechanisms to guarantee the atomicity of critical sections and prevent race conditions.

> [!idea] Locks: Ensuring Mutual Exclusion
> Locks ensure **only one thread** can access a shared resource at a time. They have two states:
>
> - **Available**: Lock is free to be acquired
> - **Acquired**: Lock is held by a thread
>
> Key concepts:
> - **Mutex**: Common lock type for **mutual exclusion**
> - **Acquisition**: Thread **acquires** available lock before entering critical section
> - **Blocking**: Other threads **wait** if lock is acquired
> - **Release**: Thread **releases** lock after exiting critical section, making it available
> - **Atomicity**: Lock operations use **atomic instructions** at low level
>
> ```c
> lock_t mutex;  // Declare a lock variable (initially available)
> 
> void update_balance() {
>     lock(&mutex);  // Acquire lock
>     balance = balance + 1;  // Critical section
>     unlock(&mutex);  // Release lock
> }
> ```

> [!consider] Locks: The "Speaking Ball" Analogy
> Locks can be compared to a classroom game where **only the person holding a ball can speak**.
> 
> - **Lock = Ball**: Only the thread with the lock (ball) can enter the critical section (speak)
> - **Critical Section = Speaking Time**: Exclusive access to shared resources
> - **Waiting Threads = Silent Students**: Other threads wait for the lock to be released
> 
> This analogy illustrates **mutual exclusion** in concurrent programming, showing how locks manage access to shared resources.

> [!example] Shared Linked List with Locks using pthreads in C
> 
> ```c
> #include <pthread.h>
> #include <stdlib.h>
> #include <stdio.h>
> 
> typedef struct Node {
>     int data;
>     struct Node* next;
> } Node;
> 
> typedef struct {
>     Node* head;
>     pthread_mutex_t lock;
> } LinkedList;
> 
> void init_list(LinkedList* list) {
>     list->head = NULL;
>     pthread_mutex_init(&list->lock, NULL);
> }
> 
> void insert(LinkedList* list, int data) {
>     pthread_mutex_lock(&list->lock);
>     
>     Node* new_node = malloc(sizeof(Node));
>     new_node->data = data;
>     new_node->next = list->head;
>     list->head = new_node;
>     
>     pthread_mutex_unlock(&list->lock);
> }
> 
> void display(LinkedList* list) {
>     pthread_mutex_lock(&list->lock);
>     
>     Node* current = list->head;
>     while (current != NULL) {
>         printf("%d -> ", current->data);
>         current = current->next;
>     }
>     printf("NULL\n");
>     
>     pthread_mutex_unlock(&list->lock);
> }
> ```
> 
> Concurrent insertion scenario:
> 
> Without locks:
> ```
> Initial list: Head -> 1 -> 2 -> NULL
> 
> Time | Thread 1 (Insert A)     | Thread 2 (Insert B)
> -----|-------------------------|-------------------------
>   1  | Read Head               | Read Head
>   2  | Create node A           | Create node B
>   3  | Set A.next = Head       | Set B.next = Head
>   4  | Set Head = A            | Set Head = B
> 
> Result: Head -> B -> NULL  (Node A is lost)
> ```
> 
> With locks:
> ```
> Initial list: Head -> 1 -> 2 -> NULL
> 
> Time | Thread 1 (Insert A)     | Thread 2 (Insert B)
> -----|-------------------------|-------------------------
>   1  | pthread_mutex_lock      | pthread_mutex_lock (blocks)
>   2  | Read Head               |
>   3  | Create node A           |
>   4  | Set A.next = Head       |
>   5  | Set Head = A            |
>   6  | pthread_mutex_unlock    | pthread_mutex_lock (acquires)
>   7  |                         | Read Head (now A)
>   8  |                         | Create node B
>   9  |                         | Set B.next = Head (A)
>  10  |                         | Set Head = B
>  11  |                         | pthread_mutex_unlock
> 
> Result: Head -> B -> A -> NULL  (Both nodes inserted correctly)
> ```
> 
> This implementation demonstrates:
> - Use of pthreads mutex for locking in C
> - Proper lock usage in insert and display functions
> - How locks prevent lost updates and maintain list consistency

> [!idea]+ Lock Implementation Essentials
> To ensure the correctness and efficiency of lock implementations, the following key aspects must be considered:
>
> 1. **Mutual Exclusion**:
>    - Locks must guarantee that only one thread can hold the lock and enter the critical section at a time.
>    - This prevents concurrent access to shared resources and maintains data consistency.
>
> 2. **Deadlock Avoidance**:
>    - Lock implementation must prevent deadlocks, where threads are waiting for each other to release locks indefinitely.
>    - Techniques like lock ordering and timeout mechanisms can help mitigate deadlocks.
>
> 3. **Starvation Prevention and Fairness**:
>    - Locks should ensure that all threads have a fair chance to acquire the lock and make progress.
>    - Starvation, where a thread is perpetually blocked waiting for a lock, should be prevented.
>    - Fairness can be achieved through FIFO queuing or randomized selection of waiting threads.
>
> 4. **Performance Optimization**:
>    - Lock operations should minimize the use of expensive atomic instructions to reduce overhead.
>    - Techniques like spin locks can be used for short critical sections to avoid context switching costs.
>
> Implementing locks correctly is crucial for maintaining the correctness and performance of concurrent programs. It requires careful consideration of mutual exclusion, deadlock avoidance, starvation prevention, fairness, and performance optimization.
