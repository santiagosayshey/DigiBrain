> [!motivation] Beyond Traditional File Systems
> Modern storage systems face challenges that make FFS-style designs less optimal:
> 
> **Technology Shifts**:
> - RAM sizes increased dramatically (MB → GB)
> - Large RAM caches make read performance less critical
> - Write performance becomes the bottleneck
> - SSDs change access pattern assumptions
> 
> **FFS Limitations**:
> - Random writes require multiple disk accesses:
>   - Update data blocks
>   - Modify inode
>   - Update bitmaps
>   - Write journal entries
> - Each write operation causes disk head movement
> - Write amplification impacts SSD lifetime
> 
> These factors led to rethinking file system design, focusing on sequential writes and write optimization rather than read optimization.

> [!idea] Log-Structured File System (LFS)
> LFS treats the disk as a **circular log**, writing all data sequentially at the log's "head". The log is divided into fixed-size regions called **segments** (typically 1MB each). Each segment is the basic unit of:
> - Writing: System buffers writes until a full segment
> - Cleaning: Old segments recycled when space needed
> - Storage management: Space allocated in segment units
> 
> ![[Pasted image 20241031081643.png|700]]
> 
> **Segment Structure**:
> Each segment contains four parts:
> - **Data Blocks**: New file contents
> - **Inodes**: File metadata
> - **Imap Pieces**: Maps inode numbers to locations
> - **Segment Summary**: Tracks segment contents
> 
> **Segment Cleaning**:
> The cleaning process runs based on available free space:
> - **Threshold-based**: Starts when free segments drop below threshold
> - **Background Processing**: Runs during idle periods
> 
> **Block Liveness**:
> A block in a segment is "dead" (cleanable) when:
> - File is deleted
> - Block is overwritten (new version exists elsewhere)
> - Inode points to newer version
> LFS uses segment summary blocks and imap to determine block status.
> 
> **Key Insight**:
> By treating the disk as a circular buffer of segments and always writing full segments sequentially at the head:
> - Writes become sequential and fast
> - Old versions remain until space is needed
> - Cleaning process reclaims space from old segments

> [!example] LFS Operations
> Consider a user updating a small file '/home/user/config.txt':
> 
> **Initial State**: 
> ```
> Segment 5: 
>   - Data: config.txt (original 2KB)
>   - Inode 127 points to this location
> ```
> 
> **After Update**:
> ```
> Segment 5: (Now contains dead blocks)
>   - Data: old config.txt (dead)
>   - Old inode 127 (dead)
> 
> Current Segment: 
>   - Data: new config.txt
>   - Updated inode 127
>   - Updated imap (points to new inode location)
>   - Segment summary (records new block locations)
> ```
> 
> **During Cleaning**:
> When segment 5 is selected for cleaning:
> 1. Check segment summary and imap
> 2. Determine config.txt blocks are dead (inode points elsewhere)
> 3. Mark segment as free
> 4. No live data to copy (in this case)

> [!idea] Write Buffering in LFS
> Write buffering is the process of collecting writes in memory until a full segment can be written. This mechanism:
> - Accumulates file data, inodes, and metadata in RAM
> - Waits until segment-sized chunk (typically 1MB) is full
> - Performs single large sequential write to disk
> - Improves performance by minimizing disk operations

> [!example] Write Buffering Example
> Writing three files with 1MB segments:
> ```
> Memory Buffer (initially empty):
> 1. Write /home/file1 (300KB)
>    [file1: 300KB][-empty-: 700KB]
> 
> 2. Write /home/file2 (500KB)
>    [file1: 300KB][file2: 500KB][-empty-: 200KB]
> 
> 3. Write /home/file3 (400KB)
>    - Buffer full! Write to disk:
>    [file1: 300KB][file2: 500KB][file3(p1): 200KB]
>    - Start new buffer:
>    [file3(p2): 200KB][-empty-: 800KB]
> ```

> [!idea] Checkpointing Mechanism
> Checkpoints provide consistency points for crash recovery. A checkpoint:
> - Records the current state of the file system
> - Stores locations of all imap pieces
> - Maintains two copies at fixed locations
> - Creates recoverable state without scanning entire disk

> [!example] Checkpoint Creation
> System performing checkpoint:
> ```
> 1. Current State:
>    - Active Segment: 45
>    - Imap pieces in segments: [40,42,44]
>    - Pending writes in memory
> 
> 2. Checkpoint Process:
>    - Flush memory buffer to segment 45
>    - Write checkpoint to disk:
>      Location 1: [Segment=45, Imap=[40,42,44]]
>      Location 2: [Backup copy of same data]
> ```

> [!idea] Garbage Collection
> Garbage collection is the process of reclaiming space from segments containing dead blocks:
> - Scans segments for live/dead block ratio
> - Selects segments based on utilization
> - Copies live data to current segment
> - Reclaims segments with mostly dead blocks
> - Maintains pool of free segments

> [!example] Garbage Collection Process
> Cleaning segments in a file system:
> ```
> Initial State:
> Segment 1: [A1][B1][C1] (A1,C1 dead, B1 live)
> Segment 2: [D1][E1][F1] (All live)
> Segment 3: [A2][B2]     (A2 live, B2 dead)
>
> Cleaning Process:
> 1. Check utilization:
>    Segment 1: 33% live  (1/3 blocks)
>    Segment 2: 100% live (3/3 blocks)
>    Segment 3: 50% live  (1/2 blocks)
>
> 2. Clean Segment 1:
>    - Copy B1 to current segment
>    - Mark Segment 1 as free
> ```

> [!idea] Block Liveness
> Block liveness tracking determines which blocks can be cleaned:
> - Uses imap to find current block versions
> - Compares segment summary with current inode data
> - Identifies blocks no longer referenced
> - Tracks version numbers for each block
> - Maintains reference counts for shared blocks

> [!example] Block Liveness Verification
> Checking if blocks are live:
> ```
> Segment Summary for Segment 5:
> - Block 1: File A, inode 127, version 1
> - Block 2: File B, inode 128, version 1
> - Block 3: File A, inode 127, version 1
>
> Current Imap State:
> - Inode 127 (File A) → points to version 2 in Segment 8
> - Inode 128 (File B) → points to version 1 in Segment 5
>
> Result:
> - Blocks 1,3: Dead (newer version exists)
> - Block 2: Live (current version)
> ```

> [!consider] Crash Recovery Mechanisms
> LFS must handle system crashes without losing data or consistency:
> 
> **Checkpoint Recovery**:
> - Start from latest complete checkpoint
> - Contains imap piece locations
> - Known consistent state
> 
> **Roll-Forward**:
> - Read segments after checkpoint
> - Use segment summaries to find changes
> - Update imap with new locations
> 
> **Trade-offs**:
> 
> | Factor | Impact | Consideration |
> |--------|---------|--------------|
> | Checkpoint Frequency | Recovery time | Higher frequency = faster recovery but more overhead |
> | Summary Blocks | Space usage | More detail = better recovery but more space |

> [!consider] Garbage Collection Policies
> Policy choices significantly impact performance:
> 
> **When to Clean**:
> - Threshold-based (X% free space remaining)
> - Continuous background cleaning
> - Idle-time only cleaning
> 
> **What to Clean**:
> 
> | Policy | Advantage | Disadvantage |
> |--------|-----------|--------------|
> | Utilization-based | Maximizes space recovery | May copy more live data |
> | Age-based | Respects temporal locality | Might miss low-utility segments |
> | Cost-benefit | Optimizes cleaning efficiency | More complex to implement |

> [!consider] Performance Trade-offs
> LFS design involves several key trade-offs:
> 
> **Write Performance**:
> - Fast sequential writes
> - But cleaning overhead can impact performance
> - Write amplification from cleaning
> 
> **Read Performance**:
> - Might require multiple segment reads
> - Imap indirection adds overhead
> - File blocks may be scattered
> 
> **Space Usage**:
> - Extra space for cleaning
> - Segment summary overhead
> - Multiple copies during cleaning

