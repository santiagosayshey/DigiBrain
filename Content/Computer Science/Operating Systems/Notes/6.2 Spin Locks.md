> [!motivation] Implementing Concurrency with Locks
> With locks, we now have a way to safely and efficiently implement concurrency in our programs. Locks allow us to ensure mutual exclusion and protect critical sections, preventing race conditions and maintaining data consistency. But **how exactly do we implement locks?**

> [!idea] Implementing Locks with Interrupts
> One approach to implementing locks is by using interrupts. The basic idea is to **disable interrupts during the critical sections of lock acquisition and release**. Here's how it works:
>
> 1. When a thread wants to acquire a lock, it disables interrupts.
> 2. The thread then enters the critical section.
> 3. While in the critical section, interrupts remain disabled, ensuring that no other thread can interrupt the current thread.
> 4. When the thread exits the critical section, it re-enables interrupts.
>
> Here's some C pseudocode to illustrate this concept:
>
> ```c
> // Assume these functions exist
> void disable_interrupts();
> void enable_interrupts();
>
> void acquire_lock() {
>     disable_interrupts();
>     // Critical section starts here
> }
>
> void release_lock() {
>     // Critical section ends here
>     enable_interrupts();
> }
>
> // Usage
> acquire_lock();
> // Perform critical section operations
> release_lock();
> ```
>
> Advantages of using interrupts for locks:
> - Simple implementation: Disabling and enabling interrupts is a straightforward mechanism.
> - Guaranteed mutual exclusion: By disabling interrupts, no other thread can preempt the current thread during the critical section.
>
> Disadvantages of using interrupts for locks:
> - Limited to uniprocessor systems: Interrupt-based locks are only effective on uniprocessor systems.
> - Processor monopolization: While interrupts are disabled, the thread holding the lock monopolizes the processor.
> - Inability to perform other necessary work: Disabling interrupts blocks not only other threads but also important system tasks and interrupt handlers.

> [!idea]+ Implementing Locks with Load and Store Instructions
> A more common and flexible approach to implementing locks is using load and store instructions, also known as locks with flags. This method relies on special atomic instructions provided by the processor to ensure mutual exclusion. Here's how it works:
>
> 1. **A lock is represented by a flag variable, typically an integer, which is initially set to 0 (unlocked state).**
> 2. When a thread wants to acquire the lock, it performs an atomic "test and set" operation on the flag variable. This operation atomically **reads the current value of the flag and sets it to 1 (locked state) in a single, indivisible step.**
> 3. If the previous value of the flag was 0, it means the lock was previously unlocked, and the thread successfully acquires the lock and enters the critical section.
> 4. If the previous value of the flag was already 1, it means another thread already holds the lock, and the current thread must wait (either by busy-waiting or blocking) until the lock becomes available.
> 5. When a thread exits the critical section, it atomically sets the flag back to 0 (unlocked state) using a store instruction, allowing other threads to acquire the lock.
>
> Here's some C pseudocode to illustrate this concept:
>
> ```c
> typedef struct {
>     int flag;  // 0 = unlocked, 1 = locked
> } lock_t;
>
> // Assume this function exists and performs an atomic test-and-set operation
> int test_and_set(int* target);
>
> void acquire_lock(lock_t* lock) {
>     while (test_and_set(&lock->flag) == 1) {
>         ; // Busy-wait (spin) while the lock is held by another thread
>     }
>     // Lock acquired, enter critical section
> }
>
> void release_lock(lock_t* lock) {
>     lock->flag = 0;  // Release the lock
> }
>
> // Usage
> lock_t my_lock = {0};  // Initialize lock
> acquire_lock(&my_lock);
> // Perform critical section operations
> release_lock(&my_lock);
> ```

> [!consider] Race Conditions in Flag-Based Locks
> While locks with flags seem straightforward, they can be **vulnerable to race conditions if not implemented correctly**. Here's why:
>
> - **Non-atomic operations**: If the check and set operations on the flag are not performed atomically, race conditions can occur.
>   - Thread A reads the flag (unlocked)
>   - Thread B also reads the flag (still unlocked)
>   - Both threads think they can enter the critical section
>   - Thread A sets the flag
>   - Thread B also sets the flag
>   - Both threads enter the critical section simultaneously
>
> - **Memory visibility**: Without proper memory barriers or volatile declarations, changes to the flag might not be immediately visible to other threads due to CPU caching or compiler optimizations.
>
> - **Priority inversion**: In a busy-wait scenario, a high-priority thread might be stuck waiting for a low-priority thread to release the lock, leading to priority inversion.

> [!motivation] The Problem with Simple Flag-Based Locks
> **Simple flag-based locks can suffer from race conditions** when multiple threads attempt to enter a critical section simultaneously. This occurs because:
> - Reading and setting the flag may not be atomic operations
> - Changes to the flag might not be immediately visible to all threads
> - There's no guaranteed fairness in lock acquisition
You're absolutely right, and I apologize for the oversight. Let's revise both the idea and example callouts to incorporate this key information more clearly:

> [!idea] Peterson's Algorithm
> Peterson's algorithm is a software-based solution for **mutual exclusion between two threads**, addressing the shortcomings of simple flag-based locks.
> 
> **Key components:**
> - Two flag variables (`flag[0]` and `flag[1]`), one for each thread
> - A turn variable to indicate which thread should yield if there's contention
> 
> **How it works:**
> 1. A thread sets its own flag to indicate interest in entering the critical section
> 2. It then sets the turn to the **other** thread's id, effectively giving up priority
> 3. The thread waits if the other thread is interested (flag is true) **and** it's the other thread's turn
> 
> ```c
> // Pseudocode for Thread i (where i is 0 or 1)
> flag[i] = true;
> turn = 1 - i;  // Set turn to the other thread
> while (flag[1-i] == true && turn == 1-i) {
>     // Wait
> }
> // Enter critical section
> ```
> 
> **Key insight:** Setting the turn to the other thread's id is crucial for fairness and preventing monopoly.

> [!example] Peterson's Algorithm in Action
> Consider two threads, A and B, trying to enter a critical section:
> 
> 1. Thread A sets `flag[A] = true` and `turn = B`
> 2. Thread B sets `flag[B] = true` and `turn = A`
> 3. Both threads check the while condition:
>    - A sees `flag[B] == true` but `turn == A`, so it enters (B gave it priority)
>    - B sees `flag[A] == true` and `turn == A`, so it waits (it gave A priority)
> 4. Thread A completes and sets `flag[A] = false`
> 5. Thread B can now enter the critical section
> 
> ```
> Time  Thread A                 Thread B                 Variables
> ────  ────────────────────────  ────────────────────────  ────────────────────
>  t1   flag[A] = true
>  t2   turn = B                                           flag[A]=T, turn=B
>  t3                             flag[B] = true           flag[A]=T, flag[B]=T
>  t4                             turn = A                 turn=A
>  t5   while(flag[B] && turn==B) 
>      → Enter critical section
>  t6                             while(flag[A] && turn==A)
>                                → Wait
>  t7   [Critical Section]
>  t8   flag[A] = false                                    flag[A]=F, flag[B]=T
>  t9                             → Enter critical section
> t10                             [Critical Section]
> t11                             flag[B] = false          flag[A]=F, flag[B]=F
> ```
> 
> **Key observation:** The last thread to set the turn variable effectively yields priority, ensuring fairness.

> [!motivation] The Need for Memory Barriers in Lock Implementation
> In modern computer architectures, **memory operations can be reordered** by processors and compilers for performance optimization. This reordering can lead to unexpected behaviors in concurrent programs, especially in lock implementations:
> 
> **Problems without memory barriers:**
> - **Visibility issues:** Changes made by one thread might not be immediately visible to other threads due to CPU caching.
> - **Instruction reordering:** The processor or compiler might reorder memory operations, breaking the intended logic of a lock.
> 
> **Example scenario:**
> ```c
> // Without memory barriers
> flag = true;  // Indicate lock acquisition
> // Critical section
> data = new_value;  // Update shared data
> flag = false;  // Release lock
> ```
> 
> In this scenario:
> 1. The update to `data` might be reordered before setting `flag = true`
> 2. Other threads might see `flag = false` before the update to `data` is visible
> 3. This can lead to race conditions and data inconsistency
> 
> Memory barriers provide a way to enforce the required ordering and visibility, ensuring that lock operations work correctly across different threads and processors.

> [!idea] Memory Barrier Instructions
> Memory barrier instructions, also known as memory fences, are **low-level synchronization primitives** used to enforce ordering in memory operations across multiple threads or processors, addressing the issues highlighted in the motivation.
> 
> **Key points:**
> - Ensures visibility of memory operations across threads
> - Prevents reordering of memory accesses by the processor or compiler
> - Critical for implementing synchronization algorithms correctly
> 
> **Types of memory barriers:**
> - Full barrier: Ensures all memory operations before the barrier complete before any after it
> - Load barrier: Ensures all loads before the barrier complete before loads after it
> - Store barrier: Ensures all stores before the barrier complete before stores after it
> 
> **Usage in lock implementation:**
> ```c
> // Example usage in a simple lock
> memory_barrier();  // Ensure all previous operations are visible
> flag = true;  // Acquire lock
> memory_barrier();  // Ensure lock acquisition is visible before entering critical section
> // Critical section
> memory_barrier();  // Ensure all critical section operations complete
> flag = false;  // Release lock
> memory_barrier();  // Ensure lock release is immediately visible to other threads
> ```
> 
> **Importance:**
> - Prevents race conditions and ensures correct behavior of locks
> - Essential for implementing lock-free algorithms and ensuring correctness of synchronization primitives
> - Provides guarantees needed for algorithms like Peterson's to work correctly on modern hardware


> [!motivation] The Need for Hardware-Supported Atomic Operations
> While software-based solutions like Peterson's algorithm are theoretically sound, they face practical limitations:
> 
> - **Complexity:** Difficult to extend to more than two threads
> - **Performance:** Often slower than hardware-supported alternatives
> - **Portability:** May behave incorrectly on modern processors due to memory reordering
> 
> These limitations led to the widespread adoption of hardware-supported atomic operations for implementing efficient and scalable locks.

> [!idea] Test-and-Set Locks
> Test-and-Set locks use **hardware-supported atomic operations** to implement mutual exclusion efficiently.
> 
> **Key concepts:**
> - **Atomic operation:** Performs read-modify-write as a single, uninterruptible unit
> - **Lock state:** Represented by a single memory location (typically a boolean or integer)
> 
> **How it works:**
> 1. **Test:** Check if the lock is free (typically 0 or false)
> 2. **Set:** If free, set it to locked state (typically 1 or true)
> 3. **Atomic:** Both steps occur in a single, indivisible operation
> 
> **Common implementations:**
> - **Test-and-Set instruction:** Directly supported on some architectures
> - **XCHG (Exchange) instruction:** Used on x86 architectures, effectively achieving the same result
> 
> **Basic structure:**
> ```c
> typedef struct {
>     int flag; // 0: unlocked, 1: locked
> } lock_t;
> 
> void acquire_lock(lock_t *lock) {
>     while (test_and_set(&lock->flag) == 1) {
>         // Busy-wait (spin)
>     }
> }
> 
> void release_lock(lock_t *lock) {
>     lock->flag = 0;
> }
> ```

> [!example] Test-and-Set Lock in Action
> Consider two threads, A and B, competing for a lock:
> 
> 1. Initially, `lock->flag = 0` (unlocked)
> 2. Thread A performs test-and-set:
>    - Atomically reads 0 and sets `lock->flag` to 1
>    - A acquires the lock and enters the critical section
> 3. Thread B performs test-and-set:
>    - Atomically reads 1 and sets `lock->flag` to 1 (no change)
>    - B spins, repeatedly trying test-and-set
> 4. Thread A releases the lock by setting `lock->flag` to 0
> 5. Thread B's next test-and-set succeeds, acquiring the lock
> 
> ```
> Time  Thread A              Thread B              lock->flag
> ────  ───────────────────   ───────────────────   ───────────
>  t1   test-and-set() -> 0                         1
>  t2   [Critical Section]
>  t3                         test-and-set() -> 1   1
>  t4                         [Spin]                1
>  t5   [Exit]
>  t6   flag = 0                                    0
>  t7                         test-and-set() -> 0   1
>  t8                         [Critical Section]    1
> ```

> [!consider] Implications of Test-and-Set Locks
> **Advantages:**
> - **Simplicity:** Easy to implement and understand
> - **Efficiency:** Hardware-supported, faster than software-only solutions
> - **Portability:** Supported on most modern architectures
> 
> **Limitations:**
> - **Busy-waiting:** Spinning consumes CPU cycles
> - **Scalability:** Can lead to high contention on multi-core systems
> - **Fairness:** No guarantee of fair scheduling among waiting threads
> 
> **Further considerations:**
> - Test-and-Set locks form the basis for more sophisticated locking mechanisms
> - Understanding these fundamentals is crucial for developing efficient concurrent systems

> [!idea] Compare-and-Swap (CAS)
> Compare-and-Swap is a more versatile atomic operation than Test-and-Set, offering additional capabilities and benefits.
> 
> **How CAS works:**
> 1. **Compare:** Check if a memory location contains an expected value
> 2. **Swap:** If the comparison succeeds, update the memory location with a new value
> 3. **Atomic:** The entire operation is performed as a single, indivisible unit
> 
> **CAS operation signature:**
> ```c
> bool compare_and_swap(int *addr, int expected, int new_value);
> ```
> 
> **Key differences from Test-and-Set:**
> - **Conditional update:** CAS only modifies the memory if it matches an expected value
> - **Flexibility:** Can be used to implement a wider range of lock-free and wait-free algorithms
> - **ABA problem awareness:** Returns the actual value, allowing detection of ABA scenarios
> 
> **Example implementation of a spin lock using CAS:**
> ```c
> typedef struct {
>     int flag; // 0: unlocked, 1: locked
> } lock_t;
> 
> void acquire_lock(lock_t *lock) {
>     while (!compare_and_swap(&lock->flag, 0, 1)) {
>         // Busy-wait (spin)
>     }
> }
> 
> void release_lock(lock_t *lock) {
>     lock->flag = 0;
> }
> ```


> [!example] Compare-and-Swap in Action
> Let's consider a scenario with two threads trying to increment a shared counter using CAS.
> 
> **Shared state:**
> ```c
> int counter = 5;  // Initial value
> ```
> 
> **CAS-based increment function:**
> ```c
> bool increment_counter(int* addr) {
>     int old_value = *addr;
>     int new_value = old_value + 1;
>     return compare_and_swap(addr, old_value, new_value);
> }
> ```
> 
> **Scenario:**
> ```
> Time  Thread A                      Thread B                      counter
> ────  ────────────────────────────  ────────────────────────────  ───────
>  t1   read counter (5)
>  t2   calculate new_value (6)
>  t3                                 read counter (5)
>  t4                                 calculate new_value (6)
>  t5   CAS(counter, 5, 6) -> true                                  6
>  t6                                 CAS(counter, 5, 6) -> false   6
>  t7                                 read counter (6)
>  t8                                 calculate new_value (7)
>  t9                                 CAS(counter, 6, 7) -> true    7
> ```
> 
> **Explanation:**
> 1. Both threads read the initial value (5) and calculate the new value (6).
> 2. Thread A successfully performs CAS, updating the counter to 6.
> 3. Thread B's CAS fails because the counter is no longer 5.
> 4. Thread B retries, reading the new value (6), calculating 7, and successfully updating.
> 
> This example demonstrates how CAS:
> - Ensures atomic updates
> - Handles contention without explicit locking
> - Allows threads to detect and react to concurrent modifications

> [!consider] CAS vs. Test-and-Set in This Scenario
> If we used Test-and-Set for this counter increment:
> - We'd need to implement a lock around the counter
> - Only one thread could access the counter at a time
> - The operation would be less efficient, especially under low contention
> 
> CAS allows for lock-free updates, potentially improving performance and scalability in scenarios like this shared counter.

