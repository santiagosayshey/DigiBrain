## *Huh?*
- Probable events convey little information when they occur
- It is much more informative to know when an unlikely event will occur

![[docs/Computer Science/Databases & Ethical Data/Definitions/Entropy]]

### Expectations
- The expectation of a discrete random variable is the weighted sum of the outcome values
- This sounds complex but, for a six sided die, it would be:

1. Each value (1,2,3,4,5,6) has a 1/6 chance of appearing
2. The contribution of each value to the expectation is weighted by its probability
3. So we add up 1/6, 2/6 ... 6/6
4. Which gives us 3.5

- This means:
	- If we roll a lot of fair d6, over time, the expected value we will see is 3.5


## Okay, But what is Entropy?
- The expected value of the information content of a random variable

$$ H(X) = - \sum_{i=1}^{n} P(x_i) \cdot \log_b P(x_i) $$


![[docs/Images/Pasted image 20230726085030.png]]

##### Examples - Coin Throw

![[docs/Computer Science/Databases & Ethical Data/Exercises/2.2.a]]

##### Examples - Thinking about Entropy

![[docs/Computer Science/Databases & Ethical Data/Exercises/2.2.b]]










